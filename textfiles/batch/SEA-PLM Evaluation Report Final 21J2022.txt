EVALUATION REPORT
December 2021
Evaluation of Southeast Asia-
Primary Learning Metrics
SEA PLM Evaluation
EVALUATION REPORT
December 2021
Evaluation of Southeast Asia-
Primary Learning Metrics
SEA PLM Evaluation
Issue and Revision Record
Revision Date Originator Checker Approver Description
1 12 Nov Fergal Turner Zeynep Ozcan Emma Mba Draft evaluation report
2021 and Rachel and Jeffrey
Outhred Marshall
2 6 Dec Fergal Turner Zeynep Ozcan Emma Mba Final evaluation report
2021 and Rachel and Jeffrey
Outhred Marshall
Document reference: R67 - UNICEF TP LTA1 43315402 | D4 | 2
Information class: Standard
This document is issued for the party which commissioned it and for specific purposes connected with the above-captioned
project only. It should not be relied upon by any other party or used for any other purpose.
We accept no responsibility for the consequences of this document being relied upon by any other party, or being used for
any other purpose, or containing any error or omission which is due to an error or omission in data supplied to us by other
parties.
This document contains confidential information and proprietary intellectual property. It should not be shown to other parties
without consent from us and from the party which commissioned it.
This report has been prepared solely for use by the party which commissioned it (the ‘Client’) in connection with the captioned project. It should not be used for any other purpose. No person other than the Client or any party who has expressly agreed terms of reliance with us (the ‘Recipient(s)’) may rely on the content, information or any views expressed in the report. We accept no duty of care, responsibility or liability to any other recipient of this document. This report is confidential and contains proprietary intellectual property.
No representation, warranty or undertaking, express or implied, is made and no responsibility or liability is accepted by us to any party other than the Client or any Recipient(s), as to the accuracy or completeness of the information contained in this report. For the avoidance of doubt this report does not in any way purport to include any legal, insurance or financial advice or opinion.
W
W
e
e
d
a
i
c
s
c
c
e
la
p
i
t
m
n o
a l
r
l
e
a
s
n
p
d
o
a
n
n
si
y
b i
l
l
i
i
a
ty
b i
f
li
o
ty
r a
w
n
h
y
e
e
th
rr
e
o
r
r
a
o
r
r
is
o
in
m
g
i s
in
s i
t
o
o
n
r t
i n
o r
t h
c
e
o n
r
t
e
r
p
ac
or
t
t
o
w
r
h
o
i
t
c
h
h
e r
is
w i
d
s
u
e
e
w
to
hi c
a
h
n
i
e
t
r
m
ro
i
r
g h
o
t
r
o
o
t
m
h
i
e
s
r
s
w
io
is
n
e
i
h
n
a
d
v
a
e
t a
t
,
o
in
a
f
n
o
y
r m
pa
a
r
t
t
i
y
on
o t
o
h
r
e
s
r
t
t
a
h
t
a
em
n t
e
h
n
e
t s
C
s
li
u
e
p
n
p
t
l i
o
e
r
d
t h
to
e
u
R
s
e
b
ci
y
p i
o
e
th
nt
e
(
r
s )
p
,
a
in
r t
r
ie
e
s
s p
in
e
c
c
l
t
u d
o
i
f
n
t
g
hi s
th
r
e
e
c
p
l
o
ie
rt
n
,
t
o
(
r
‘D
a
a
n
t
y
a ’
i
)
n
.
f
W
orm
e
a
h
t
a
io
v
n
e
a
n
t
o
tr
t
i
i
b
n
u
d
te
ep
d
e
to
n d
it
e
.
n tly verified such Data and have assumed it to be accurate, complete, reliable and current as of the date of such information.
Forecasts presented in this document were prepared using Data and the report is dependent or based on Data. Inevitably, some of the assumptions used to develop the forecasts will not be realised and unanticipated events and circumstances may occur. Consequently Mott MacDonald does not guarantee or warrant the conclusions contained in the report as there are likely to be differences between the forecasts and the actual results and those differences may be material. While we consider that the information and opinions given in this report are sound all parties must rely on their own skill and judgement when making use of it.
Under no circumstances may this report or any extract or summary thereof be used in connection with any public or private securities offering including any related memorandum or prospectus for any securities offering or stock exchange listing or announcement.
Cambridge Education Mott MacDonald Limited trading as Cambridge
Education. Registered in England and Wales
22 Station Road
no. 1243967.
Cambridge CB1 2JD
United Kingdom
Registered office: Mott MacDonald House, 8-10
T +44 (0)1223 463500 Sydenham Road, Croydon CR0 2EE, United
Kingdom
camb-ed.com
The purpose of publishing evaluation reports produced by the UNICEF Evaluation Function is to fulfil a
corporate commitment to transparency through the publication of all completed evaluations. The reports are
designed to stimulate a free exchange of ideas amongst those interested in the topic and to assure those
supporting the work of UNICEF that it rigorously examines it strengths, results, and overall effectiveness.
The content of the report does not necessarily reflect the policies or views of UNICEF.
The text has not been edited to official publication standards and UNICEF accepts no responsibility for error.
Designations in this publication do not imply an opinion on the legal status of any country or territory,
or of its authorities, or the delimitation of frontiers.
The copyright for the support is held by the United Nations Children’s Fund. Permission is required
to reprint/reproduce/photocopy or any other way to cite or quote from this report written form.
UNICEF has a formal permission policy that requires a written request to be submitted. For non-commercial
uses, the permission will normally be granted free of charge. Please write to the UNICEF-EAPRO at the address
below (or email) to initiate a permission request.
Copyright: United Nations Children’s Fund (UNICEF)
East Asia and the Pacific Regional Office
Evaluation Section
Date of Final Version: 10 December 2021
Published by: UNICEF East Asia and Pacific Regional Office
19 Phra Atit Road Bangkok 10200 Thailand
Tel: (66 2) 356-9499 Fax: (66 2) 280-3563
E-mail: eapro@unicef.org or asia.pacific.evaluate@unicef.org
www.unicef.org/eapro
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Executive Summary
Background
The Southeast Asian Primary Learning Metrics Programme (SEA-PLM) focuses on supporting
the Association of Southeast Asian Nations (ASEAN) and the Southeast Asian Ministers of
Education Organisation (SEAMEO) member countries to better understand the status of student
learning achievement, in order to improve the quality of their education systems (UNICEF,
2019). The programme assesses reading, writing, Mathematics and global citizenship, and
administers questionnaires contextualised to the regional context (UNICEF, 2019, p. 9).
The overall goal of SEA-PLM is that by 2025, all Southeast Asia countries have regional
standardised quality measures to assess learning outcomes for Grade 5 students, and
consistently use these to inform improvement within the education sector (SEA-PLM SEAMEO
UNICEF, 2019b, p. 16).
Evaluation purpose and objectives
In 2020, an evaluation of SEA-PLM was commissioned by UNICEF. UNICEF has been a key
participant and funder of SEA-PLM. Prior to investing in the longer-term expansion and
institutionalisation of the survey, UNICEF at the present juncture wishes to take stock of the
experience to date. The main purpose of this independent evaluation is to draw lessons learned
and account for results to inform the next phase of the SEA-PLM Programme. The evaluation is
retrospective (summative), but it is also forward-looking (formative) in providing conclusions and
recommendations for regional- and country-level programming. The evaluation objectives
include reviewing the relevance, effectiveness, efficiency, likely impact and sustainability of the
SEA-PLM programme and identifying strategic approaches that can help better position
UNICEF and collaborators for the SEA-PLM programme in achieving the Sustainable
Development Goal agenda.
The primary audience for the evaluation is UNICEF, EAPRO, SEAMEO, the SEA-PLM
Secretariat, UNICEF Country Office Management teams and Education teams. The secondary
audience includes governments, UN agencies, development partners, donors, non-
governmental organisations, and civil society organisations working in education, as well as
external stakeholders such as contractors and other collaborators.
Methodology
The evaluation has assessed the validity of the results chain, as described in the Theory of
Change (ToC). This included the identification of barriers to delivery, uptake, individual change
and institutional reform, reviewing how delivery is achieved, identifying the likely mechanisms of
impact1, and identifying any contextual issues that shape theories of how the intervention works.
This was done by empirically verifying activities, outputs, and outcomes and assumptions posited
along the causal chains in the ToC and drawing conclusions about what is working and what is
not working in SEA-PLM and likely impact.
The data sources include a desk review of SEA-PLM documentation, a literature review of large-
scale learning assessments, qualitative data collection with stakeholders in three SEA-PLM
participating countries and with a wide range of regional stakeholders, secondary quantitative
analysis of education data across Southeast Asia and a survey of stakeholders in SEA-PLM
participating countries.
1 It is recognised that as the programme has not completed a full cycle yet, impact of student learning cannot be assessed. However, the
likely mechanisms for impact can be explored by testing the assumptions that exist along the results chain.
i
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Findings
The findings for each evaluation question are listed by Development Assistance Committee
evaluation criteria.
Relevance
SEA-PLM alignment to i) the national development policies and priorities of national education
stakeholders and ii) UNICEF’s national, regional and global objectives and intended impacts
SEA-PLM objectives align significantly with national development policies and priorities and
reflects a trend across countries in the region to prioritise learning data.
SEA-PLM objectives align with UNICEF national, regional and global objectives, but are
narrower than UNICEF’s broad focus on holistic child development. UNICEF national priorities
within the area of education focus on, among other priorities, strengthening government
capacity to improve the quality and relevance of teaching and the learning environment. At the
regional level, UNICEF focuses on inequity in terms of access to education and learning, which
aligns with SEA-PLM's stated objectives to improve capacity to analyze learning assessment
data towards more equitable learning outcomes.
Programming gaps or unaddressed needs - What could be done better?
The activities related to the development of the assessment framework and delivery of the first
round of data collection have been largely successful. However, SEA-PLM has so far missed a
valuable opportunity to provide tailored technical capacity strengthening to support participating
countries. The level of capacity and ambition varies significantly across countries in the region,
a fact that was not well reflected in the approach to training.
Communication planning and implementation can be improved. A communications plan was
developed in part and is currently a working document. However, several of the priorities of SEA-
PLM outlined in the plan have not yet been reflected in programme activities.
Related to this, there is a widely recognised need for a more structured approach to political
advocacy, both for attracting new countries, as well as for supporting the use of data. A more
structured forward-looking approach is missing. Ministries of education should be involved in
setting key questions that they would like SEA-PLM data to answer.
Regional endorsement activities took place during SEA-PLM’s emergence, however there
is a need for a longer-term strategy. In addition, while fundraising activities with donors and
countries have taken place, a long-term funding strategy is missing.
Effectiveness
The extent to which objectives and expected outcomes of SEA-PLM have been achieved or are
likely to be achieved
There is clear evidence that changes in national assessments are being informed by SEA-PLM.
There is also unambiguous evidence that SEA-PLM data is supporting increased dialogue
around learning data, but it is not yet clear how this will inform decision making.
Participation in SEA-PLM is strengthening capacity of partner institutions but is largely limited to
the development of tools and the collection of data. While capacity of individuals and units is
growing, it is not clear whether or how this will translate into systemic change in capacity for
assessments or the utilisation of data from assessments.
Major factors influencing the achievement or non-achievement of SEA-PLM objectives and
activities - Enabling factors, barriers and bottlenecks
ii
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
The level of engagement in large-scale assessments (LSAs) has been a key enabling factor for
participating countries, as has the work of UNICEF at the regional and country levels that has
enabled the successes seen so far on SEA-PLM. The lack of long-term funding is a barrier to
success in limiting the ability of the programme to attract and maintain long-term human
resources.
UNICEF and partners actions to ensure the objectives of SEA-PLM are met in the future –
Country and regional level initiatives.
Flexible, locally rooted support and advice and technical support proved to be a particularly
effective mechanism for maximising SEA-PLM impact. Collaboration with academia and the
private sector can help to multiply the amount of analysis of SEA-PLM data. Integration of SEA-
PLM into a broader assessment strategy has benefits both in use of data and in capacity building.
Efficiency
Strengths and weaknesses of SEA-PLM management processes - How could management of
SEA-PLM activities be improved?
The management structure of the SEA-PLM programme has become stronger over the course
of the programme so far. Regional ownership of the programme is a key strength for SEA-PLM,
but the balance between technical and political participation should be considered. More
engagement from political or planning departments within ministries of education would support
a stronger link between SEA-PLM data and policy throughout the programme cycle.
In addition to political participation, ministries of education need greater support in packaging and
messaging the results of SEA-PLM. Ministries of education in SEA-PLM participating countries
are struggling to form a common and understandable language around SEA-PLM results, tie the
results to an intended action and communicate with citizens.
While the technical support arrangement has allowed for the delivery of the first round of SEA-
PLM data, it presents challenges for future rounds of the programme. Country and regional
stakeholders do not view the technical support component of the work as value for money.
Likely impact
SEA-PLM’s contribution to national education systems and assessment practices and policies
and discourses thus far - And positive and negative changes SEA-PLM has brought about at
the regional level
There are several tentative examples of where SEA-PLM has influenced education dialogue at
the national level and there are concrete examples of how SEA-PLM is informing decisions for
national assessments. There is wide ranging vocal commitment to use SEA-PLM data for a range
of different policy issues across participating countries, however it is important to differentiate
these conversations from concrete discussions around policy change. The impact of SEA-PLM
hinges on the ability of the programme to build systemic change in the capacity to use SEA-PLM
data and engage in the political aspects of policy change and education reform.
Sustainability
To what extent SEA-PLM activities, plans and strategies are fully integrated and implemented
by the government (s), both technically and financially and to what extent are they likely to
continue
It is the view of many stakeholders that the SEA-PLM model has room for more cost efficiency,
and for developing a clearer articulation of purpose. The question of sustainability for SEA-PLM
is tied to SEA-PLM’s long-term vision for assessment in the region and the affordability of future
iii
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
rounds. Funding availability is uncertain for future rounds and is dependent on a more cohesive
articulation of what it is that SEA-PLM offers.
Key barriers and bottlenecks towards achieving sustainability of SEA-PLM activities
The continued existence of SEA-PLM is dependent on maintaining existing engagement,
expanding to other countries in the region and establishing a clear narrative on what it provides,
what differentiates it from other International Large-Scale Assessments (ILSAs), and/or how it can
leverage participation in other ILSAs.
For technical sustainability, the capacity of SEAMEO to provide technical and political support on
assessment is a limiting factor.
How SEA-PLM can attract other countries in Southeast Asia and better link other international
and regional initiatives
While the cost of participating is the most obvious barrier to entry, it is only one of the reasons
identified through this evaluation. The second is technical. It is felt that countries were reluctant
to participate until they had seen the quality of data and outputs produced from the first round.
The third is political, worsened by the COVID-19 pandemic. Currently, it is not clear that the SEA-
PLM secretariat has the dedicated human resources to address all these questions. Support to
political narrative building at the national levels is vital, so it is important to ensure that the team
has the necessary level of political influence and understanding.
Equity and Gender Equality
The extent to which SEA-PLM is conducive to supporting the most marginalised populations
and genders (including those furthest left-behind)
Beyond the inclusion of contextual questionnaires for learners, teachers and parents, there is little
evidence of equity being formally mainstreamed within SEA-PLM programme activities. There is
evidence that countries are intending to use SEA-PLM learning data to answer equity questions.
At the regional level, there are ongoing efforts to use SEA-PLM data to answer equity questions,
including secondary reporting on gender differences in learning across the region, and the World
Bank’s use of SEA-PLM data for looking at learning poverty.
Conclusion
The SEA-PLM Secretariat and partners have delivered a successful first cycle. This has taken
place despite human and material resource constraints and is a commendable achievement for
all the stakeholders involved. This has also been achieved despite the significant disruptions
caused by the COVID-19 pandemic.
There is evidence that SEA-PLM is relevant to the needs of participating countries. Increasing the
amount of data available on learning is a priority in most of the countries that participated in SEA-
PLM. What is less clear is what exactly the relevance of SEA-PLM is, whether as a uniquely
regional learning assessment, or as a compliment to other ILSAs operating in the region.
The validity of the SEA-PLM Theory of Change from inputs to outputs is strong but can be
improved. The strong delivery of technical outputs is necessary but not sufficient to deliver on
SEA-PLM intended outcomes and impact. Overall, the success of SEA-PLM hinges on the ability
of the programme to move from data delivery to planning for and supporting the uptake of
evidence to inform decision-making. In addition, a more sustainable delivery model for SEA-PLM
is needed to ensure the programme remains affordable for countries and attracts additional
regional funding to support the programme in the future.
iv
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
SEA-PLM is entering a pivotal phase in 2022. Moving into the second round of data collection,
SEA-PLM has left the “proof of concept” phase and the programme has proved that it can
generate data that is robust and trusted by participating and observing governments. Now it must
build on this and present a long-term strategic vision.
Recommendations
Based on the conclusions of this evaluation we propose a number of recommendations
to be considered for the next round and beyond. Recommendations are divided between
three strategic areas related to; i) building of regional assessment capacity; ii) strengthening the
link between data and policy formation and; iii) establishing SEA-PLM’s regional presence.
Strategic area one: Building Long Term Sustainable Capacity for Assessment in
Participating Countries
Recommendation 1.1: Work with countries to develop long term capacity objectives and
needs. For the next round, SEA-PLM should work individually with countries to define roles and
responsibilities and capacity gaps. This will also feed into a long-term sustainability plan.
Recommendation 1.2: Develop a bank of sustainable and transferrable capacity
development materials. While the first round of SEA-PLM has established a strong and valuable
dataset, it produced little in the way of transferrable training or capacity building materials. SEA-
PLM should take the opportunity to build a library of resources that can be applied beyond those
involved directly in training.
Strategic area two: Strengthening the links between SEA-PLM data and policy decisions
Recommendation 2.1: Plan strategically for the communication, uptake and use of data by
policy makers. In the first round of SEA-PLM, data has resulted in countries recognising
challenges with learning. However, assessment data on its own cannot necessarily influence are
the policy and politics streams. The theory of change for the next round of SEA-PLM should
include activities which address the political aspects of policy change.
Recommendation 2.2: Mainstream equity considerations into SEA-PLM design decisions.
Equity considerations should be tailored to each country and driven by (1) an education sector
analysis concerning equity (usually available for GPE member countries) and (2) the research
questions that Ministries of Education have regarding inequity.
Strategic Area three: Building SEA-PLM as a sustainable regional body
Recommendation 3.1: Establish network of technical assistance individuals and
agencies to support technical teams. In addition to, or as an eventual replacement for, a sole
technical assistance supplier, SEA-PLM should see itself as having a coordinating and
networking function for technical assistance. This would potentially be a more cost-effective
approach to technical assistance, as well as promoting and supporting technical resources
within the region.
Recommendation 3.2: Develop a collaboration and networking strategy. In addition to an
external communications strategy, the strategy for the next round of SEA-PLM should explicitly
focus on networking and collaboration.
Recommendation 3.3: Plan strategically for the establishment of SEA-PLM as a regional
technical hub on learning assessment and education quality. In the long-term, SEA-PLM
should consider the development of a regional technical agency, housed within SEAMEO. This
agency would provide technical inputs for SEA-PLM but would also support other assessment
activities across the region for participating and non-participating countries.
v
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Contents
Executive Summary i
Background i
Evaluation purpose and objectives i
Methodology i
Findings ii
Relevance ii
Effectiveness ii
Efficiency iii
Likely impact iii
Sustainability iii
Equity and Gender Equality iv
Conclusion iv
Recommendations v
List of abbreviations xi
1 Introduction 1
1.1 Background 1
1.2 Report structure 1
1.3 Colour-Coding scheme used throughout the report 1
2 The Evaluation SEA-PLM Programme in Southeast Asia 3
2.1 Regional and country context 3
2.2 Timeline of intervention and implementation status 3
2.2.1 Emergence of SEA-PLM 3
2.2.2 Institutional arrangements 4
2.2.3 Assessment framework 5
2.3 Reconstructed Theory of Change (ToC) 6
3 Evaluation purpose 8
3.1 Type of evaluation (why being done at this time) 8
3.2 Primary and secondary audiences 8
3.3 Duty bearers and rights holders (and their involvement) 8
3.4 Evaluation objectives 9
3.5 Scope of the evaluation 9
3.6 Any adaptations from the initial ToR 9
4 Methodology 10
4.1 Evaluation criteria and questions 10
4.2 Evaluation framework 11
vi
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
4.3 Evaluation design and methods 11
4.3.1 Data collection methods 11
4.3.2 Data collection instruments 12
4.3.3 Data collection consistency and quality 13
4.4 Data analysis 13
4.4.1 Qualitative Comparative Analysis 13
4.4.2 Synthesis 13
4.4.3 Triangulation, bias and informed judgements 13
4.5 Equity and gender 13
4.6 Ethics 13
4.7 Gaps and limitations 14
5 Findings 15
5.1 Relevance 15
5.1.1 To what extent is SEA-PLM aligned to i) the national development
policies and priorities of national education stakeholders and ii)
UNICEF’s national, regional and global objectives and intended
impacts? 15
5.1.2 What are the programming gaps or unaddressed needs? What
could be done better? 16
5.2 Effectiveness and Efficiency 19
5.2.1 What were the strengths and weaknesses of SEA-PLM
management processes? How could management of SEA-PLM
activities be improved? 19
5.2.2 To what extent have the objectives and expected outcomes of
SEA-PLM been achieved or are likely to be achieved? 22
5.2.3 What have been the major factors influencing the achievement or
non-achievement of SEA-PLM objectives and activities? What
were the enabling factors, barriers and bottlenecks? 24
5.2.4 What can UNICEF and its partners do to ensure the objectives of
SEA-PLM are met in the future? What kind of initiatives should
UNICEF prioritize at the country and regional levels? 24
5.3 Likely Impact 25
5.3.1 How has SEA-PLM contributed to national education systems and
assessment practices and policies and discourses thus far? And at
the regional level, what positive and negative changes has SEA-
PLM brought about? 25
5.4 Sustainability 27
5.4.1 To what extent can SEA-PLM activities, plans and strategies be
fully integrated and implemented by the government (s), both
technically and financially? To what extent are they likely to
continue? 27
5.4.2 What are the key barriers and bottlenecks towards achieving
sustainability of SEA-PLM activities? 28
5.4.3 How can SEA-PLM attract other countries in Southeast Asia and
better link other international and regional initiatives? 29
5.5 Equity and gender equality 30
vii
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
5.5.1 To what extent is SEA-PLM conducive to supporting the most
marginalized populations and genders (including those furthest
left-behind)? 30
6 Conclusions and lessons learned 32
7 Recommendations 34
7.1 Building Long Term Sustainable Capacity for Assessment in Participating
Countries 35
Unmissable opportunities 35
Long term visions 35
7.2 Strengthening the links between SEA-PLM data and policy decisions 35
Quick Wins 35
Unmissable opportunities 36
7.3 Building SEA-PLM as a sustainable regional body 36
Unmissable opportunities 36
Long-term visions 37
8 Bibliography 38
Annex A Terms of Reference 40
Annex B Access, quality and equity of education in SEA 51
B.1 Access and equity across the region 51
B.2 Learning assessment programmes and Southeast Asia 53
B.3 Education System Quality Indicators in SEA-PLM participating countries 54
B.4 Access, retention, survival and equity in SEA-PLM participating countries 56
Annex C SEA-PLM logframe 59
Annex D Reconstructed ToC evidence 62
D.1 Impact 62
D.2 Longer term and intermediate Outcomes 62
D.3 Outputs 63
D.4 Activities 64
D.5 Inputs 67
D.6 Assumptions to be tested 67
Annex E Evaluation matrix 69
Annex F Evaluation Framework and Methodology 75
F.1 Conceptual framework 75
F.2 Evaluation approach 76
viii
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
F.3 Data 76
F.4 Data analysis 78
F.5 Equity and gender 79
F.6 Ethics 80
Annex G Data collection tools 81
Annex H Interview Guide 84
G.1 Approach to Interviews 84
G.2 Focus group discussions 84
Annex I Case Study Country findings against the ToC 85
Annex J Documents related to ethical clearance 91
Tables
Table 1 Summary of SEA-PLM alignment to national and UNICEF priorities 15
Table 2 Summary programming gaps, needs and possible improvements 16
Table 3 Summary of the strengths and weaknesses of SEA-PLM management processes
and the achievement of objectives and outcomes 19
Table 4 Summary of SEA-PLM’s contribution to national education systems and
assessment practices so far 25
Table 5 Indications of policy directions and changes in assessment practice 25
Table 6 Summary of the integration of SEA-PLM to government 27
Table 7 Summary of SEA-PLM’s support to the most marginalized 30
Table 8 Contextual questionnaire topics 30
Table 9 Trends across access, enrolment, schooling late expectancy, repetition and
graduation indicators (2013 to 2019), using stoplight rating 52
Table 10 SEA-PLM Activities 65
Table 11 Interview items by stakeholder group 81
Table 12 Assessing inputs across case study countries 85
Table 13 Assessing translating inputs into activities 86
Table 14 Accessing translating activities into outputs 88
Table 15 Assessing progress towards achieving outcomes 89
Figures
Figure 1 Colour coding key 2
Figure 2 Strength of evidence colour coding key 2
Figure 3 SEA-PLM Reconstructed ToC 7
ix
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Figure 4 Map of SEAMEO Member States, SEAP-PLM participating countries and SEA-
PLM Evaluation case study countries 12
Figure 5 Management structure of SEA-PLM 20
Figure 6 Proportion of costs by implementation/TA 28
Figure 7 Assessment of the validity of the SEA-PLM Theory of Change after Round One 33
Figure 8 Overview of Kingdon's Multiple Streams Approach 36
Figure 9 Trends in Grade 8 Maths Proficiency, TIMMS 53
Figure 10 Trends in Grade 8 Science Proficiency, TIMMS 53
Figure 11 Average Proficiency in Maths, by Gender (TIMMS 2015) 54
Figure 12 Average Proficiency in Science, by Gender (TIMMS 2015) 54
Figure 13 Primary school age population (in millions) 2019 55
Figure 14 Pupil-teacher ratio in primary education 55
Figure 15 Percentage of trained primary teachers 55
Figure 16 Proportion of primary schools with basic handwashing facilities 55
Figure 17 Primary Gross enrolment ratio, Primary, Gender parity Index 56
Figure 18 Primary school life expectancy, by gender 2017 56
Figure 19 Survival rate to the last grade of primary education, by gender 57
Figure 20 Number of repeaters across primary grades, by gender 57
Figure 21 Percent of enrolled primary school students who are overage and underage 58
Figure 22 Key functions of process evaluation, from Moore et al, 2014 75
x
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
List of abbreviations
ACER Australian Council for Educational Research
ANLAS Analysis of National Learning Assessment Systems
ASEAN Association of Southeast Asian Nations
CLMV Cambodia, Lao PDR, Myanmar and Vietnam
DAC Development Evaluation Criteria
DIFF Differential Item Functioning
DTRPs Domain Technical Review Panels
EAPRO East Asia and Pacific Regional Office
ECD Early Childhood Development
ESCAP Economic and Social Commission for Asia and the Pacific
ETS Education Testing Service
FGD Focus Group Discussions
GAML Global Alliance for Monitoring Learning
GEROS Global Evaluation Reports Oversight System
GOM Government of Myanmar
GPE Global Partnership for Education
IEA International Association for the Evaluation of Educational Achievement
ILSA International Large-Scale Assessments
KII Key Informant Interviews
Laboratorio Lantinoamericano de Evaluacíon de la Calidad de la
LLECE
Educacíon
LMIC Low- and Middle-Income countries
LMTF Learning Metrics Task Force
LSA Large-Scale Assessments
MOE Ministry of Education
NEQMAP Network on Education Quality Monitoring in the Asia-Pacific
ODA Overseas Development Assistance
OOS Out of School
OOSC Out of School Children
PASEC Programme d’Analyse des Systèmes Educatifs de la CONFEMEN
PISA Programme for International Student Assessment
QCA Qualitative Comparative Analysis
xi
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
RERG Regional Expert Reference Group
RRSC Refugee Resettlement in Small Cities
SACMEQ Southern Africa Consortium for the Measurement of Education Quality
SC Steering Committee
SDG Sustainable Development Goal
SEA-PLM Southeast Asia Primary Learning Metrics
SEAMEO Southeast Asian Ministers of Education Organization
SEAMEO
SEAMEO Innovation and Technology Centre
INNOTECH
SEAMES SEAMEO Secretariat
SES Socio-Economic Status
TAG Technical Advisory Group
TIMSS Trends in International Mathematics and Science Study
ToC Theory of Change
UIS UNESCO Institute of Statistics
UNEG United Nations Evaluation Group
UNESCO United Nations Educational, Scientific and Cultural Organisation
UNICEF United Nations Children’s Fund
UNICEF CO UNICEF Country Office
VfM Value for Money
KICE Korea Institute for Curriculum and Evaluation
KIDE Korea Institute for Development and Education
xii
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
1 Introduction
1.1 Background
Around the world, large-scale survey assessments of learning are conducted at national, regional and
international levels. These surveys are to produce evidence of learning achievement which enables
statistically valid comparisons to be made over time and across different contexts. Such surveys are
instruments intended for the measurement of progress, to ascertain the effectiveness of education
systems, to monitor progress towards equity, and to guide investment and effort.
In Southeast Asia, the Southeast Asia Primary Learning Metrics (SEA-PLM) programme has been
developed and piloted in six countries. SEA-PLM is a sample-based survey of learning at Grade 5 level.
It has been developed with the technical assistance of the Australian Council for Educational Research
(ACER). UNICEF has been a key participant and funder of SEA-PLM. The first reporting cycle of data
was completed in 2020. Prior to investing in the longer-term expansion and institutionalisation of the
survey, UNICEF wishes to take stock of the experience to date. To do so, UNICEF has commissioned
an evaluation of SEA-PLM, to guide itself, and other stakeholders, for the way forward. The Terms of
Reference for this evaluation is included in Annex A.
The Southeast Asian Ministers of Education Organization (SEAMEO) is also a key stakeholder in the
SEA-PLM. UNICEF provides support to the SEAMEO Secretariat. The Association of Southeast Asian
Nations (ASEAN) and the Global Alliance for Monitoring Learning (GAML) are also identified in the
Terms of Reference as beneficiaries of the strengthening of regional alignment and integration. SEA-
PLM focuses on supporting ASEAN and SEAMEO member countries to better understand the status of
student learning achievement, in order to improve the quality of their education systems (UNICEF,
2019). SEA-PLM is nested within the regional Initiative for ASEAN Integration (IAI), which was launched
in 2000 to provide regional integration in order to narrow the development gap within ASEAN. Additional
information on primary and secondary stakeholders for this evaluation is included in section 3.2 of this
report.
In this way the SEA-PLM addresses both technical and political complexities, across a variety of diverse
countries in partnership with a diverse set of actors. Understanding how the programme is working thus
far to address technical and political realities towards individual behaviour change, institutional reform
and ultimately learning outcomes, is crucial at this stage of SEA-PLM implementation.
In order for SEA-PLM to be effective and sustainable, reflection and possible adaptation of the
programme delivery model or structure should be undertaken. This evaluation provides robust data to
inform decisions regarding SEA-PLM.
1.2 Report structure
In the remaining section of this introduction the colour-coding scheme used throughout this report in
outlined. In Section 2 the evaluation of SEA-PLM is described, and a reconstructed Theory of Change
(ToC) is presented. Section 3 discusses the evaluation purpose and Section 4 the evaluation
methodology. Much of the detail of the evaluation is annexed for readability purposes. The findings of
the evaluation are presented, by evaluation criteria, in Section 5. Based on the findings, the conclusions
and lessons learnt are outlined in Section 6 and the strong and weak points in the reconstructed ToC
are identified. A set of actionable recommendations are outlined in Section 7 and are grouped by quick
wins, unmissable opportunities and long-term visions.
1.3 Colour-Coding scheme used throughout the report
Throughout the report, we use the same simple colour-coding scheme to provide readers with broad
overviews of findings against the evaluative judgements for each of the evaluation questions addressed.
Its colour-coding scheme represents a three-level rating scale (and a fourth where there is insufficient
data of evidence).
1
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
The coding scheme reflects a qualitative orientation tool for readers, rather than a quantifiable metric.
The scheme allows for identifying stronger or weaker progress of achievement across the programme
at this point in time. The coding does not represent the potential to improve and should therefore be
interpreted with caution.
Figure 1 Colour coding key
Green indicates strong, Amber indicates Red signifies weak or not Grey indicates a lack of
achieved or improved moderately strong, achieved data or evidence.
partially improved or
partially achieved
We have also included summary regarding the evidence related to the key areas of consideration for
many of the evaluation questions. We report on the strength of the evidence that supports each of
these summary judgements.
Figure 2 Strength of evidence colour coding key
Green indicates the Amber indicates the Red indicates the Grey indicates no
strength of confirming or strength of confirming or strength of the confirming or refuting
refuting evidence is refuting evidence is confirming or refuting evidence. The body of
strong. Evidence is moderate. Evidence is evidence is weak. evidence has
strong when moderate when the body Evidence is weak when unacceptable
documentation, of evidence is stable but the body of evidence has deficiencies, precluding
deliverables and most there are some major or numerous reaching a conclusion.
stakeholder experiences deficiencies. deficiencies (or both) and
and perceptions align. additional evidence is
The findings are stable needed before being
and another evaluation able to conclude that the
would confirm the finding is stable.
findings.
2
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
2 The Evaluation SEA-PLM Programme in
Southeast Asia
The overall goal of the SEA-PLM Programme is that by 2025, all Southeast Asia countries have regional
standardized quality measures to assess learning outcomes for grade 5 students in the domains of
reading, writing, math and global citizenship and consistently use these to inform improvement within
the education sector (SEA-PLM SEAMEO UNICEF, 2019b, p. 16).
2.1 Regional and country context
The East Asia and Pacific region is home to one-third of the global population and more than a quarter
of the world’s children. There is significant diversity in geography and culture as well as economic and
political systems across the region. Disparities in wealth between nations can be observed and in many
countries, there are also very large disparities in wealth within borders. This also applies to ethnic and
linguistic diversity in the region (UNICEF EAPRO, 2020).
Increased enrolment, retention and completion rates and decreased gender gaps have been observed
within some countries in the region, however many children remain out of school (OOS) or lack access
to quality education (UNICEF EAPRO, 2020).
Across the eleven countries in Southeast Asia, recent progress in improving access, quality and
equity varies (See Annex B).
2.2 Timeline of intervention and implementation status
2.2.1 Emergence of SEA-PLM
SEA-PLM was launched as the world refocused education development goals away from access alone,
and towards inclusive and equitable quality education for all. The idea informed by the progress of the
global Learning Metrics Task Force (LMTF) initiative and was the brainchild of UNICEF EAPRO and
SEAMEO in 2012 (UNICEF, 2019, p. 13). The concept was presented during the SEAMEO Council at
the 25th SEAMEO High Officials Meeting in November 2012 and SEA-PLM was recommended for
approval by the Council of Ministers. A Working Groups was established in 2013, including UNICEF
EAPRO, UNESCO Bangkok, UNESCO IS – Bangkok, Economic and Social Commission for Asia
and the Pacific (ESCAP) Statistics Division, SEAMEO Secretariat, SEAMEO INNOTECH and ACER
(UNICEF, 2019, p. 13). The initiative was further conceptualised and endorsed at the 47th SEAMEO
Council Conference in March 2013 and a cooperation agreement was established between UNICEF
EAPRO and SEAMEO INNOTECH to coordinate the Regional Expert Reference Group (RERG),
develop research papers to review education curriculums and assessment programmes across
Southeast Asia, and to formulate strategic plans for the future of SEA-PLM (UNICEF, 2019, p. 13). The
RERG sat in late 2013 and included 30 curricula, assessment and examination experts from eight
SEAMEO Member Countries, along with other partners (UNESCO Bangkok, UIS Bangkok, UNICEF
and ACER). The early structure of SEA-PLM was shaped by this meeting.
At the 36th SEAMEO High Officials Meeting in February 2014, targeting grade 5 in the four domains
was fully endorsed. The formal collaboration was then established between the SEAMEO Secretariat
and UNICEF EAPRO and the agreement continued to 2018. So far ACER has been contracted several
times on one or two year contracts under UNICEF EAPRO and/or UNICEF Country Offices as the main
technical partner for supporting Ministries of Education of participating countries, SEAMEO Secretariat
and UNICEF EAPRO (SEA-PLM Secretariat) in designing and implementing the first round of SEA-
PLM.
Brunei Darussalam, Cambodia, Lao PDR, Malaysia, Philippines and Thailand agreed to further support
the process and a formal launching ceremony of SEA-PLM took place at the end of 2014 (UNICEF,
3
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
2019, p. 14). At the launching ceremony, the official Regional SEA-PLM Steering Committee was
established, with membership of the initial Working Group being expanded to include several MOE
officials from SEAMEO Member Countries and a revised Terms of Refences was established (UNICEF,
2019, p. 14). Consistent SEA-PLM endorsement took place at regional key High-Official meetings and
updates on SEA-PLM were presented during SEAMEO High Officials Meetings (UNICEF, 2019, p. 14).
A range of visits and workshops took place including SEA-PLM Secretariat visits to several field trial
countries in 2015, regional item development workshop in June 2015, a regional field operations and
data management workshop in August 2015 and a regional workshop on coding in 2016.
Interest in SEA-PLM was raised within the region and at the end of 2015 Myanmar decided to join the
field trail, followed by Vietnam, Philippines and Malaysia. Field trails were conducted in Brunei
Darussalam, Lao PDR, Cambodia, Myanmar, Malaysia, Philippines and Vietnam between 2015 and
2018 (UNICEF, 2019, p. 15).
In 2016 SEA-PLM was included in the ASEAN Work Plan on Education (2016-2020), under Priority
Area 2.2 Improving the quality of basic education through quality-focused interventions (The ASEAN
Secretariat, 2016, p. 6).
2.2.2 Institutional arrangements
At the time of writing for this evaluation (November 2021), SEA-PLM is in the last stages of the first
round of the first regional large-scale assessment in the Southeast Asian Region. The results for SEA-
PLM 2019 were reported and disseminated in 2020, supporting the use of the data through to 2021
(UNICEF, 2019). For this reason, it is necessary for partners to conduct this evaluation, in order to
explore the strength and weaknesses of the first assessment round before engaging in the second
round of SEA-PLM. This includes a shift from a biannual plan to a five-year commitment requiring
increased stability, visibility and ownership of the project (UNICEF, 2019, p. 4).
Since inception, the SEA-PLM programme’s structure has evolved. Originally, UNICEF EAPRO acted
as the conduit between all parties within the partnership (SEAMES, ACER and UNICEF Country
Offices) and facilitated all of the contractual arrangements for SEA-PLM (UNICEF, 2019, p. 4). UNICEF
COs delivered advocacy for SEA-PLM with the national MOEs. They provided funding for
implementation and ensured SEA-PLM queries were directed to the right people within MOE. UNICEF
COs acted as the conduit between the MOE and all other partners and ensured continuous follow up
with technical teams, highlighting any risks or challenges as they arose. UNICEF EAPRO defined the
roadmap for SEA-PLM, identifying needs and ensuring relevant technical support. The regional office
supervised all contracts under SEA-PLM, represented SEA-PLM when needed and appropriate,
tracked progress, addressed risks and maintained continuous communication with UNICEF COs to
deliver support and guidance. UNICEF EAPRO monitored and communicated regularly with technical
suppliers to ensure progress and identify opportunities (UNICEF, 2019, p. 43).
From the outset, the partnership with the SEAMES was viewed by UNICEF as crucial in positioning
SEA-PLM within the regional intergovernmental governance structures and as a key to the success of
SEA-PLM (UNICEF, 2019, p. 4). SEAMES delivered the regional mandate and facilitated the regional
and national buy-in. This included coordinating regional workshops, steering committee meetings and
processes of commitments. They also represented SEA-PLM at the global, regional and national levels
and delivered policy expertise and guidance towards securing SEA-PLM as an integrated and
sustainable programme.
ACER, contracted by UNICEF, provided the technical expertise and all direct country implementation
support and training for SEA-PLM. ACER were engaged from 2013. External partners included Network
on Education Quality Monitoring in the Asia-Pacific (NEQMAP, Brookings), UNESCO Institute of
Statistics (UIS), SEAMEO Innovation and Technology Centre (SEAMEO INNOTECH), the United
Nations Economic and Social Commission for Asia and the Pacific (ESCAP), the Korea Educational
Development Institute (KEDI), and the Korea Institute for Curriculum and Evaluation (KICE).
As the original governance structure was designed for the inception period, in mid-2018 an external
consultancy was undertaken to review, analyse and provide recommendations on how the governance
4
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
mechanism for SEA-PLM can be improved and made sustainable, ensuring quality founded within
existing regional structures and aspirations. Consultation on the new governance structure was
undertaken during the Regional SEA-PLM Consultation in 2018. As a result of the external consultancy,
recommended changes included (1) that members of the SEA-PLM Steering Committee will only be
Ministry of Education (MOE) country representatives, (2) that MOE representatives of non-participating
SEAMEO countries should be invited to the Steering Committee on a no-vote basis, (3) that each MOE
will have 2 representatives in the SC, and (4) that a Technical Reference Groups should be established
that would advise the Secretariat and Steering Committee (UNICEF, 2019, p. 46).
A Ministerial Declaration was pre-endorsed in draft form and was reviewed by countries during the
regional consultation in August 2018. A new approved SEA-PLM Secretariat structure that reports to its
chairs, UNICEF and SEAMEO, and ultimately to the Regional Steering Committee and the SEAMEO
Council, was endorsed by countries in March 2019. It was agreed that the Secretariat is responsible for
the day-to-day management of region and national technical and operational tasks in collaboration with
countries and contractors, the SEA-PLM Secretariat is chaired and jointly overseen by UNICEF EAPRO
and SEAMEO Secretariat and that the partnership between the two organisations is crucial for enabling
the commitment of National Ministries of Education to implement SEA-PLM (SEA-PLM, 2019a). There
was recognition that the structure of the Secretariat has shifted over the years based on available
financial and human resources. It was decided that given its technical and political networks, the
SEAMEO Secretariat would maintain the ultimate ownership and oversight of SEA-PLM through its
internal structures. Under the new governance structure, SEAMES contributes to technical
development processes, strategic direction and operations of SEA-PLM. UNICEF contributes financial
resources at regional and country levels, and technical expertise and operational leadership at the
regional and country levels (SEA-PLM, 2019a, p. 6).
In 2019, the Technical Advisory Group (TAG) for SEA-PLM was established. The TAG sought to provide
independent advice on learning assessment matters and to play a consultative and advisory role on
SEA-PLM methodology and results. The TAG chair and members were selected by the SEA-PLM
Secretariat and mandated for one SEA-PLM cycle. The current TAG includes five experts, drawn from
the Korean Institute for Curriculum and Evaluation; the University of Bath; the Chinese University of
Hong Kong and Hong Kong Institute of Educational Research; Universit Sains Malaysia; and Education
Testing Service (ETS) Research Institute and the International Association for the Evaluation of
Educational Achievement (IEA) (SEAMEO and UNICEF, 2019b).
2.2.3 Assessment framework
An assessment framework has been developed and published for SEA-PLM, outlining the technical
design of the assessment. The framework was developed in close consultation with national and
regional experts and led by the SEAMEO Secretariat and UNICEF EAPRO. The SEA-PLM Assessment
Framework was developed to identify ‘what should be measured and in what way’. Recognising the
variation between countries, rather than being curriculum-based, SEA-PLM is curriculum-referenced
and represents structures, conceptual underpinnings and overarching orientations across all ASEAN
countries (ACER, SEAMEO & UNICEF, 2017).
The SEA-PLM Framework was reviewed in February 2015 by the Domain Technical Review Panels
(DTRPs), established for each domain subject consisting of national experts in each domain (UNICEF,
2019, p. 14).
The assessment framework is designed for the first cycle of SEA-PLM implementation, aimed at
students in Grade 5 or equivalent, but leaves scope for further development for SEA-PLM beyond the
SEA-PLM 2019 first cycle (UNICEF and SEAMEO, 2019a). For the future, it is suggested the framework
could be adjusted to suit other grades should these be included in future rounds of SEA-PLM, and that
the assessment programme could eventually provide measures of students’ progress at key stages –
middle primary school, towards the end of primary school, and towards the end of compulsory
secondary school (UNICEF and SEAMEO, 2019a, pp. 3-4).
5
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
The assessment framework includes four domains: mathematical literacy, reading literacy, writing
literacy, and global citizenship. A separate assessment framework has been developed specifically for
global citizenship (UNICEF and SEAMEO, 2019a, p. 3).
SEA-PLM aims to measure both curricular and cross-curricular knowledge, skills and understanding
that are likely to allow school-aged students to progress successfully through school and ultimately to
play constructive and fulfilling roles as citizens in society. It adopts broad definitions for the domains of
mathematics, reading, writing and global citizenship that are consistent with curriculum specifications
but that allow for a focus on the extent to which students in a Southeast Asian context are able to make
effective use of their knowledge in a variety of relevant contexts (UNESC0 Asia and Pacific Regional
Bureau for Education, 2018).
The programme focuses on comparisons in learning achievement both within and between countries.
Programme documentation outlines that sub-national comparisons by social groups are important to
inform decisions from an equity perspective, sub-national comparisons by school type are important to
inform policy decisions, and cross-country comparisons are important to provide an external frame of
reference (ACER, SEAMEO & UNICEF, 2017, p. 5 & 6). In addition, trends over time are expected to
provide important information that contributes to monitoring movement towards the learning
achievement goals. The trends monitored include changes in achievement at one grade level over time,
and changes in differences between sub-populations over time (ACER, SEAMEO & UNICEF, 2017, p.
6).
2.3 Reconstructed Theory of Change (ToC)
The only logframe developed for the overall SEA-PLM programme was developed in July 2015, to
support activities to December 2017. The stated plan was for the logframe to be further elaborated and
also supported by a Theory of Change, developed collaboratively at a SEA-PLM Steering Committee.
The accompanying narrative for the logframe states that the logframe will being used as a means of
verification and will be regularly reviewed, validated and updated by the SEA-PLM Committee (SEA-
PLM, 2015, p. 4). No further iterations, validations or verifications were identified during the desk review.
The logframe can be found in Annex C.
The ToC for SEA-PLM has been implicit, rather than working to a formally prepared initial design. This
is discussed in a paper prepared by staff of UNICEF, SEAMEO and ACER for the Oxford UKFIET
conference in 2015 (Ahmad, 2015).
The implicit theory of how SEA-PLM programme implementation can lead to the desired outcomes and
impact is rarely expressed in the language of a ToC in SEA-PLM documentation. While “impact” and
“outputs” are referred to in the logframe, the language of the implicit ToC tends to focus on the
programme vision, goals and aims.
One reference to a ToC map within the SEA-PLM 2019 Communication Strategy does not follow the
usual logical chain of a ToC (inputs, activities, outputs, outcomes and impact) and rather maps goals
to activities, to levers of change, to systemic change and finally, impact (SEA-PLM SEAMEO UNICEF,
2019b, p. 18).
For the purpose of this evaluation, a reconstruction of the SEA-PLM ToC was undertaken. The evidence
supporting the reconstructed ToC is provided in Annex E and the ToC is presented below. The agreed
assumptions that underpin the ToC are presented in the Findings section of this report, where the
evaluation team assesses the extent to which each assumption holds.
6
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Figure 3 SEA-PLM Reconstructed ToC
7
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
3 Evaluation purpose
This section of the report outlines the purpose of the SEA-PLM evaluation and why it is being
undertaken at this time. The primary and secondary audience and users, duty bearers and rights
holders are described, and the objectives and scope of the evaluation are outlined.
3.1 Type of evaluation (why being done at this time)
The main purpose of this independent evaluation is to draw lessons learned and account for results to
inform the next phase of the SEA-PLM Programme. The evaluation is retrospective (summative) and
covers (secondary) the conception period of SEA-PLM (2012-2015) and (primarily) the implementation
of SEA-PLM 2019 from 2015 to 2019, but it will be forward-looking (formative) in providing conclusions
and recommendations for regional- and country-level programming.
The first reporting cycle of SEA-PLM was 2020 and the dissemination of results have taken place in
2000 and 2021 and throughout this time, SEA-PLM has moved towards a longer-term Programme. For
this reason, now is an appropriate time to explore the strengths and weaknesses of SEA-PLM
governance and assess technical, managerial, funding and partnership perspectives (UNICEF, 2019).
3.2 Primary and secondary audiences
The primary audience of this evaluation is UNICEF EAPRO, SEAMEO, the SEA-PLM Secretariat,
UNICEF Country Office Management teams and Education teams. The secondary audience includes
governments (in particular government agencies working on assessment), UN agencies, development
partners, donors, non-governmental organisations, and civil society organisations working in education,
as well as external stakeholders such as contractors and other collaborators. The findings, conclusions,
and recommendations will be used by both the regional and country offices to inform the development
or implementation of their country Programme documents and adjust their strategies and
implementation modalities as necessary.
3.3 Duty bearers and rights holders (and their involvement)
The right to education is a right of every human being and the principal right-holder is the person being
educated (Monteiro, 2010). In the case of SEA-PLM, children in primary school in Southeast Asia are
right holders and this is why the impact of the SEA-PLM programme must be their improved learning
outcomes. The guarantee of the right to education is an obligation of State Parties in the instruments of
International Human Rights Law. Therefore, it is the duty of governments to provide quality education
for all. Parents and caregivers, communities, teachers, civil society organizations and the international
community each have a subsidiary and auxiliary responsibility to support states and individuals to
achieve these duties (Monteiro, 2010). UNICEF and the international community can be major
participants in the realization of the right to education. This is captured in the Dakar Framework for
Action, which introduces the role of the international community in the allocation of a larger share of
resources to support basic education and ensuring education strategies complement other strategies
for poverty elimination. The Paris Declaration on Aid Effectiveness supports rights-based approaches
in terms of capacity building and accountability. In the case of SEA-PLM, the role of non-state actors
includes the provision of resources, supporting system strengthening and national ownership and
advocating for human rights thinking and practice in aid delivery (UNICEF, 2007). It is important, given
the context of this evaluation to consider the impact the global COVID-19 pandemic and the subsequent
school closures has had on the children as the principal right-holders for education. In particular the
pandemic has forced governments and other duty holders to consider a balance between rights to
education, and public health measures. An investigation of this balance, and the decisions made, falls
outside of the scope of this evaluation, but is an essential part of the context in which it was carried out.
8
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
3.4 Evaluation objectives
The objectives of this evaluation are to (UNICEF, 2019):
● Review the relevance, effectiveness, efficiency, likely impact, and sustainability of the SEA-PLM
Programme and assess the extent to which it has been implemented in line with the objectives and
global good practices on regional learning assessments.
● At regional and national level, identify what strategic approaches, programmatic components and
intervention modalities can help better position UNICEF and collaborators for SEA-PLM
Programme and much broader in a fast-changing EAP region and global initiatives and in support
of country programming in achieving the Sustainable Development Goal (SDG) agenda.
This evaluation will also strengthen and update the knowledge and evidence base on SEA-PLM in
complementarity with other initiatives recently completed, on-going or recently launched by UNICEF in
the region such as the literature review of what works to improve learning in the region (reference to lit
review?).
3.5 Scope of the evaluation
The time boundaries of the evaluation are 2012 to 2021, with a focus on the current implementation
period of SEA-PLM (2015-2021).
The scope of the evaluation in terms of SEA-PLM organisation includes UNICEF country offices
covered by SEA-PLM, UNICEF regional office, the Regional Secretariat, SEA-PLM participating
countries and consultation with non-participating countries.
Geographically, the evaluation covers the six Southeast Asian countries participating in SEA-PLM2,
through the use of three of these countries as case studies.3
3.6 Any adaptations from the initial ToR
The evaluation was designed based on the Terms of Reference, informed by the methodology outlined
in the technical proposal and discussions with UNICEF. Variations from the initial Terms of Reference
are as follows.
1. The evaluation includes case studies of three countries which have participated in SEA-PLM,
rather than of six countries as stated in the Terms of Reference.
2. Due to restrictions on travel and face-to-face working imposed globally and in South East Asia
to tackle the COVID-19 pandemic, significant adjustments were required. All of the tasks
originally envisaged as involving travel and face-to-face meetings by the evaluation team,
potentially including the country case studies and central meetings, workshops, presentations
and consultations, have been conducted remotely, by telecommunications.
The timing and sequence of activities and deliverables were adjusted as indicated in the Inception
Report, and were made in agreement with UNICEF during the course of the assignment, rather than
fixed against the tentative deadlines indicated in the initial Terms of Reference.
2 SEA-PLM countries include: Cambodia, Lao PDR, Malaysia, Myanmar, Philippines, Vietnam
3 Case study countries were Lao PDR, Malaysia and the Philippines
9
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
4 Methodology
This section of the report outlines the conceptual framework, the evaluation methodology, the country
selection for case studies, data sources, data collection methods, data collection instruments, data
analysis techniques, equity and gender considerations and ethics guiding the evaluation.
4.1 Evaluation criteria and questions
The evaluation answers the following questions, here organised according to the evaluation criteria of
the Organisation for Economic Cooperation and Development’s Development Assistance Committee
(OECD-DAC).
Relevance
1. To what extent SEA-PLM aligned to i) the national development policies and priorities of
national education stakeholders and ii) UNICEF’s national, regional and global objectives and
intended impacts?
2. What are the programming gaps or unaddressed needs? What could have been done better?
Effectiveness
3. To what extent have the objectives and expected outcomes of SEA-PLM been achieved or are
likely to be achieved?
4. What have been the major factors influencing the achievement or non-achievement of SEA-
PLM objectives and activities? What were the enabling factors, barriers and bottlenecks?
5. What can UNICEF and its partners do to ensure the objectives of SEA-PLM are met in the
future? What kind of initiatives should UNICEF prioritize at the country and regional levels?
Efficiency
6. How well have SEA-PLM activities been managed by UNICEF in terms of the technical and
financial resources?
7. What were the strengths and weaknesses of SEA-PLM management processes? How could
management of SEA-PLM activities be improved?
Likely Impact
8. How has SEA-PLM contributed to national education systems and assessment practices and
policies and discourses thus far? And at the regional level, what positive and negative changes
has SEA-PLM brought about?
Sustainability
9. To what extent can SEA-PLM activities, plans and strategies be fully integrated and
implemented by the government(s), both technically and financially? To what extent are they
likely to continue?
10. What are the key barriers and bottlenecks towards achieving sustainability of SEA-PLM
activities?
11. How can SEA-PLM attract other countries in Southeast Asia and better link other international
and regional initiatives?
Equity and gender equality
12. To what extent is SEA-PLM conducive to supporting the most marginalized populations and
genders (including those furthest left-behind)?
The hypothesis tested in this evaluation is the ToC. The evaluation matrix turns the ToC of the SEA-
PLM into a set of hypotheses, including judgement criteria, indicators, data sources and analytical
10
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
approaches, on which the evaluation design and instrumentation is based. The evaluation matrix can
be found in Annex E.
4.2 Evaluation framework
The evaluation used theory-based Process Evaluation to answer the evaluation questions. Therefore,
the principal organising structure and framework for the evaluation is the ToC, however we will follow
the key functions of process evaluation. The aim of the process evaluation was to explain how SEA-
PLM works and has examined the processes through which the programme generates outcomes in
different contexts. The evaluation framework is outlined in Annex F.
4.3 Evaluation design and methods
The evaluation explores the interactions between context, SEA-PLM design and causal assumptions;
how delivery was achieved; how participants responded to and interacted with SEA-PLM and has
assessed the validity of the results chain, as described in the ToC. This included the identification of
barriers to delivery, uptake, individual change and institutional reform, reviewing how delivery is
achieved, identifying the likely mechanisms of impact4, and identifying any contextual issues that shape
theories of how the intervention works, contextual factors which affect (and may be affected by)
implementation, intervention mechanisms and outcomes and any causal mechanisms present within
the context which act to sustain the status quo or enhance effects and the mechanisms for impact.
This was done by empirically verifying activities, outputs, and outcomes and assumptions posited
along the causal chains in the ToC and drawing conclusions about what is working and what is not
working in SEA-PLM and likely impact.
4.3.1 Data collection methods
4.3.1.1 Inception report development and Desk Research
In order to outline the key approach, methodology and workplan for this evaluation, the evaluation team
developed a reconstructed ToC based on SEA-PLM programme documentation and an evaluation
matrix based on the final evaluation questions.
An inception workshop with key stakeholders was undertaken to validate the ToC. Results were
reported on in the evaluation inception report.
As part of the inception period, the team reviewed available programme materials and conducted a
review of literature on international and regional assessments.
4.3.1.2 Qualitative data collection
The evaluation methods were predominantly qualitative in nature, allowing us to understand what
change is occurring, how and why. The project has a focus on capacity development, which is not
readily quantifiable and many of the evaluation questions are ‘how’ and ‘why’ questions (rather than
‘how much’).
A country case study approach was utilized, and qualitative data collection was conducted in three of
the six participating countries through virtual and face-to-face country visits. Consultations with
countries other than those featured as case studies was light touch.
4.3.1.3 Country selection for case studies
The selection of case study sites was informed by consideration of a theorised and purposive sampling
approach to produce data from a diverse set of countries that are collectively likely to produce data on
a range of experiences to inform future decisions.
4 It is recognised that as the programme has not completed a full cycle yet, impact of student learning cannot be assessed. However, the likely
mechanisms for impact can be explored by testing the assumptions that exist along the results chain.
11
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Sampling ‘is a core design issue because the purposeful sample will determine what you learn about’
(Patton 2015). Through a carefully chosen sampling strategy, the most relevant sample can be selected,
and rigour can be ensured. The chosen approach to sampling for the country case studies was designed
to generate responses from small numbers of individuals and groups that are representative (though
not statistically) of groups relevant to SEA-PLM and which allow some identification of heterogenous
contributions and experiences.
Purposive sampling can serve this purpose. Purposive sampling involves selecting a sample of ‘typical’
and ‘extreme’ cases based on available data. In essence, the aim is to select countries that cover the
range of national and education system features.
The three case studies were:
1. Lao PDR;
2. Philippines; and
3. Malaysia.
Lao PDR represents a small education system with relatively less experience in large scale
assessments. The Philippines is a larger system, with somewhat more established experience and
Malaysia is a country with significant previous participation in international surveys.
Additionally, the Philippines has seen a deterioration in many education indicators, Laos has had some
very positive progress but remains challenged in some areas and Malaysia has shown progress and
very little deterioration across the range of education indicators.
Figure 4 Map of SEAMEO Member States, SEAP-PLM participating countries and SEA-PLM
Evaluation case study countries
4.3.2 Data collection instruments
The qualitative research tools consisted of Key Informant Interviews (KII) and Focus Group Discussions
(FGD). Semi-structured questionnaires were designed in alignment to the evaluation questions.
Instruments followed the results chain from inputs to outcomes but used the language of SEA-PLM
activities and SEA-PLM expected outcomes rather than the language of evaluators.
12
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
4.3.3 Data collection consistency and quality
4.4 Data analysis
The evaluation team used qualitative comparative analysis and synthesis techniques, carefully ensuring
non-bias using triangulation and informed judgements.
4.4.1 Qualitative Comparative Analysis
Within the structure of the overarching ToC, the evaluation used elements of Qualitative Comparative
Analysis (QCA). QCA is a useful method to identify different factors or combinations of factors that are
likely to have led to a specific outcome, in a given context.5 In line with Contribution analysis, the report
will not claim to reliably identify which, of the factors tested, are necessary or sufficient to obtain
envisaged results, rather it assesses the factors that are likely to have facilitated or hindered achieving
the desired results.
4.4.2 Synthesis
The qualitative analysis has allowed the evaluation team to identify the underlying assumptions related
to the ToC and the extent to which these did or did not hold in the different contexts.
To ensure efficient analysis of the data, the team kept a running record of evidence against each
evaluation question in an evidence matrix in an Excel document.
4.4.3 Triangulation, bias and informed judgements
With qualitative interviews, it is often likely that respondents will provide biased answers. This is why a
range of respondents is needed for triangulation. The data of each respondent was organised around
the themes / questions which then provided an overview of the frequency and strength of responses for
each theme. Furthermore, data from interviews and document reviews was triangulated in order to
provide nuanced answers to the evaluation questions. In the case of contradictory and opposing views
the evaluation team represented these views in our analysis. Both for primary and secondary data, the
evaluation matrix was used to structure the results and the findings.
4.5 Equity and gender
Our approach to mainstreaming equity and gender into the evaluation is captured in the Evaluation
Matrix (Annex E). In order to answer the evaluation question “to what extent is SEA-PLM conducive to
supporting the most marginalized populations and genders (including those left behind)?” the evaluation
team sought evidence that equity and inclusion has been mainstreamed into programme activities and
that the data SEA-PLM produces will be used to inform equity decisions.
4.6 Ethics
The design and conduct of this evaluation was guided by Cambridge Education’s core values of
integrity, respect and excellence as we constantly stretch our thinking to find extra value for our clients
and their end-users. These values also guided our rigorous internal ethical standards, which are aligned
with the United Nations policies and standards for ethics and evaluations. Applicable ethical norms and
procedures of UNICEF and Cambridge Education was applied throughout the evaluation.
The evaluation methodology is consistent with the UNICEF Evaluation Policy (2018), the United Nations
Evaluation Group (UNEG) Norms and Standards for Evaluation (2016), UN SWAP Evaluation
Performance Indicator, UNEG Ethical Guidelines (2008), UNICEF Procedure for Ethical Standards and
Research, Evaluation and Data Collection and Analysis (2015), as specified in the Terms of Reference.
The final evaluation report will conform to the UNICEF-Adapted UNEG Evaluation Report Standards
5 See, for example: Baptist, C. and Befani, B. (2015): Qualitative Comparative Analysis: A Rigorous Qualitative Method for Assessing Impact.
Available at: http://www.coffey.com/assets/Ingenuity/Qualitative-Comparative-Analysis-June-2015.pdf . See also: Qualitative Comparative
Analysis and the Study of Policy Processes. Journal of Comparative Policy Analysis, Research and Practice Volume 19, 2017 - Issue 4:
Special Issue: Validating methods for comparing public policy: Perspectives from academics and “pracademics”.
13
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
(2017), including consideration of a gender equality perspective and human rights based approach,
including child rights. The respective signed documents that supported the ethical clearance are
presented in Annex J.
4.7 Gaps and limitations
There were three key limitations to the evaluation:
1. The first limitation to the evaluation was the timeframe. Because the regional results of SEA-
PLM 2019 were only released in 2020 and the country reports released in 2021, the
evaluation was not able to assess impact of results on children and education systems.
Rather it took a more formative approach, looking at intermediate outputs, outcomes and
likely impact.
2. Secondly, accessing sufficient comparable information across the set of implementing
countries, and from the five other SEAMEO countries, was difficult, given the diversity and
range of these countries. However, a combination of our proposed approach and the
assistance of UNICEF’s EAPRO and Country Offices addressed this challenge satisfactorily.
With accordance to the Terms of Reference, each of UNICEF’s involved Country Offices
facilitated these respects, by designating a focal point person to support the evaluation, who
provided all the information, resource documents and contacts necessary for the evaluation,
facilitated communications with implementing partners in-country, organised and facilitated
logistics, security, meetings and workshops, with financial support from UNICEF’s regional
assessment budget, and by providing comments on key deliverables to minimise factual
errors, misinterpretations and omissions.
3. Thirdly, a significant challenge was the limitations of availability of cost and resource data
disaggregated from general government expenditure and hidden costs borne by various
parties. Typically, countries do not cost their activities such as participating in surveys
separately. For this reason and due to the highly labour-intensive nature of doing in-depth
financial exegesis and analysis, adherence to the principals of VfM will be assessed in the
evaluation, as there are not sufficient resources available for a cost-effectiveness component.
4. The impact of the COVID-19 pandemic on the evaluation was complex. While the necessity to
conduct fieldwork remotely may have introduced some minor limitations in the range of
stakeholders engaged for the three case studies, this is not a certainty, as the remote
approach allowed for a wider timeframe to be used, which potentially made scheduling more
flexible. The timeline for the evaluation also shifted significantly, with data collected taking
place across 2021, rather than in Q3 of 2020. This potentially presented a positive opportunity
to see more of the impact of SEA-PLM data, after its launch at the end of 2020.
14
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
5 Findings
This section of the report outlines the findings for each evaluation question, listed by DAC evaluation
criteria.
5.1 Relevance
Table 1 Summary of SEA-PLM alignment to national and UNICEF priorities
Area of evidence Assessment Strength
of the
evidence
SEA-PLM objectives align with SEA-PLM objectives align significantly with national development policies Strong.
national development policies and priorities.
and priorities
SEA-PLM objectives align with SEA-PLM objectives align with UNICEF national, regional and global Strong.
UNICEF global, national and objectives, but narrower than UNICEF’s broad focus on holistic child
regional objectives and development.
intended impacts
5.1.1 To what extent is SEA-PLM aligned to i) the national development policies and priorities
of national education stakeholders and ii) UNICEF’s national, regional and global
objectives and intended impacts?
SEA-PLM as a programme emerged from the priorities of actors at the regional level, including
SEAMEO and UNICEF. Two major driving forces at the regional level which led to the creation of the
SEA-PLM initiative were the focus on regional integration by ASEAN, and the desire to have more
internationally comparative learning data for the region. The latter was largely driven by UNICEF in
collaboration with other regional bodies, in particular by NEQMAP.
SEA-PLM's stated aims are to achieve three key outcomes:
1. Enhanced capacity to generate and analyse assessment data at regional, national and sub-
national levels;
2. Enhanced capacity to utilize assessment data for education improvement and more equitable
learning outcomes at regional, national and sub-national levels; and
3. Enhanced ASEAN integration in terms of approaches to assessment.
SEA-PLM objectives align significantly with national development policies and priorities.
Malaysia’s Education Blueprint 2013-2025 outlines the importance of student cognitive performance
against international standards, Lao PDR’s new Education Sector Plan’s High-Level Outcome 2 is
“increased number of primary school children with functional literacy and numeracy skills”. The plan
outlines the work being undertaken on the national learning outcomes assessment framework and
articulates the need to improve assessment methodologies at the upper secondary education level and
seeks to improve student learning outcomes measurement. The Philippines has a long tradition of
participation in large scale learning assessments and has both increasing capacity in the DepEd to
undertake assessments and has a National Assessment Technical Working Group on System
Assessment 6. Vietnam has recently participated as a pilot country in the Global Partnership for
Education’s (GPE) Analysis of National Learning Assessment Systems (ANLAS) as it begins to
implement a new general education curriculum and is developing assessment regulations based on the
new curriculum (Global Partnership for Education, 2019). In Cambodia a National Education Congress
conducts annual reviews of sector progress and relies on learning assessment outcome data to
undertake this review (Global Partnership for Education, 2021). Finally, student assessment and
6 ACTRC, Large-Scale Assessments for use in the Philippines.
15
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
examinations is one of the National Education Sector Plan “transformational shifts” in Myanmar
(Government of Myanmar).
SEA-PLM reflects a trend across countries in the region to prioritise learning data. Commitment
to learning assessment in general is evidenced by the participation in other ILSAs across the SEA-PLM
participating countries, as well as non-participating countries. Philippines, Malaysia, Cambodia and
Vietnam (along with Singapore, Brunei Darussalam and Indonesia) have taken part in recent rounds of
PISA and/or TIMSS. Secondly, functional literacy and numeracy, and the need for improved data on
learning levels feature as priorities in all of the national education strategies of the participating countries
(See Table 2 for details).
SEA-PLM objectives align with UNICEF national, regional and global objectives, but are
narrower than UNICEF’s broad focus on holistic child development. UNICEF national priorities
within the area of education focus on, among other priorities, strengthening government capacity to
improve the quality and relevance of teaching and the learning environment. At the regional level,
UNICEF focuses on inequity in terms of access to education and learning, which aligns with SEA-PLM's
stated objectives to improve capacity to analyze learning assessment data towards more equitable
learning outcomes. Across national, regional and global landscapes, UNICEF has a strong focus on
holistic child development. While SEA-PLM objectives align to part of UNICEF’s mandate, it is quite a
narrow alignment. This is unlike the Multiple Indicator Cluster Surveys, for example, which tracks holistic
child development.
5.1.2 What are the programming gaps or unaddressed needs? What could be done better?
Table 2 Summary programming gaps, needs and possible improvements
Area of evidence Assessment Strength
of the
evidence
Field studies and main survey Field studies and main survey data collection have been completed. The Strong.
data collection activities were implemented reasonably smoothly, with a few minor issues
which were resolved.
Assessment framework and The assessment framework, item development, sampling, analysis and Strong.
item development, sampling, reporting were all completed. There are some questions regarding the utility
analysis and reporting of the citizenship measure, which all stakeholders agreed did not live up to
the original aspiration.
Technical capacity building Technical capacity building activities were delivered as expected to ensure Strong.
activities the generation of data. Tailored capacity building to suit the needs of
individual countries and capacity building for data utilization needs additional
focus.
Develop and disseminate Communication planning and implementation can be improved. Strong.
communications strategy
Regional endorsement Regional endorsement activities took place during SEA-PLM emergence, Strong.
activities however there is a need for a longer-term strategy.
Fundraising with donors and Fundraising activities with donors and countries have taken place, but Strong.
countries haven’t resulted in adhered to funding agreements with countries or donors.
In addition, a long-term strategy is missing.
The activities included in the Theory of Change for the SEA-PLM programme have largely been
achieved. At the time of evaluation, the first round of data had been collected, analysed and
disseminated both regionally, and in several of the participating countries. The core activities involved
in this, as outlined by the theory of change (see Figure 3) include the development of the assessment
framework, the field trial and main survey data collection, the provision of technical capacity building,
the development of dissemination and analysis strategy, regional endorsement activities, and
fundraising. While all of these have occurred to some extent, there are a number of unaddressed needs,
or missed opportunities across the activities.
16
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
The activities related to the development of the assessment framework and delivery of the first
round of data collection have been largely successful. While there were some minor issues during
field trial and main survey data collection, none impacted the timeliness or quality of the results.
However, a potential area of missed opportunity was the global citizenship questionnaire. This was
included as a priority for ASEAN. Considering the difficulty in developing a cognitive measure for global
citizenship the format of an opinion survey was the most feasible approach, but lacked depth compared
to other approaches. Some have questioned whether this should have been prioritised as a domain,
compared to the other potential non-academic domains which could have been included. Development
of the global citizenship framework was slowed by the difficulty in reaching consensus as to what should
be included. It is important to recognise that citizenship is one of several enlightenment concepts and
that enlightenment concepts (which include liberty, progress, tolerance, constitutional government and
the separation of church and state) are not equally pursued across all countries. Therefore, agreement
was always going to be slow and the final output was unlikely to fully capture an in-depth measure of
citizenship.
SEA-PLM has so far missed a valuable opportunity to provide tailored technical capacity
strengthening to support participating countries. The level of capacity and ambition varies
significantly across countries in the region, a fact that was not well reflected in the approach to training.
An approach based on a formal and collaborative “capacity assessment” would have allowed for specific
support to be provided based on the plans and capacities of the partner bodies. For example, in both
Malaysia and the Philippines partners expressed a desire to have been more involved in analysis and
reporting in order to build the units’ capacity.
The development of transferrable technical support materials would allow SEA-PLM to
overcome some of the challenges related to staff turn-over. It is a well-recognised fact, that
systemic capacity strengthening is limited by turnover in staff in key partners. One way to adapt to this
is to create materials that can be used in the medium- to long-term to support continuous training. The
evaluation found that, while materials from training sessions are available, they are usually in the form
of PowerPoint presentations, not intended to be used as standalone training materials. The materials
available relate also to test administration and encoding, rather than on analysis and interpretation of
data.
Communication planning and implementation can be improved. A communications plan was
developed in part and is currently a working document. While the communications plan outlines that
SEA-PLM prioritizes “improving national and regional capacity to apply the assessment and to work
with decision-makers in understanding results and translating these into meaningful education reform”
the activities within the communications plan do not reflect this. Similarly, while the “national target
audiences” key areas of messaging include “factors that lead to certain learning outcomes”, “result
implications for curriculum” and “schools and supervisor action to further support learning, monitoring
and planning” (ACER), these messages have not reached the intended audiences. In addition,
confusion regarding item and data ownership, item security policies and processes and data access
has been evident in several countries.
There is a widely recognised need for a more structured approach to political advocacy, both
for attracting new countries, as well as for supporting the use of data. SEA-PLM supported by its
technical partners developed several tools to support the dissemination and analysis of data. This
included a newsletter and mailing list, a document mapping the key stakeholders at regional, national
and global level, and a written communication and uptake strategy for the first round of data, released
in 2020. While these are important exercises, a more structured forward-looking approach is missing.
Key stakeholders consulted during the evaluation felt that the work on dissemination and uptake should
have begun from the beginning of the programme, with ministries of education being involved in setting
key questions that they would like SEA-PLM data to answer. This could have allowed the uptake
strategy to inform questions of sampling, or the questions asked in the contextual questionnaires. While
the activities included were adequate, there is an unaddressed need for a more robust approach that
places evidence users at the centre of the design.
17
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
An area for improvement highlighted by country level respondents was how comparative data
is presented regionally. There has been some dissatisfaction expressed from ministries of education,
who felt that their desire for SEA-PLM not to be a ranking exercise had not been met by the regional
reporting. The regional report does consistently place the six participating countries next to each other
in data visualisations but does so without ranking. Instead, they are placed in alphabetical order. This
is visually harder to interpret but has clearly been done to avoid ranking countries. While there is some
comparison of countries in the text of the report, this is something of a necessity in a regional report.
When compared to the reporting of PISA or other ILSAs, the work done on SEA-PLM to avoid visually
ranking countries is obvious. This dissatisfaction could have potentially been avoided by more thorough
engagement with ministries of education prior to the release of the report.
Regional endorsement activities took place during SEA-PLM’s emergence, however there is a
need for a longer-term strategy. Regional engagement included a formal launching ceremony with
high-level representation, country visits from SEAMES and EAPRO, and validation of regional and
national activity plans. This resulted in SEA-PLM being included in the ASEAN 2016-2020 Education
Work Plan. However, regional endorsement activities have not led to a longer-term strategy beyond the
generation of data following round one.
Fundraising activities with donors and countries have taken place, however a long-term strategy
is missing. The initial projections regarding the costs of SEA-PLM were not accurate and countries
have spent more than the original commitment. Additionally, while fundraising activities with donors has
taken place, these activities have not resulted in funding agreements for SEA-PLM. There is a need to
develop a long-term fundraising/financing strategy regarding the costs of SEA-PLM, the modality of
funding and funding bodies.
Box 1 Testing Assumptions –– Inputs to Activities7
The assessment at this stage of SEA-PLM implementation is:
Assumption 1 holds, but can be improved in some contexts. Stakeholders across case study countries
have opportunities to engage in SEA-PLM activities, however in some countries the resources, time and
conducive environment allows for deeper engagement. This relates to two key areas. (1) The delivery of
capacity building activities and (2) the interpretation of SEA-PLM results towards change.
Assumption 2 holds, but can be improved for some activities. Stakeholders have greater levels of
motivation and incentive to participate in SEA-PLM activities related to technical components of SEA-PLM,
including data collection, assessment and item development, sampling, analysis and reporting, technical and
capacity building activities. Less effective activities (such as dissemination and communication strategies and
endorsement and fundraising activities) has resulted in limited stakeholder capacity to address the political
challenges of using learning assessment data to inform change.
Assumption 3 holds. The Secretariate has delivered on the first round of SEA-PLM, including influencing
SEA-PLM participation and monitoring activities.
7 From inputs to activities there are three assumptions were: (1) Stakeholders have the opportunities (resources, time,
conducive environment) to engage in SEA-PLM activities; (2) Stakeholders have the motivation and incentives to participate in
SEA-PLM activities; and (3) The Secretariat has sufficient leverage within SEA countries to influence SEA-PLM participation
and monitor SEA-PLM activities.
18
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
5.2 Effectiveness and Efficiency
Table 3 Summary of the strengths and weaknesses of SEA-PLM management processes and
the achievement of objectives and outcomes
Strength
Area of evidence Assessment of the
evidence
Strengths of SEA-PLM Regional ownership of the programme, delivery of SEA-PLM data Strong.
management processes generation
Challenges to be addressed Increased political participation and support for messaging SEA-PLM Strong.
results, overstretching of secretariat resources, technical support to
understand, analyse and use SEA-PLM data
Policy makers are preparing to
While there are some examples of decisions being considered based on Strong.
use SEA-PLM data to inform
decisions SEA-PLM data, there is not sufficient utilisation preparations regarding
SEA-PLM data to inform decisions. This is primarily related to the capacity,
rather than will of stakeholders.
Improved regional and national There is clear evidence of improved regional and national capacity to Strong.
capacity to produce, analyse generate data. However, a stronger focus on utilisation is required.
and disseminate data
Stakeholders are aligning There is clear evidence that changes in national assessments are being Strong.
national assessments with SEA- informed by SEA-PLM.
PLM
SEA-PLM is supported by SEA-PLM is supported by national and regional governments and is on the Strong.
national governments and is on regional agenda (evidenced by being included in the ASEAN 2016-2020
the regional agenda Education Work Plan). However, there are threats to ongoing support that
need to be addressed, particularly in the availability of funding, a situation
exacerbated by the economic shock caused by the COVID-19 pandemic.
5.2.1 What were the strengths and weaknesses of SEA-PLM management processes? How
could management of SEA-PLM activities be improved?
The management structure of the SEA-PLM programme has become stronger over the course
of the programme so far. The management structure of SEA-PLM (outlined in Figure 5) is based on
a division between implementing roles, decision making roles, and technical advisory roles. At the
regional level there is a steering committee of members nominated by participating countries with non-
participating countries and key partners invited as observers. At the national level this is replicated for
national decision making. For programme implementation there is a regional secretariat, staffed from
SEAMEO and UNICEF EAPRO, and at national level there are national technical teams, staffed from
ministries of education. Technical support at the regional level is provided by ACER, as well as by a
Technical Advisory Group of global assessment experts. At the national level, technical assistance
arrangements have varied, with a number of different approaches being taken to find technical support
on tasks such as reporting and dissemination. In some cases, this has been done internally, but in most
cases, it has been outsourced to consultants, either nationally, or through the regional and national
contracts with ACER.
19
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Figure 5 Management structure of SEA-PLM
Regional ownership of the programme is a key strength for SEA-PLM, but the balance between
technical and political participation should be considered. It is noted that the regional steering
committee is largely (though not exclusively) composed of assessment experts from ministries of
education in the participating (and non-participating countries). While this has been important for
ensuring the quality of technical outputs during the first round, it potentially weakens the advocacy
potential of the programme.8 More engagement from political or planning departments within ministries
of education would support a stronger link between SEA-PLM data and policy throughout the
programme cycle.
In addition to political participation, ministries of education need greater support in packaging
and messaging the results of SEA-PLM. In almost all countries globally, ministers of education can
adapt and use a common language when communicating with their citizens regarding examination
results. However, ministries of education in SEA-PLM participating countries are struggling to form a
common and understandable language around SEA-PLM results, tie the results to an intended action
and communicate with citizens. It is not yet clear exactly what information will be publicly available as
at the time of evaluation national reports had not all been published. The concerns however are felt by
stakeholders both nationally and regionally. This is an area in which SEA-PLM can provide greater
support to ministries of education in developing communication strategies for SEA-PLM results.
Programme effectiveness, efficiency and sustainability is threatened by an overstretching of
secretariat resources. The delivery of SEA-PLM is heavily dependent on a small secretariat of
individuals located within SEAMEO and UNICEF EAPRO. While the work of UNICEF EAPRO and the
individuals in the secretariat is regarded as being high quality, there are resource limitations.
As previously mentioned, technical support to understand, analyse and use SEA-PLM data is
lacking across participating countries. The majority of technical support thus far has focused on
support to complete the activities required to generate data on learning levels. Country level
stakeholders reported that a more collaborative approach between education departments, ACER and
UNICEF should be used in the analysis and reporting phase of the study. Crucially, meaningful follow
up and action to use the high-quality data that has been generated is needed. In many SEA-PLM
countries government officials are keenly aware of the low achievement of their children but do not
know how to address the problem. While webinars have allowed countries to share insights, key
stakeholders feel that more could be done to support the process of translating evidence to policy.
Technical support on how to use SEA-PLM data to inform decisions to improve learning levels is
fundamental moving forward.
8 Advocacy does not only refer to advocating for participation in SEA-PLM but advocating for the use of SEA-PLM data in decision-making.
20
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
The focus on data generation is also evident when reviewing the clarity of scope, management
and monitoring of outputs across the SEA-PLM programme. Activities that led to reliable, valid,
relevant and rigorous learning and contextual data, the regional common learning metric and the
alignment of SEA-PLM to the global UIS scale have all be completed to a high standard. However, it is
less clear that the strategy for analyzing and disseminating data and the development of common tools
and protocols to utilise assessment data have been clearly defined, clearly assigned to specific actors,
managed and monitored.
While the technical support arrangement has allowed for the delivery of the first round of SEA-
PLM data, it presents challenges for future rounds of the programme. Country and regional
stakeholders view the technical support component of the work as costly and not necessarily value for
money. Participants with sufficient technical expertise have also questioned the extent to which
transparency is possible with an individual technical partner, suggesting technical experts from more
than one institution should be involved in various aspects of the technical processes. Other stakeholders
have championed building technical expertise within SEAMEO, to reduce costs and build capacity in
the region.
Future decisions regarding the technical support arrangement for SEA-PLM need to consider a
wide variety of trade-offs. For example, while expanding the number of technical partners involved in
SEA-PLM may increase transparency, the current arrangement relies on the institutional capacity of
ACER to deliver on tight timelines and to high technical standards. Changes in the model will need to
ensure there remains sufficient institutional capacity to deliver.
Building technical expertise within SEAMEO would require a strategy to build institutional
capacity to deliver. Building relationships with other regional assessment centres and learning
from their models would be beneficial. For example, the UNESCO Latin American Laboratory for the
Assessment of the Quality of Education located in Santiago has delivered four rounds of regional
assessments across up to 18 countries. Creating opportunities to draw on their technical expertise could
reduce costs and increase peer to peer learning.
Box 2 Testing Assumptions – Activities to outputs9
The assessment at this stage of SEA-PLM implementation is:
Assumption 1 holds in some countries, but sustainability of funding remains a concern. Donors and
participating countries have had sufficient resources to invest in SEA-PLM thus far, however some countries
have struggled to keep up with the data collection logistics and budget activities, delaying progress. The cost
of external technical assistance represents a threat to the sustainability of SEA-PLM and a more sustainable
financial model for SEA-PLM needs to be developed for the future.
Assumption 2 holds. Technical and capacity building inputs have been sufficient to result in the desired
results for round one of SEA-PLM. The aspiration for longer-term capacity building outcomes for future rounds
is, however, worthy of articulating, with a clear strategy for how strengthened capacity will be achieved.
Assumption 3 holds. Agreement on the analysis and dissemination of SEA-PLM results has so far been
achieved across SEA-PLM countries. However, following the first round of data dissemination, strategies need
to be developed to support countries to use the data to understand how to improve learning levels.
Assumption 4 holds. The common regional metric has been achieved and linked to the global learning
metric.
Assumption 5 holds. The data generated by SEA-PLM has resulted in a high-quality learning assessment
programme, producing results trusted by countries and partners in the region.
9 From activities to outputs the five assumptions were: (1) Donors and participating countries have sufficient resources to invest
in SEA-PLM and consider SEA-PLM Value for Money (VfM); (2) Technical and capacity building inputs are sufficient to result in
the desired outputs; (3) There is sufficient common ground between countries to find agreement on the analysis and
21
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
5.2.2 To what extent have the objectives and expected outcomes of SEA-PLM been
achieved or are likely to be achieved?
There is clear evidence that changes in national assessments are being informed by SEA-PLM.
In the Philippines the Bureau of Education Assessment has begun to use approaches from SEA-PLM
to inform assessment practices nationally. The Bureau of Education Assessment has a clear mandate
to integrate approaches from ILSAs into national assessment practices. This includes both the design
and administration of assessments, as well as data analysis and supporting policy makers to use the
assessment data in decision making. In Lao PDR there is an intention to use SEA-PLM to contribute to
a long-term curriculum review process, however the details of how SEA-PLM data can or will be used
in this way remain elusive. In Malaysia there is some suggestion that SEA-PLM could be used to help
teachers develop their own assessment approaches and measure learning loss due to the COVID-19
pandemic.
There is also unambiguous evidence that SEA-PLM data is supporting increased dialogue
around learning data, but it is not clear how this will inform decision making. The data is trusted
as valid and reliable across participating countries and most countries explicitly state that they aim to
use ILSA to generate insights to inform policy formation. What remains to be seen is how the data from
SEA-PLM can be used to move beyond the recognition of a problem, and toward the generation of
solutions.
Participation in SEA-PLM is strengthening capacity of partner institutions, but is largely limited
to the development of tools and the collection of data. Capacity building activities covered the
necessary training needed to deliver on SEA-PLM activities needed to support data collection. It was
felt by many stakeholders, across various groups, that more could be done to take a structured, future-
oriented approach to building sustainable capacity to independently continue SEA-PLM activities.
While capacity of individuals and units is growing, it is not clear whether or how this will
translate into systemic change in capacity for assessments. Challenges exist in retaining systemic
capacity across countries, with individual staff members regularly being replaced due to promotions and
retirements etc. The same institutional capacities may be difficult to sustain in UNICEF as personnel
are reassigned regularly to different posts and locations.
Support for SEA-PLM at the national and regional level is strong, but there remain several
threats to continued support. At the regional level this is evidenced by the inclusion of participation
in SEA-PLM as a strategic objective for the ASEAN 2016-2020 education sector workplan. At the
national level, there have been public commitments to SEA-PLM from the majority of ministries of
education. However, this does not necessarily translate into agreement to participate in the next round.
As is detailed under section 5.4, there are a number of factors that will influence participation in future
rounds. Of these, the financial question is the one most pertinent for those countries already
participating. It is clear across countries, that the economic shock of the COVID-19 pandemic will have
an impact on education sector budgets. This introduces an extra element of uncertainty to the
commitments already made publicly.
dissemination of SEA-PLM results; (4) There is sufficient domain coverage, international and regional data and political will to
support a common regional and global metric; and (5) The data generated by countries is of sufficient quality to support a
regional common learning metric.
22
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Box 3 Testing Assumptions – Outputs to intermediate outcomes10
The assessment at this stage of SEA-PLM implementation is:
Assumption 1 does not hold in some countries and is unknown in others. Some countries do not have
sufficient national capacity to analyse available data. In all three country case studies additional technical
support was required in order to analyse the available data and develop country reports. However, Malaysia
registered an interest in being more involved in the analysis and suggested using a more collaborative
approach between country staff, ACER and UNICEF. It was felt that skills exist within countries to analyse the
data, with support.
Assumption 2 does not hold in all settings. Policy makers in different countries have different capacities,
incentives and political space to access and utilize SEA-PLM data to inform decisions. Malaysia has a strong
history of using ILSA data to inform policy making, in the Philippines there is a clear push by the DepEd to use
ILSA (with participating highlighting potential avenues for policy change based on SEA-PLM round one
results) and in Lao PDR SEA-PLM will likely increase the demand for assessment and research. However,
across countries there is limited capacity to know how to use the data to inform solutions, rather than describe
problems (low levels of learning) and how to approach messaging of low learning levels in complex political
environments.
Assumption 3 does not fully hold. The fidelity of SEA-PLM capacity building activities is difficult to assess,
as there were limited protocols and/or programme models clearly outlining the expected implementation of
capacity building processes. The dose of capacity building activities has been sufficient to administer and
release results in the first round of SEA-PLM, however the dose is not sufficient to support uptake of SEA-PLM
evidence yet.
Assumption 4 does not hold. The current financial model for SEA-PLM is not sustainable.
Assumption 5 partly holds. Countries participating in SEA-PLM see the value of a common approach to
regional assessment. However, few participants from across SEA-PLM participating countries see the current
financial model as sustainable.
Testing Assumptions – Intermediate outcomes to longer term outcomes11
The assessment at this stage of SEA-PLM implementation is:
Assumption 1 is unlikely to hold, if gaps are not addressed. Using SEA-PLM data to improve relevance,
equity and learning in education systems relies on building the capacity of stakeholders to analyse SEA-PLM
data, generate results and findings, act on these findings and disseminate information from assessments
down to different audiences, including schools and teachers.
Assumptions 2 and 3 do not hold in all countries. SEA-PLM participating countries do not currently have
the capacity to solve education sector issues. While SEA-PLM round one results have allowed for discussions
on low learning levels, the capacity to use the data to improve learning is not evident across all SEA-PLM
participating countries.
1010 From outputs to intermediate outcomes the five assumptions were: (1) There is sufficient national capacity (technical
capabilities, political will, resources) to analyse available data; (2) Policy makers have sufficient capacity, incentives and
political space to access and utilize data to inform decisions; (3) The reach, fidelity and dose of capacity building activities are
sufficient at national and regional levels; (4) The costs of SEA-PLM are sustainable during integration into regional systems and
beyond; and (5) ASEAN countries see the value of a common approach to regional assessment and believe SEA-PLM is
sustainable.
11 From intermediate outcomes to longer-term outcomes the three assumptions were: (1) The use of SEA-PLM data leads
to the improvements of previous shortcomings in relation to relevance, equity and learning in the education system; (2)
Stakeholders have the capacity and incentives to solve education sector issues; and (3) Stakeholders have the opportunities
(resources, time and a conducive environment) to solve education sector issues.
23
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
5.2.3 What have been the major factors influencing the achievement or non-achievement of
SEA-PLM objectives and activities? What were the enabling factors, barriers and
bottlenecks?
The level of engagement in LSAs has been a key enabling factor for participating countries. Four
of the six participating countries had recently taken part or were simultaneously taking part in other
large-scale assessments. While it may not always have been the same individuals across both
assessments, certainly familiarity with the process and activities involved in large scale assessments
facilitated the delivery of SEA-PLM. The inclusion of learning assessments as a priority in the ESPs of
several countries ensured that human resources could be consistently dedicated to the programme.
The relative consistency of staff across technical teams was also seen as an enabler for success (linked
to engagement and buy-in).
The work of UNICEF at the regional and country level is another key enabler of success for SEA-
PLM. UNICEF are widely seen as having been a key factor in driving the delivery of the first round of
SEA-PLM. At the country level, the presence of UNICEF country office teams allowed for flexible
support to be provided to governments. This support was financial, but also included the facilitation of
technical assistance (either from ACER, or from other contracted bodies). The responsiveness and
adaptability of this arrangement was seen as a key enabler for success in the first round of SEA-PLM.
A lack of long-term funding is a barrier to success in limiting the ability of the programme to
attract and maintain long-term human resources. The skillset needed for the development and
delivery of large-scale learning assessments is specific and not common across the ASEAN region or
globally. This means that it will always be a challenge to recruit assessment experts. It is considered by
many respondents that this is exacerbated by the lack of long-term funding. Regardless of whether this
is the case, difficulties in finding and recruiting assessment experts at the regional level, particularly as
SEA-PLM moves towards more in-house technical capacity, presents a key barrier.
5.2.4 What can UNICEF and its partners do to ensure the objectives of SEA-PLM are met in
the future? What kind of initiatives should UNICEF prioritize at the country and regional
levels?
To avoid overlapping with recommendations, the findings of this section are organised to detail
particularly useful mechanisms observed from the first round that could support future planning.
Flexible, locally rooted support and advice and technical support proved to be a particularly
effective mechanism for maximising SEA-PLM impact. As mentioned in section 5.2.3, the presence
of UNICEF country offices as a source of flexible support for national technical teams has proved to be
very useful. Beyond facilitating the provision of technical support, the UNICEF country offices have
acted as a “thought partner” for governments in navigating the dissemination and use of SEA-PLM data.
To a certain extent, ACER have also played this role at the regional level, but the understanding which
UNICEF country offices have of context has been particularly valuable. Whether or not it is UNICEF or
another agency playing this role, this has been a particularly useful mechanism for the SEA-PLM
programme.
Collaboration with academia and the private sector can help to multiply the amount of analysis
of SEA-PLM data. SEA-PLM has created a rich dataset on learning across the region. This dataset is
of great interest to a range of organisations working in education research. Ensuring that the dataset is
made available in as complete a format as possible has already allowed for secondary research to be
undertaken, for example by the World Bank or UNICEF Innocenti at the regional level. Encouraging,
facilitating and consolidating this kind of work can allow, and has already allowed SEA-PLM data to be
applied to a range of different research questions, with minimal additional investment for the
programme.
Integration of SEA-PLM into a broader assessment strategy has benefits both in use of data and
in capacity building. For both the Philippines and Malaysia, SEA-PLM was undertaken to complement
PISA and TIMSS. In the Philippines SEA-PLM was included in a broader national assessment strategy
which included both the other ILSAs, as well as the national assessments. In both cases the approach
24
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
allowed for the data from assessments of different ages and subjects to be compared to each other,
increasing the visibility and scope of findings. In the case of the Philippines, it also helps with longer
term planning for the role and capacity of the Bureau of Education Assessment.
5.3 Likely Impact
Table 4 Summary of SEA-PLM’s contribution to national education systems and assessment
practices so far
Strength of
Area of evidence Assessment
the evidence
Changes in the topics, SEA-PLM has tentatively influenced the topics, focus and amount of Strong.
focus and amount of education dialogue at the country level. Results have not been circulating for
education dialogue at the long enough to change regional dialogue yet.
country and regional
levels
Changes to national SEA-PLM has influenced changes in national assessment practices. Strong.
assessment practices
Regional changes in It is too early to identify regional changes in assessment practices. Weak.
assessment practices
5.3.1 How has SEA-PLM contributed to national education systems and assessment
practices and policies and discourses thus far? And at the regional level, what
positive and negative changes has SEA-PLM brought about?
There are several tentative examples of where SEA-PLM has influenced education dialogue at
the national level. There is wide ranging vocal commitment to use SEA-PLM data for a range of
different policy issues across participating countries. It is important to differentiate these conversations
from concrete discussions around policy change. In many cases the dialogue around SEA-PLM data is
at the level of recognizing that challenges exist, and that learning needs to be prioritised. In some cases,
the level of dialogue has moved beyond this, and towards discussions of policy change. These are
detailed in Table 5. While these conversations are largely tentative and yet to be borne out in concrete
changes to policy, this is to be expected. At the time of writing, not all countries had published national
reports on SEA-PLM data. It was noted in Lao PDR that compared to previous assessments, SEA-PLM
had had a larger impact in terms of being discussed publicly. This is testament to the increased degree
of buy-in that comes from a regional assessment.
In the Philippines the data from SEA-PLM is being used to reform the curriculum. An example of
ways in which SEA-PLM might expect to be used in the future can be seen in the Philippines. The
Philippines Normal University is currently conducting an analysis of the curriculum in comparison with
SEA-PLM questions. The purpose of this analysis is to highlight where the kinds of analytic thinking
assessed by SEA-PLM are not currently being covered by the curriculum. Should this result in concrete
changes to the curriculum it would be a positive example of how SEA-PLM can explicitly inform
education reform. It shows how assessment data cannot just highlight a challenge, but can also provide
a concrete policy response.
Table 5 Indications of policy directions and changes in assessment practice
Country Indications of Policy Directions Changes in Assessment Practice
Cambodia Suggested policy responses include: Item models are being used to improve
assessment items used in national
• Increase number of community pre-schools
assessments
• Introduce GC concepts into curriculum
• Review quality and contents of textbooks
Too early to say how these will be manifested
Lao PDR Too early to say. Some indications that SEA-PLM Some overlap with developments in national
results have better uptake than previous assessments and SEA-PLM approach, but no
assessments, but this has yet to translate into conclusive influence of SEA-PLM
concrete policy directions.
25
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Country Indications of Policy Directions Changes in Assessment Practice
Malaysia Too early to say. At the time of evaluation Malaysia Early stages, intention to use SEA-PLM
was producing its national report and preparing for methods to help teachers develop assessment
discussions on policy implications. Central use of approaches for their schools.
PISA and TIMSS results in sector planning shows a
strong likelihood that SEA-PLM data will be used as
part of planning.
Myanmar No data No data
Philippines Wide ranging commitment to use SEA-PLM data as SEA-PLM approaches used to deliver the first
part of the process of reviewing the curriculum (with a national assessment of writing in 2019/20
review underway), and to re-examine the national
Introduction of contextual questionnaires on the
language policy. Too early to say whether these will
basis of SEA-PLM experience.
result in concrete action.
Vietnam Strong commitment to further analysis and wide SEA-PLM methods have been used to inform
dissemination of SEA-PLM results. No concrete policy the development of a circular on the evaluation
directions as of the time of evaluation. of primary school students.
SEA-PLM used to inform a national large-scale
assessment of learning outcomes for the school
year 2019-2020
There are concrete examples of how SEA-PLM is informing decisions made for national
assessments. In the Philippines, Vietnam and Cambodia there are concrete examples of how SEA-
PLM is being used to inform national assessment practices. While these are positive indications, it is
important to reflect both on association and causality. Cambodia, Vietnam and the Philippines took part
in SEA-PLM as part of a broader commitment to assessment reform, rather than SEA-PLM inspiring a
reform of assessment practices. Similarly, Cambodia, Vietnam and the Philippines have all recently
taken part in PISA and/or TIMSS, making it difficult to dissociate the impact of SEA-PLM from its
environment. Regardless of causality or attribution, these are positive indications, and should be tracked
and built upon for the next round.
Box 4 Testing Assumptions – Longer-term outcomes to impact
From longer-term outcomes to impact the main assumption was that changes in the education system
positively affect relevant learning outcomes and equity.
The assessment at this stage of SEA-PLM implementation is:
Assumption 1 may or may not hold. There is insufficient empirical evidence regarding how changes in the
education system positively affect relevant learning outcomes and equity, mainly due to a lack of data. The
evaluation of GPE support to 29 countries conducted between 2017 and 2020 found that 15 of the 29
jurisdictions had insufficient data to identify learning outcome trends. Of the 14 jurisdictions where data was
available, outcomes improved modestly in 7, remained the same in 5 and deteriorated in 2 (Universalia 2020).
26
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
5.4 Sustainability
Table 6 Summary of the integration of SEA-PLM to government
Strength
Area of evidence Assessment of the
evidence
Capacity expectations to
There is sufficient capacity to implement future rounds of data generation within Strong.
implement future rounds
being achievable within the technical assistance budgets allocated in round one. However, there is
TA budgets currently insufficient resources to implement future rounds within TA budgets to
support the utilization of data, using the current SEA-PLM delivery model
National technical
There is increased national technical capacity to generate SEA-PLM data, Strong.
capacity to undertake
technical tasks however continued technical assistance is required and increased technical
assistance is required to improve national capacity to use data.
Funding availability and
Funding availability is uncertain for future rounds and SEA-PLM is not seen as Moderate.
SEA-PLM affordability
for future rounds affordable by many stakeholders.
5.4.1 To what extent can SEA-PLM activities, plans and strategies be fully integrated and
implemented by the government (s), both technically and financially? To what extent
are they likely to continue?
The question of sustainability is tied to the question of what SEA-PLM’s long-term vision is for
assessment in the region. Across the stakeholders consulted for this evaluation, there is some
disagreement about the role of SEA-PLM, as compared to other ILSAs. It is clear from UNICEF and
SEAMEO that there is a vision to have a regular regional assessment for the future. However, outside
of these two bodies, some have questioned how SEA-PLM should relate to TIMSS and PISA. The
suggestion from some quarters that Singapore would not take part due to their achievement in PISA
suggests that SEA-PLM is a stepping stone towards achievement in PISA rather than a standalone
regional assessment.
It is clear that capacity building activities will be necessary for future rounds, at both the national
and regional level. There is wide agreement that capacity building activities should continue as an
integral part of SEA-PLM. This is driven by two findings. The first is that the scope of capacity building
has not reached all of the technical areas that participating countries would wish to build their capacity
in (see section 5.1.2), particularly for data interpretation and use. The second is that turnover in key
technical staff means that some degree of capacity building will always be required.
The affordability of future rounds depends on what decisions are made regarding the technical
assistance arrangements. The technical assistance arrangement in place for the first round was
sufficient to deliver activities. However, it is widely considered that this arrangement will not be
financially sustainable in the long term. Figure 6 shows that country technical support (largely covered
by the UNICEF EAPRO and country offices) amounted to 34 percent of the total cost of the SEA-PLM
programme (excluding pre-2018 costs for developing the assessment framework). This cost burden for
UNICEF may be sustainable in the short term but will need to be reduced if ministries of education are
to take this cost on. There is also some reluctance in participating countries for this technical assistance
to be sourced internationally, when more cost-effective alternatives may exist.
27
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Figure 6 Proportion of costs by implementation/TA
17%
34%
Country Technical Support
Country Implementation
14%
Regional Technical Support
Regional Implementation
35%
Source: Author’s calculations based on SEA-PLM budget data
Funding availability is uncertain for future rounds and is dependent on a more cohesive
articulation of what it is that SEA-PLM offers. SEA-PLM seeks funding for learning assessment from
its national government partners at an extremely challenging time for government budgets. While
governments have in many cases publicly committed to participation in future, budget allocations have
not been confirmed. The response from UNICEF has been to re-assure participating countries that they
can ensure that funding is available to support participation in the next round. While there are ongoing
efforts to secure funding at the regional level, either from high income countries in the ASEAN region,
or from international funders, funding is currently not yet confirmed. It is the view of many stakeholders
that these efforts are undermined by a model that is viewed as having room for more cost efficiency, as
well as not having a clear articulation of purpose. In comparison to other ILSAs, SEA-PLM is seen as
not doing well in “marketing” itself internationally as a worthy investment. However, it is important to
nuance this perspective in recognition of SEA-PLM as “the baby” (the newest) regional assessment and
the early stage of implementation for the programme.
5.4.2 What are the key barriers and bottlenecks towards achieving sustainability of SEA-
PLM activities?
The continued existence of SEA-PLM is dependent on maintaining existing engagement and
expanding to other countries in the region. It is unanimously agreed that the long-term legitimacy of
SEA-PLM as an initiative depends on maintaining or increasing the number of countries who participate.
There is a strong possibility that Myanmar will not participate in the next round, considering the ongoing
political situation in the country. This means that it will be necessary to include at least one more country
for the next round to maintain at least the same number of participating countries, something cited as a
priority for a number of key regional stakeholders. This will be a challenge for the secretariat. If not
achieved there is a risk of creating a vicious circle in which undermined legitimacy through a reduction
in the number of countries makes it harder to convince new countries or funders to come on board.
SEA-PLM needs to establish a clear narrative on what it provides and what differentiates it from
other ILSAs, and/or how it can leverage participation in other ILSAs. As mentioned in 5.4.1 there
is a sense among stakeholders that the narrative around SEA-PLM’s unique value is not clear. One
SEA-PLM stakeholder suggested that SEA-PLM is a programme which aims to create a narrative
around improving learning, space for discussions around the improvement of learning outcomes, and
data to describe learning outcomes, alongside building national and regional assessment capacity. This
is a clear articulation of SEA-PLM’s value as a regional ILSA. However, in analysis of documentation
used to market SEA-PLM (brochures, concept notes, minutes from launch meetings etc.) these
differentiations are not clearly articulated. While capacity building, support in generating political
28
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
narratives and policy solutions, and forums for regional collaboration are mentioned, they are not given
clear framing or structure. The absence of a concrete and clearly articulated service offering is a barrier
for achieving the sustainability of SEA-PLM activities.
For technical sustainability, the capacity of SEAMEO to provide technical and political support
on assessment is a limiting factor. It has been suggested by a range of stakeholders that a
sustainable solution for SEA-PLM would be to have a regional assessment technical agency, located
within SEAMEO. This would be equivalent to SEAMEO-INNOTECH (which is focused on innovation
and technology), and would be able to provide on-demand, cost-effective support to SEA-PLM
countries, both for SEA-PLM, as well as potentially across other assessment activities. While this is a
promising avenue, there will be challenges in its development. It will be difficult to attract technical
expertise in assessment needed to create such an agency without long term funding commitments.
5.4.3 How can SEA-PLM attract other countries in Southeast Asia and better link other
international and regional initiatives?
Reasons for not participating are complicated, including financial, technical and political
reasons. While the cost of participating is the most obvious barrier to entry, it is only one of the reasons
identified through this evaluation. The second is a technical issue. It is felt that countries were reluctant
to participate until they had seen the quality of data and outputs produced from the first round. The third
is a political issue, worsened by the COVID-19 pandemic. As outlined above, SEA-PLM needs to do
more to market itself as a solution to ministries of education. This solution needs to be more than just a
dataset and capacity building activities, but an articulation of how data can support the goals and
ambitions of ministries. This is made harder by the recognition that learning levels have almost certainly
suffered due to school closures in the last year. Governments considering participation may wish to
wait until a future round to allow time for recovery.
Currently, it is not clear that the SEA-PLM secretariat has the dedicated human resources to
address all these questions. The SEA-PLM secretariat is widely praised as having been the key
player in the delivery of the first round of data. It is however, a small team, with several team members
having significant responsibilities outside of SEA-PLM. 2021 and 2022 are crucial years for the
expansion and sustainability of the SEA-PLM programme. It is crucial that sufficient human resources
are dedicated to resolving some of the financial, technical and political reasons outlined above. As
mentioned above, it is the political narrative building, which is the key area, so it is important to ensure
that the team has the necessary level of political influence and understanding.
The economic and political consequences of the COVID-19 pandemic both complicate the
process of attracting countries to SEA-PLM, as well as emphasizing the importance of the
programme. As mentioned above, there is a potential risk that countries may not wish to engage in a
new comparative assessment at a time when they know that learning levels have suffered. However,
this also presents an opportunity for SEAMEO and UNICEF. SEA-PLM 2019 was the last data set
collected in the region before 2020. This is a unique position from which to monitor recovery of learning
levels from the pandemic. This opportunity is widely recognised but should be clearly articulated and
taken into account both in the process of attracting donors and countries, but also in looking at how the
assessment is designed and delivered.
There are numerous opportunities for better networking for SEA-PLM both regionally and
internationally. SEA-PLM follows in a strong tradition of regional learning assessments. In particular
PASEC12, SACMEQ13 and the LLECE14 have to varying degrees, and for varying lengths of time,
delivered regional learning assessments. There is an opportunity for SEA-PLM to establish, as part of
a revised approach to technical assistance, networks with experts involved in the delivery of these
assessments across participating countries. While this would not necessarily “localise” technical
assistance, it would promote south-south cooperation. This approach could be replicated for sharing
12 Programme d’Analyse des Systèmes Educatifs de la CONFEMEN
13 Southern Africa Consortium for the Measurement of Education Quality
14 Laboratorio Lantinoamericano de Evaluacíon de la Calidad de la Educacíon
29
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
insights on policy linking across regions. Within the region, both NEQMAP and the GPE KIX East Asia
Pacific Regional Hub are active in learning assessments in SEA-PLM participating countries. At a
minimum these forums should be used to amplify messages from SEA-PLM. Pathways for deeper
collaboration would be to use both networks to leverage training resources and/or experts to support
participating countries.
5.5 Equity and gender equality
Table 7 Summary of SEA-PLM’s support to the most marginalized
Strength
Area of evidence Assessment of the
evidence
Equity and inclusion has been There is little evidence of equity being formally mainstreamed within SEA- Strong.
mainstreamed into programme PLM programme activities
activities
Data will be used to inform There is evidence across countries that SEA-PLM data will be used to Strong.
equity decisions explore inequities in learning.
5.5.1 To what extent is SEA-PLM conducive to supporting the most marginalized
populations and genders (including those furthest left-behind)?
The SEA-PLM assessment includes contextual questionnaires for learners, teachers and
parents. The purpose of the included questions was to allow the exploration of correlations between
learning and individual, home and school characteristics. Some of the content of these questionnaires,
as reported in the main regional report in 2019, is shown in Table 88.
Table 8 Contextual questionnaire topics
Questionnaire Topics
Child background, home • Gender
influence and school • Age
experience • Socioeconomic status (SES)
• combined gender, school location and socioeconomic status
• Preschool education
• School readiness in language and mathematics
• Speaking the language of instruction at home
• Grade repetition.
School Environment and • School size
teacher profiles • School location
• Access to textbooks and library
• Teachers’ preparation and specialization.
Children’s, parents’, • Children’s attitudes about School
teachers’ and Schools • Parental engagement in children’s learning
principles attitudes • Perception of issues affecting children’s learning in the classroom
• School principal’s perception of issues that hinder their school’s
capacity to provide instruction
• Teachers’ perception of issues that hinder their ability to provide
instruction
Beyond the contextual questionnaires there is little evidence of equity being formally
mainstreamed within SEA-PLM programme activities. Equity is mentioned in a number of SEA-PLM
design documents but is never given an explicit strategy as to how SEA-PLM will actively address
questions of equity in learning outcomes. Similarly, there is not an explicit strategy to support ministries
of education in developing secondary analysis to address equity questions. In the country reports that
30
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
have been published there is equity-based analysis (as in the regional report), but this is not aligned to
a formalised equity strategy. What is also missing is a country specific equity strategy, which would
have informed both the exact content of the contextual questionnaires, as well as the approach to
sampling (e.g., oversampling minority groups to allow for between group comparisons). This was done
in some cases, for example in Malaysia where the MoE requested that the sample take into
consideration the need for analysis between the three major language groups. This was done, but the
numbers of schools sampled did not allow for comparison between schools within language groups.
What was absent from the planning process was an equity-focused sampling strategy across all
countries. In the case of Malaysia, it was driven by the MoE, rather than being a standard programme
offering. For GPE member countries, the most recent sector analysis can serve as useful guides to
collecting data on learning equity.
There is evidence that countries are intending to use SEA-PLM learning data to answer equity
questions. Through the three case studies, it is clear that equity in learning outcomes is a strategic
focus, and that to a certain extent, SEA-PLM data is being used to further these conversations. While
it is too early to say conclusively, a potential limitation of these conversations will be the lack of country
specific contextual data that would align to national strategic objectives. At the regional level, there are
ongoing efforts to use SEA-PLM data to answer equity questions, including secondary reporting on
gender differences in learning across the region, and the World Bank’s use of SEA-PLM data for looking
at learning poverty.
31
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
6 Conclusions and lessons learned
SEA-PLM has emerged from the priorities of SEAMEO and UNICEF EAPRO in 2012 to a
regionally endorsed regional learning assessment programme, with a successful first cycle in
2021. A significant amount of work has taken place to develop the SEA-PLM concept, present the
concept in High Level Meetings in the region, establish and develop working groups, develop a
cooperation agreement, establish a Steering Committee, develop an assessment framework, conduct
field trials, have SEA-PLM included in the ASEAN Work Plan on Education and successfully deliver
valid, reliable and comparable data across six countries in SEA.
This has taken place despite human and material resource constraints and is a commendable
achievement for all the stakeholders involved. This has also been achieved in spite of the significant
disruptions caused by the COVID-19 pandemic. There is evidence that SEA-PLM is relevant to the
needs of participating countries. Increasing the amount of data available on learning is a priority in
most of the countries that participated in SEA-PLM for the first round. SEA-PLM builds well on this
prioritisation of learning data. What is less clear is what exactly the relevance of SEA-PLM is, whether
as a uniquely regional learning assessment, or as a compliment to other ILSAs operating in the region.
The validity of the SEA-PLM Theory of Change from inputs to outputs is strong but can be
improved. The strongest components of the ToC are those which have focused on data collection, the
development of the assessment framework, and technical capacity building activities (see green marks
in Figure 7). These activities have led to reliable, valid, relevant and rigorous learning and contextual
data, a regional common learning metric and alignment with the global UIS learning outcomes scale.
The strong delivery of technical outputs is necessary but not sufficient to deliver on SEA-PLM
intended outcomes and impact. Less-strong components of the ToC include activities intended to
result in a strategy for analysing and disseminating tools and funding agreements with donors and
participant countries. These weaker aspects of the ToC may potentially limit the realisation of outcomes
and impact in the future.
There are improvements to be made to ensure SEA-PLM can both continue to generate data on
learning and to ensure this data is used to inform decision-making. As has been highlighted within
this report, a vision for the future of SEA-PLM is needed with associated strategies to see this vision
come to fruition.
Overall, the success of SEA-PLM hinges on the ability of the programme to move from data
delivery to planning for and supporting the uptake of evidence to inform decision-making. While
capacity building leading to the generation of data has been achieved, a stronger focus on the utilization
of data is now needed. This is crucial as the only avenue for SEA-PLM to achieve impact is through the
uptake of evidence generated by SEA-PLM at the country level.
In addition, a more sustainable delivery model for SEA-PLM is needed to ensure the programme
remains affordable for countries and attracts additional regional funding to support the
programme in the future. This evaluation has shown that, while the current arrangement has delivered
successes, it is widely considered not to be sustainable. There are differing views on what the final
structure and technical assistance arrangements for SEA-PLM should be.
SEA-PLM is entering a pivotal phase in 2022. Moving into the second round of data collection, SEA-
PLM has left the “proof of concept” phase. The programme has proved that it can generate data that is
robust and trusted by participating and observing governments. Now it must build on this and present
a long-term strategic vision. Recovery from school closures places extra importance on SEA-PLM data,
which can now play a role in helping governments measure the impact of school closures on learning
and can play a key role in the recovery process for the region.
32
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Figure 7 Assessment of the validity of the SEA-PLM Theory of Change after Round One
33
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
7 Recommendations
There is an important role for SEA-PLM to play through the next round, amplified by the impacts
of the COVID-19 Pandemic. SEA-PLM 2019 was the last learning data collected in many of the
participating countries pre-pandemic, and therefore can provide valuable insights into learning loss
through the next round.
Based on the conclusions of this evaluation we propose a number of recommendations to be
considered for the next round and beyond. Recommendations are divided between three strategic
areas related to; i) building of regional assessment capacity; ii) strengthening the link between data and
policy formation and; iii) establishing SEA-PLM’s regional presence. Within each of these areas we
have used an investment/impact matrix (detailed in Box 5.) to help differentiate between
recommendations to be considered for the next round (2022-2025), and those which should be
considered as part of the long term vision for the programme.
The recommendations are aimed at the SEA-PLM programme as a whole, though specific
responsibilities may lie with a number of different actors. The primary recipient of these
recommendations will be the SEA-PLM secretariat (consisting currently of UNICEF and SEAMEO).
However, actioning these recommendations may rely on inputs from other actors involved in the
delivery of the programme, including participating country governments, technical suppliers, and other
partners.
Box 5 Categorising and prioritising recommendations
The recommendations from this evaluation are designed to be
easily actionable in support of strengthening the next round of
SEA-PLM data collection. They are also intended to inform the
longer-term development of the programme. To simplify the
process, we have categorised recommendations based on their
potential impact on the programme, and the level of investment
needed to implement them. This has resulted in three categories
of recommendations:
Quick Wins: These are changes that will not have a
transformational impact on achieving the programmes objectives
but will also not require significant investment. They should be implemented, but not in isolation.
Rather they should be included as part of a suite of changes.
Unmissable opportunities: These are changes to the programme which will not take large
investments but will have a large positive impact on the programme. They are un-missable
opportunities and should be prioritised.
Long Term Visions: These changes will take large, long-term investment, but will be
transformational for the programme. They may not be immediate priorities but should be
considered as part of long-term planning.
There is a fourth quadrant of the matrix; those changes best avoided. We do not include any here
as they involve a large investment for a modest impact.
34
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
7.1 Building Long Term Sustainable Capacity for Assessment in Participating
Countries
Unmissable opportunities
Recommendation 1.1: Work with countries to develop long term capacity objectives
and needs
The findings of this evaluation show that in some cases there is a mismatch between the kind of capacity
building that technical teams in participating ministries of education want, and what was made available.
This can be partly explained by a one-fits-all approach to what training was provided. For the next
round, SEA-PLM should work individually with countries to define roles and responsibilities and capacity
gaps. This will allow countries to contribute their technical capacity where it already exists. It will also
create a long-term sustainability plan. While it is unlikely that ministries of education would ever take
responsibility for all SEA-PLM tasks, each should set their own ambitions as to which skills they would
like to have in-house, and which they are happy to have externally available.
Long term visions
Recommendation 1.2: Develop a bank of sustainable and transferrable capacity
development materials
This is an interim goal towards establishing a regional technical support hub (Recommendation 3.3).
While the first round of SEA-PLM has established a strong and valuable dataset, it produced little in the
way of transferrable training or capacity building materials. In addition to continuing and refining face-
to-face capacity building activities SEA-PLM should take the opportunity to build a library of resources
that can be applied beyond those involved directly in training. This would be a strong public good for
the region and would also reduce the need for re-training due to staff turnover in technical teams.
7.2 Strengthening the links between SEA-PLM data and policy decisions
Quick Wins
Recommendation 2.1: Plan strategically for the communication, uptake and use of
data by policy makers.
The Kingdon’s Multiple Streams Approach (MSA) provides a useful framework for thinking about how
policy change windows occur. The MSA includes a problem stream, a policy stream and a politics
stream. When these three streams intersect a window for meaningful policy change opens. Learning
assessment data most commonly influences the problem stream. This is seen from the first round of
SEA-PLM data, with all countries recognising challenges with learning. What assessment data on its
own cannot necessarily influence are the policy and politics streams.
This framing can be used to inform SEA-PLM planning in two ways:
4. The theory of change for the next round of SEA-PLM should include activities which explicitly
seek to influence all three streams towards opening policy windows.
5. The SEA-PLM secretariat and technical partners should work with participating countries from
the outset to ensure that SEA-PLM data, contextual questionnaires, dissemination plans and
secondary analysis plans can work to bring the three strands together.
Using this planning would allow for SEA-PLM to support political communications at the national level,
ensuring that findings from the assessment are useful in identifying problems, designing policies, and
importantly in establishing a political narrative around the improvement of learning outcomes.
35
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Figure 8 Overview of Kingdon's Multiple Streams Approach
Unmissable opportunities
Recommendation 2.2: Mainstream equity considerations into SEA-PLM design
decisions
Equity considerations should be tailored to each country and driven by (1) an education sector analysis
concerning equity (usually available for GPE member countries) and (2) the research questions that
Ministries of Education have regarding inequity. Key considerations regarding what is known and
unknown about the learning gap between different groups or the marginalisation of certain groups
should be identified and SEA-PLM’s potential contribution to the evidence discussed with education
ministries. Sampling and contextual questionnaire decisions should be informed by the equity focus
agreed. Mainstreaming equity into the design and communications around SEA-PLM will be particularly
important in the next round, where school closures are likely to have exacerbated existing inequalities
in learning outcomes. It will be crucial that the data produced is suitable for examining these inequities
in learning outcomes.
7.3 Building SEA-PLM as a sustainable regional body
The final set of recommendations relate to the structure and purpose of SEA-PLM as a regional body.
There is an opportunity for SEA-PLM to establish itself as more than a regional metric, but also as
technical support hub for assessment across the region, as well as a networking and collaboration
organisation, providing a space for discussions on education quality and learning data. In the short
term this means reforming the approach to technical assistance and focusing on collaboration and
networking. In the long term there is an opportunity to establish a dedicated technical agency at the
regional level.
Unmissable opportunities
Recommendation 3.1: Establish network of technical assistance individuals and
agencies to support technical teams
In addition to, or as an eventual replacement for, a sole technical assistance supplier, SEA-PLM
should see itself as having a coordinating and networking function for technical assistance. The
secretariat (supported in the medium term by a coordinating partner) should develop a database of
experts in assessment and other related skills (communication, training, policy development and
planning). This database would focus on experts and institutions (academic or otherwise) from
participating countries, as well as countries in the region and global experts. This would allow for both
the secretariat and participating countries to take advantage of a call down service for specific
36
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
support. This would potentially be a more cost-effective approach to technical assistance, as well as
promoting and supporting technical resources within the region.
Recommendation 3.2: Develop a collaboration and networking strategy
In addition to an external communications strategy, the strategy for the next round of SEA-PLM should
explicitly focus on networking and collaboration. Specifically, SEA-PLM should look to:
a. Strengthen collaborations in countries, particularly with Academia and the private sector. This
should focus on generating additional secondary analysis, as well as building the policy and
political narrative around SEA-PLM data.
b. Facilitate increased cooperation between participating countries. The results of the country case
studies for this evaluation highlights that there are missed opportunities to facilitate knowledge
sharing on the use of assessment data between countries. This is a valuable chance to create a
learning focused policy community in the region.
c. Strengthen networks within the region. SEA-PLM should be explicit on how it will support, and be
supported by other projects, organisations and networks working on learning assessment.
d. Foster global collaboration. There are other learning assessments with similar purposes and scope
being implemented in other regions of the world. Fostering collaboration with these organisations,
or affiliated organisations is a way that SEA-PLM can draw on, and contribute to global expertise,
and South-South collaboration.
Long-term visions
Recommendation 3.3: Plan strategically for the establishment of SEA-PLM as a
regional technical hub on learning assessment and education quality.
In the long-term, SEA-PLM should consider the development of a regional technical agency, housed
within SEAMEO. This agency would provide technical inputs for SEA-PLM but would also support
other assessment activities across the region for participating and non-participating countries. This
would mirror the work done by SEAMEO-INNOTECH in supporting innovation and technology in
education. While this recommendation may not be immediately feasible to deliver upon, it should be
explicitly considered as part of the articulation of SEA-PLM’s long-term vision. Establishment of this
technical capacity at the regional level would allow SEA-PLM to work towards an aim of being more
that a metric, and would allow the programme
37
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
8 Bibliography
ACER. (n.d.). National Target Audiences.
ACER, SEAMEO & UNICEF. (2017). Southeast Asia Primary Learning Metrics (SEA-PLM)
Assessment Framework. Melbourne: ACER.
Ahmad, A. e. (2015). A critical analysis of the evolution of the Southeast Asia Primary Learning Metric
(SEA-PLM). Learning for sustainable futures: Making the connections. Oxford: UKFIET.
Best, M., Knight, P., Lietz, P., Lockwood, C., Nugroho, D., & Tobin, M. (2013). The impactof national
and international assessment programmes on education policy, particularly policies regarding
resource allocation and teaching and learning paractices in developing countries. London:
EEPI-Centre, University of London.
DFAT. (2020a). DFAT Cambodia. Retrieved from DFAT :
https://www.dfat.gov.au/geo/cambodia/Pages/cambodia-country-brief
DFAT. (2020b). Overview of Australia's aid program to Myanman. Retrieved from DFAT:
https://www.dfat.gov.au/geo/myanmar/development-assistance/Pages/development-
assistance-in-myanmar
Global Partnership for Education . (2020e). Where we work. Myanmar. Retrieved from Global
Partnership for Education: https://www.globalpartnership.org/where-we-work/myanmar
Global Partnership for Education. (2019). ANLAS in Vietnam.
Global Partnership for Education. (2020a). Where we work. Cambodia. Retrieved May 2020, from
Global Partnership for Education: https://www.globalpartnership.org/where-we-
work/cambodia
Global Partnership for Education. (2020b). GPE's expanding role to improve education in the Asia
Pacific. Retrieved from Global Partnership for Education:
https://www.globalpartnership.org/blog/gpes-expanding-role-improve-education-asia-pacific
Global Partnership for Education. (2020c). Where we work. Lao PDR. Retrieved from Global
Partnership for Education : https://www.globalpartnership.org/where-we-work/lao-pdr
Global Partnership for Education. (2020d). Where we work. Vietnam. Retrieved from Global
Partnership for Education: https://www.globalpartnership.org/where-we-work/vietnam
Global Partnership for Education. (2021, October 22). Cambodia Partner Country information.
Retrieved from GPE Partner Country Information: https://www.globalpartnership.org/where-
we-work/cambodia
Government of Myanmar. (n.d.). National Education Strategic Plan 2016-2021.
Iyer, P., Azubuike, O. B., & Rolleston, C. (2017). Young Lives School Survey 2016-2017. Oxford:
Young Lives.
Marivin, A. (2019). Matrix analysing funding needs and priorities in UNICDEF and COs and roadmap
to support COs, particularly GPE countries.
Monteiro, A. R. (2010). The righ tof the child to enducation: what right to what education? Procedia
Social and Behavoiral Sciences, 1988-1992.
Mullis, I. V., Martin, M. O., Foy, P., & Hooper, M. (2016). Retrieved from TIMSS 2015 International
Results in Mathematics. Retrieved from Boston College, TIMSS & PIRLS International Study
Center website: http://timssandpirls.bc.edu/timss2015/international-results/
38
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
n.p. (2015, July 16). South East Asia Primary Learning Metrics background, rationale and programme
logic. South East Asia Primary Learning Metrics background, rationale and programme logic.
n.p.
OECD. (2020). OECD-DAC. Retrieved from Receipts:
https://public.tableau.com/views/OECDDACAidataglancebyrecipient_new/Recipients?:embed
=y&:display_count=yes&:showTabs=y&:toolbar=no?&:showVizHome=no
OXFAM. (2019). Financing for Development in Vietnam. Oxford: Oxfam.
Pawson, R., & Tilley, N. (1997). Realistic Evaluation. London: Sage.
SEAMEO and UNICEF. (2019b, October 10). Meet the new Technical Advisory Group for SEA-PLM.
Retrieved from SEA-PLM website:
https://www.seaplm.org/index.php?option=com_k2&view=item&id=15:meet-the-new-
technical-advisory-group-for-sea-plm
SEA-PLM. (2015). Southeast Asian Primary learning Metrics. Background, Rationale and Programme
Logic. SEA-PLM.
SEA-PLM. (2019a). SEA-PLM 2019 Technical report.
SEA-PLM. (2019b). Target Audience Matrix.
SEA-PLM SEAMEO UNICEF . (2019a). Regional and national activity plan for supporting analysis,
reporting, dissemination and use of results of SEA-PLM.
SEA-PLM SEAMEO UNICEF. (2019b). Communication plan for the SEA-PLM Secretariat to
disseminate and support the use of results, methodology and data at regional and national
level for SEA-PLM 2019.
Spink, J. (2019). Policy Priorities and Possibiliites for Data Analysis. SEA-PLM.
The ASEAN Secretariat. (2016). Initiative for ASEAN Integration (IAI)Work Plan III. Jakarta.
UNESC0 Asia and Pacific Regional Bureau for Education. (2018, June 15). Better and more
information on factors influencing learning across Southeast Asia. Retrieved from UNESCO
Bangkok: https://bangkok.unesco.org/content/better-and-more-information-factors-
influencing-learning-across-southeast-asia
UNICEF. (2007). A Human Rights-Based Approach to Education for All. New York: UNICEF.
UNICEF. (2019). Final SEA-PLM Hand-over notes.
UNICEF. (2019). Terms of Reference for an Evaluation of the SEA-PLM Programme in Southeast
Asia. Bangkok: UNICEF Bangkok.
UNICEF and SEAMEO. (2019a). SEA-PLM 2019 Assessment Framework (Ist ed.). Bangkok,
Thailand: United Nations Chikdren's Fund (UNICEF) & Southeast Asian Ministers of
Education Organization (SEAMEO) - SEA-PLM Sercretariat.
UNICEF EAPRO. (2020, May 6). Education: Every child has the right to go to school and learn.
Retrieved from UNICEF ESAPRO Education Programme: https://www.unicef.org/eap/what-
we-do/education
UNICEF EAPRO. (n.d.). Terms of Reference for SEA-PLM Advisor. Bangkok: UNICEF EAPRO.
UNICEF EAPRO. (n.d.). TOR for SEAPLM Team - UNICEF and SEAMEO Secretariat.
Universalia, Itad and R4D. (2020). GPE Country-level Evaluations - Final Synthesis Report.
Washington: Global Partnership for Education.
39
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Annex A Terms of Reference
TERMS OF REFERENCE FOR INSTITUTIONAL CONTRACTS.
UNICEF Bangkok
Requesting Section: EDUCATION/EVALUTION SECTION, EAPRO
TITLE: Terms of Reference for an Evaluation of the South East Asia Primary Learning Metrics
(SEA-PLM) Programme in Southeast Asia
1- Background and rationale
The Regional Context in Southeast Asia
UNICEF East Asia and the Pacific Regional Office (EAPRO) is supporting a regional large-scale
learning assessment with the Southeast Asian Ministers of Education Organization (SEAMEO) since
2012. The Southeast Asia Primary Learning Metrics (SEA-PLM) is the first regional learning
assessment at primary level (Grade 5) in Southeast Asia – www.seaplm.org.
As an alternative to international large-scale assessments initiatives (OCDE-PISA, IEA-PIRLS,
TIMSS) and national tools (EGRA, national assessments) already used in most of the Southeast Asian
countries as part of their system-level mechanisms to generate robust learning data on national
representative sample thanks to standardized testing. SEA-PLM is designed as a regional Programme
of cyclic assessments to measure quality and equity trends in education over time, sub-population and
countries by repeating cycles. This regional initiative is a step forward for measuring, accelerating and
tracking progress over SDG 4.1.1 (b) and 4.7.4 by defining and reporting collected data on common
scales, levels and learning descriptors.
SEA-PLM collects learning data on national representative samples of students as required by the
Global Partnership in Education and the Unesco Institute for Statistics for reporting learning progress.
SEA-PLM is integrated into international mapping as one of the active Regional Large-Scale
Assessments (ERCE, PASEC, SEA-PLM, SACMEQ, PILNA). Regional assessments are considered
by the international community as critical tools to track SDG 4.1 targets over time thanks to their
nationally representative samples and reliable competencies scales.
The implementation of each cycle of the assessment is framed to follow a generic framework and
timeline of 4-5 years standardized with participating countries. The SEA-PLM first round is branded
SEA-PLM 2019 in reference to the data collection year and is having an average duration period of 5-
6 years (2015-2020/2021 for SEA-PLM 2019). Six15 (6) of the eleven16 (11) Southeast Asia countries
have decided to implement SEA-PLM 2019 survey, however, all Southeast Asia countries are involved
in the Programme governance structure, methodology development and policy exchange. Those
countries may join the next round of the assessment. The initial methodological framework and
procedures and SEA-PLM Programme inception have been developed between 2012 and 2015 with the
contribution of all countries.
15Cambodia, Malaysia, Myanmar, Lao PDR, Philippines and Vietnam.
16 Brunei Darussalam, Cambodia, Indonesia, Lao PDR, Malaysia, Myanmar, Philippines, Singapore, Thailand, Timor-Leste and Vietnam.
40
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
GPE countries use results from Regional Large-Scale Assessment to report national progress in learning
outcomes, actors’ practices, expectation and attitudes, and evaluate project impact on targeted groups
within specific domains and context. Such assessments are used as a strong component of Education
Sector Plans to monitor, plan, design and refine policies and strategy documentation related to
curriculum, assessment, resources allocation, equity, workforce development and accountability.
SEA-PLM focuses also on supporting Southeast Asia countries by developing a network and
community of practices, mentorship and peer-learning between experts and countries to better measure
and understand the status of student learning achievement and thereby improve the quality of their
education systems. The regional collaborative framework offers the possibility to build individual and
collective capacities around learning policies and assessment through SEAMEO and ASEAN agenda.
SEA-PLM expects to achieve three key outcomes including:
1. Enhanced capacity to generate and analyse assessment data at regional, national and sub-
national levels.
2. Enhanced capacity to utilize assessment data for education improvement and more equitable
learning outcomes at regional, national and sub-national levels
3. Enhanced ASEAN integration in terms of approaches to assessment, with an initial focus on
primary Grade 5 in the domains of numeracy (mathematics), literacy (reading & writing), and
global citizenship.
The initiative gathers multiple actors and stakeholders from the regional and national level on time-
consuming timelines, standardized in-countries survey activities, high-level expertise for methodology
development and quantitative/qualitative research, political commitment and transformation of results
in educational policies and practices. The Programme is also engaged in international and regional
learning policy dialogue on SDGs, learning policies and large-scale assessment area.
The direct beneficiaries of the assessments and policy exchanges are the Ministry of Education officials,
the Education policymakers and sector planners. The development partners, researchers in education
and technical institutions get also the opportunity to take benefit from quality data, capacity building in
assessment and improved standards across the region. At bottom level, all current and future children
studying at primary level and teachers in the region will benefit from the future education reforms that
are informed by SEA-PLM findings for improving learning opportunities and professional practices.
ASEAN, SEAMEO and other regional and international organizations will also benefit from increased
coordination, alignment and increasing Southeast integration at regional and international level as
Global Alliance for Monitoring Learning (GAML).
UNICEF EAPRO and UNICEF CO are supporting SEAMEO Secretariat and participating ministries
in their Programme participation by supporting the institutional development of the Programme,
mobilizing external expertise to undertake and control technical operations and contribute to funding
national and regional operation for training, data collection, communication and human resources. The
day to day management, reporting and decision making is jointly managed by UNICEF EAPRO and
SEAMEO Secretariat through different bodies and actors at the regional and national level.
Time to take stock: Context for the external evaluation
With the coming implementation of the last stages of the first round of the first regional large-scale
assessment in Southeast Asian Region, the Programme is going to deliver SEA-PLM 2019 by reporting,
disseminating and supporting the use of results in 2020 and 2021. In the meantime, the Programme is
continuing to transform this approach toward a long-term Programme.
The necessity of exploring strength and weakness of the first assessment on a governance, technical,
managerial, funding and partnership perspectives, is necessary before engaging in the second round
41
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
of the assessment and switching from a biannual plan and commitment to a 5 years approach to support
a complete round of assessment and increase the stability, visibility and ownership of the project.
As one of the Co-Chair of the Secretariat of SEA-PLM and main donors of the assessment, UNICEF
wants to commission an independent evaluation of this intervention.
The evaluation will focus both on (i) the inception phase of the assessment (2012-2015) and (ii) the
implementation of the first round of the assessment (SEA-PLM 2019). The evaluation primary focus
will be on SEA-PLM 2015 to 2019. SEA-PLM has the potential to organize three rounds of assessment
before 2030 SDG’s.
2- Purpose, Objectives & Scope
The main purpose of this independent evaluation is to draw lessons learned and account for results to
inform the next phase of the SEA-PLM Programme. As such it combines a formative and summative
focus and intends to inform future Programme planning and re-positioning. To do so, this evaluation
will assess the relevance, effectiveness, efficiency, impact and sustainability as well as equity, gender
equality and human rights considerations of the SEA-PLM Programme. It will also consider these
criteria in relation to issues including national coordination, leadership and institutionalization
according to national priorities and global frameworks. The evaluation will draw evidence-based
findings and recommendations which will identify both emerging good practices and new regional
strategic approaches, programmatic components and intervention modalities needed to better position
UNICEF and its partners in a fast-changing EAP regional context in support of national governments
in achieving their national goals and targets and contribute to the SDG agenda.
The primary audience of this evaluation is UNICEF EAPRO, SEAMEO, the SEA-PLM Secretariat,
Country Office Management teams and Education teams; the secondary audience includes
governments’ development partner, donors and external stakeholders as contractors and other
collaborators. The findings, conclusions, and recommendations will be used by both the regional and
country offices to inform the development or implementation of their country Programme documents
and adjust their strategies and implementation modalities as necessary.
At the regional level, it will inform the design and the implementation of a SEA-PLM long-term strategy
and multi-annual activity plan as the regional learning Strategy of the UNICEF regional office, and as
well as the regional policy advocacy and partnerships leveraging efforts. Findings of the evaluation will
also be disseminated to other UNICEF Regional Offices and UNICEF Headquarters.
The objectives of this evaluation are to:
1. Review the relevance, effectiveness, efficiency, impact, the sustainability of the SEA-PLM
Programme and assess the extent to which it has been implemented in line with the objectives and
global good practices on regional learning assessments;
2. At regional and national level, identify what strategic approaches, programmatic components and
intervention modalities can help better position UNICEF and collaborators for SEA-PLM
Programme and much broader in a fast-changing EAP region and global initiatives and in support
of country programming in achieving the SDG agenda.
This evaluation will also strengthen and update the knowledge and evidence base on SEA-PLM in
complementarity with other initiatives recently completed, on-going or recently launched by UNICEF
in the region such as the literature review of what works to improve learning in the region.
42
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Evaluation Scope
• The evaluation will be retrospective (summative) and cover (secondary) the conception period
of SEA-PLM (2012-2015) and (primarily) the implementation of SEA-PLM 2019 from 2015
to 2019, but it will be forward-looking (formative) in providing conclusions and
recommendations for regional- and country-level programming.
• Time boundaries: from 2012-2019 with a focus of the current implementation period of SEA-
PLM 2019 (2015-2019).
• Organizational level: UNICEF country offices covers by SEA-PLM, UNICEF regional office
and the Regional Secretariat and a focus of SEA-PLM 2019 participating countries and
consultation of non-participating countries.
• The geographic scope of the assignment will cover six (6) Southeast Asian countries
participating in SEA-PLM 2019: Cambodia, Lao PDR, Malaysia, Myanmar, Philippines, and
Vietnam. The desk review will cover eleven (11) SEA countries.
• Type of interventions: both upstream (enabling environment and system strengthening work as
learning and/or assessment policies planning, implementing and monitoring) and downstream
programming strategies (enabling local environment and actors as learners, teachers, head
teachers and community in developing capacities and new opportunities for teaching and
learning.
Evaluation framework and questions
Evaluation evidence will drawn against the Organisation for Economic Co-operation and Development,
Development Assistance Committee’s criteria of relevance, effectiveness, efficiency, sustainability and
impact These criteria are prioritized because they capture the evaluation questions presented below. In
addition, the evaluation will review equity and gender as cross-cutting issues.
Key evaluation questions (and sub-questions) are clustered according to the evaluation criteria
provided. This initial list of questions will be further refined and unfolded by the evaluation team and
included in the Inception Report following desk review of key documents.
Relevance of SEA-PLM to the priorities and policies of national education stakeholders in the eleven
(11) Southeast Asian countries
1. To what extent is SEA-PLM aligned to i) the national development policies and priorities of
national education stakeholders and ii) UNICEF’s national, regional and global objectives and
intended impacts?
2. What are the programming gaps or unaddressed needs? What could be done better?
Effectiveness of SEA-PLM to intending outcomes in the six (6) SEA-PLM 2019 participating
countries
3. To what extent have the objectives and expected outcomes of SEA-PLM been achieved or are
likely to be achieved?
4. What have been the major factors influencing the achievement or non-achievement of SEA-
PLM objectives and activities? What were the enabling factors, barriers and bottlenecks?
5. What can UNICEF and its partners do to ensure the objectives of SEA-PLM are met in the
future? What kind of initiatives should UNICEF prioritize at the country and regional?
Efficiency of the management of SEA-PLM and timely use of resources at country and regional levels
in the six (6) SEA-PLM 2019 participating countries
6. How well have SEA-PLM activities been managed by UNICEF in terms of the technical and
financial resources? Have SEA-PLM activities been implemented in the most cost-efficient
way compared to alternative approaches?
43
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
7. What were the strengths and weaknesses of SEA-PLM management processes? How could
management of SEA-PLM activities be improved? To what extent and how did the UNICEF
Regional Office contribute to efficiency of SEA-PLM in education programming?
Impact resulting from the SEA-PLM Programme at this stage of its implementation (positive and
negative changes, intended and unintended) in the six (6) SEA-PLM 2019 participating countries
8. How has SEA-PLM contributed to national education systems and assessment practices and
policies and discourses thus far? And at the regional level, what positive and negative changes
has SEA-PLM brought about?
Sustainability of SEA-PLM and benefits
9. To what extent can SEA-PLM activities, plans and strategies be fully integrated and
implemented by the government (s), both technically and financially? To what extent are they
likely to continue?
10. What are the key barriers and bottlenecks towards achieving sustainability of SEA-PLM
activities?
11. How can SEA-PLM attract other countries in Southeast Asia and better link other international
and regional initiatives?
Equity and gender equality considerations
12. To what extent is SEA-PLM conducive to supporting the most marginalized populations and
genders (including those furthest left-behind)?
These questions can be further refined by the evaluation team and included in the Inception Report
following desk review of key documents.
3- Evaluation approach and methodology
The primary task of the consultant is to undertake an evaluation of the SEA-PLM Programme to date
and provide formative guidance to inform the future developments of the Programme.
It is expected that the evaluation will employ a non-experimental design and use both a theory-based
(re-constructing the theory of change), utilization-focused, participatory and a mixed-methods approach
drawing on key background documents, monitoring frameworks at country and regional levels and
interviews with key informants. Key documents, data and a contact list of relevant informants will be
provided to the evaluation team once a contractual agreement has been made. Documentation will
include methodological materials, ToR and meeting minutes of bodies of governance, activity plans,
letter of commitment and political statement as an example.
The methodology will primarily use mixed data collection methods include:
• Brief Literature review of emerging good practice concerning regional and global large-scale
learning assessments;
• Desk review of key SEA-PLM Programme documents;
• Key informant interviews with SEA-PLM technical team, SEAMEO, UNICEF staff at
regional and global levels, and key counterparts at national, regional and global level
considering the eleven (11) SEA countries; and,
• Six (6) Case studies covering SEA-PLM 2019 participating countries
A limitation to the evaluation is the timeframe. Because the results of SEA-PLM 2019 will only be
released in 2020, the evaluation will not be able to assess impact of results on children and education
systems. Rather it will take a more formative approach, looking at intermediate outputs and outcomes.
The applicants should discuss the above or other potential limitations in their proposal.
44
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Based on the above-mentioned purpose and scope of the evaluation, the bidder is expected to submit a
proposal for methodologies including data sources and selection criteria among regional and country
programming strategies as well as the set of key evaluation questions. Methodological rigor will be
given significant consideration in the assessment of proposals. Hence bidders are invited to interrogate
the approach and methodology preferred in the ToR and improve on it, or propose an approach they
deem more appropriate. In their proposal, bidder should clearly refer to triangulation, sampling plan
and methodological limitations and mitigation measures. Bidders are encouraged to also demonstrate
methodological expertise in large-scale assessment.
Country visits are envisaged to cover six countries case studies.
The evaluation will occur in three phases, which are further described below: 1) Inception phase, 2)
Data collection and analysis phase, and 3) Reporting and communication phase. The following offers
guidance on the evaluation process and UNICEF’s expectations and thinking. It should be commented
on, further developed and improved by the bidders in their respective proposals. Alternative approaches
can also be proposed. The methodology will be further specified and finalized by the selected evaluation
team in collaboration with UNICEF during the inception phase.
Phase 1. Inception Phase
Step 1. Literature search and review. The evaluation team will review relevant data and analyses
from national, regional and global frameworks to understand the context in SEA-PLM were and are
operating, and research and practical evidence on successful approaches to regional and global large-
scale learning assessment relating to national framework to monitor learning. The literature review
should form the basis of the background and context section of the evaluation.
Step 2. Desk review. The evaluation team will review key SEA-PLM background documents from the
eleven countries and the regional level, including assessment framework, technical standards,
Secretariat strategy documents, monitoring reports, communication materials and field trial documents.
The aim of the desk review is to familiarize the consultants with SEA-PLM, to contribute to the
background and context sections of the report and to shape the methodology for the inception report.
Step 3. Preparation of Inception Report that includes evaluation methodology and tools. The
methodology should be prepared to cover all the intended objectives of the evaluation, including:
Summary of literature and desk review, reconstructed theory of change, evaluation criteria and
questions, methodology and data collection and analysis tools, foreseen limitations, list of priority
interviewees, workplan with timeline and bibliography. The evaluation methodology design and
evaluation questions will be finalized in agreement with the UNICEF Regional Office.
Phase 2. Data collection and analysis phase
Step 4. Data collection. The evaluation team will collect data using a mixed methods approach. Data
will be collected remotely from all the six countries. Field visits will be included in bidders’ proposals
for covering the six countries.
Step 5. Data analysis. The evaluation team will compile and analyze data. The methodology used
should be clearly described in the final report.
Step 6. Case studies and preliminary findings. The evaluation team should prepare a case study of
about 5 pages for each country participating in SEA-PLM to document the evolution of SEA-PLM in
the country and its current role and contributions to the education sector as well as gaps, opportunities
and recommendations for maximizing impact and long term Programme sustainability.
Phase 3. Reporting and communication phase
45
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Step 7. Draft report. The consultant will prepare a draft report, with conclusions, lessons learned and
recommendations for the way forward drawn from the analysis. The report structure should follow the
UNICEF-Adapted UNEG Evaluation Report Standards (2017).
Step 8. Validation workshop with UNICEF, evaluation reference group, and other stakeholders
(validation workshop if applicable, virtual meetings) to validate findings, conclusions and prioritize
recommendations.
Step 9. Finalisation of the evaluation report. The consultant will present the final draft evaluation
conclusions and recommendations to SEAMEO and UNICEF RO, using a PowerPoint presentation
and/or other methodologies. Comments and feedback on the findings and recommendations should be
incorporated to finalise the report.
The evaluation methodology should follow the UNICEF Evaluation Policy (2018), the United Nations
Evaluation Group (UNEG) Norms and Standards for Evaluation (2016), UN SWAP Evaluation
Performance Indicator, UNEG Ethical Guidelines (2008), UNICEF Procedure for Ethical Standards
and Research, Evaluation and Data Collection and Analysis (2015) (Report must be compliant with the
UNICEF-Adapted UNEG Evaluation Report Standards (2017))
4- Deliverables and payment schedule
No. Tentative Deliverable Percent
Deadline payment
Deliverable March 2020 Phase 1 10%
1
Inception Report and presentation, including:
• Literature review
• Desk Review
• Methodology and data collection and analysis
tools including sampling strategy
• Foreseen limitation and mitigation measures
• Workplan with timeline
• Annotated Bibliography
• Presentation to SEAMEO and UNICEF EAPRO
(remotely)
Deliverable May-June 2020 Phase 2 10%
2
Case Study Reports
• Key stakeholder interviews and country visits
• Case studies in six countries (5 pages each)
46
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Deliverable August 2020 Phase 3 30%
3
Draft Evaluation Report, including the following
content in draft form:
• Executive summary
• Literature review (from inception phase)
• Synthesis of desk review (from inception phase)
• Final methodology including sampling strategy
• Analysis of key findings for each evaluation
question
• Conclusions and Lessons Learned
• Recommendations, country specific and regional
• Bibliography (from inception phase)
Validation Workshop of preliminary findings,
conclusions and prioritization of recommendations from
draft report.
Deliverable November 2020 Final Evaluation Report, Brief, Video and 50%
4 Presentation, including the following content:
• Executive summary
• Literature review (from inception phase)
• Synthesis of desk review and mapping of country
level activities (from inception phase)
• Final methodology
• Analysis of key findings for each evaluation
question
• Conclusions and Lessons Learned
• Recommendations, country specific and regional
• Annotated bibliography (from inception phase)
• A 4-page brief intended for a broader, non-
technical audiences
• A PPT and video for UNICEF and SEAMEO to
use for general audiences
*Deadlines and timelines are subject to change.
5- Reporting requirements
The deliverables listed above should include the reporting requirements, as described in the table above.
The final report should follow UNICEF guidelines and be no more than 40 pages (or 12,000 words
excluding annexes) and should include an executive summary. The final report should also include a
summary PPT presentation.
In addition, regular progress reports should be provided either via email or phone to the UNICEF RO
team, as requested.
All deliverables must be in professional level standard English and in compliance with UNICEF Style
Book 2019 and UNICEF Brand Toolkit 2012. They must be language-edited / proof-read by a native
speaker.
All reports will be in Microsoft Office Word format while all presentations will be in Microsoft Office
PowerPoint. No PDF or hard copy will be submitted by the evaluation team. No page limit is set but all
47
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
deliverables should be of the necessary length but not longer, and should be reader friendly. The use of
bullet points, tables, graphs, photos, and other visualization methods is encouraged. The use of annexes
is required for the evaluation tools, for all secondary information that is not directly related to the
evaluation findings, as well as for any long technical documentation intended to a specific audience.
PowerPoint presentations must include notes below each slide to make them easy to understand for
people who could not attend the meeting.
All data collected, documentation gathered, and photos/videos taken and analyses produced for the
purpose of the evaluation are to be made available to UNICEF in the appropriate format. Graphs and
maps must be in editable format for layout purposes. All key deliverables will be made available on the
UNICEF public website and widely disseminated to all target audience
6- Evaluation management and quality assurance
The evaluation will be conducted by an external evaluation team recruited by UNICEF EAPRO. The
evaluation team will be led by a team leader, whose role will be to:
- Oversee and manage the other team members;
- Manage communications with UNICEF;
- Ensure adequate workspace, work equipment, accommodation, international and in-country travel,
food, security and insurance arrangement to all evaluation team members;
- Orient and train team members where applicable;
- Organize and facilitate key meetings and workshops;
- Ensure that deadlines are met and all draft and final evaluation products and deliverables are of the
required quality as per UNICEF standards (format and content).
- If necessary, the team leader will mobilize a dedicated quality assurance person for additional
quality review.
- All draft and final deliverables submitted by the evaluation team leader will be accompanied by a
detailed comment matrix describing whether and how earlier comments received have been
incorporated, and when they have not been fully included, providing an appropriate justification.
The evaluation team will operate under the guidance and supervision of an evaluation management
team led by the Regional Evaluation Advisor and the Regional Evaluation Consultant with support
from the SEA-PLM Project Manager and the Regional Education Advisor/Specialist and the Reference
Group. The evaluation management team will be responsible for the contractual aspects, day-to-day
oversight and management of the evaluation as well as evaluation budget. They will facilitate the
communications with the country offices, the reference group members (see below), and other relevant
stakeholders. They will be also responsible for the quality of the evaluation, and provide the first round
of comments to the evaluation team before submission of the revised draft to the reference group. They
will check whether the findings and conclusions from the evaluation are relevant and recommendations
are implementable, and propose improvements to the recommendations. They will approve all
deliverables and payments. In addition, the team will contribute for dissemination of the evaluation
findings and to follow-up on the evaluation recommendations with a management response.
The role of the country office Education and Evaluation teams will be the following:
- Designate a focal point for supporting the evaluation,
- Provide the evaluation team with all information, resource documents and contacts necessary for
the evaluation,
- Facilitate the communication and coordination between the evaluation team and UNICEF’s
implementing partners in-country,
- Organise / facilitate logistics, security, meetings and workshops; any related costs will be part of
the regional evaluation and not from county office budgets,
- Provide comments on the key deliverables to minimize factual errors, misinterpretations, and
omissions
48
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
UNICEF will provide quality assurance on all evaluation tools and documents based on the UNEG’s
and UNICEF’s norms, standards, and guidelines as well as on other best practices related to Programme
evaluations. Once approved, the final evaluation report will be submitted to the UNICEF’s global
evaluation reports oversight system (GEROS) for an independent quality rating. The report and the
review will be made available on the UNICEF Internet website, in compliance with the commitment
for transparency of evaluation findings.
Evaluation Reference Group
A reference group will be established with the following people and led by the evaluation management
team.
o Regional Education Advisor, EAPRO, UNICEF
o Education Specialist, EAPRO, UNICEF
o Regional Evaluation Advisor, EAPRO, UNICEF
o Chiefs of Education section, UNICEF COs in EAPR (3 from case study countries, 2 from other
countries)
o Education Specialist, HQ, UNICEF
o Representatives from development partners (donors: Korea, Australia, NZ, ASEAN, JPE, WB,
ADB, OECD)
o SEA-PLM Secretariat representatives
o Three (3) SEA-PLM 2019 participating countries
o Two (2) SEA-PLM 2019 non-participating countries
Additional members may be identified during inception phase.
The reference group will have the following roles.
o Generally, advise the evaluation management team on various aspects of the evaluation and
help this team make decisions
o Contribute to the preparation and design of the evaluation
o Provide feedback and comments on the second draft of the inception report and on the technical
quality of the work of the consultants
o Assist in identifying internal and external stakeholders to be consulted during the evaluation
process
o Participate in review meetings organized by the evaluation management team
o Provide comments and substantive feedback from a technical point of view to ensure the quality
of the second draft and final evaluation reports
o Propose improvements/inputs to the preliminary recommendations
o Play a key role in learning and knowledge sharing from the evaluation results
o Contribute to disseminate the findings of the evaluation
o Advise on the management response to the evaluation, and follow up when appropriate
7- Location and Duration
The indicative start date of the assignment will be 15 December 2019. The proposed institutional
contract will have a duration of one year months, with all work being carried out between December
2019 and November 2020. The consultant will work remotely from home, with travel as agreed and
two travels to Bangkok to present the inception report and the final report recommendations.
8- Qualification requirements and required experience
The core evaluation team may be comprised of 3 experts, while UNICEF is flexible in the team’s
composition so long as the evaluation work is of high quality. A gender balanced and culturally diverse
team composition is strongly encouraged.
49
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
The consulting company must provide details of qualification and a work profile of the team leader who
should meet the following required minimum conditions:
• Have proven experience (minimum 8 years of experience) in the use of participatory,
qualitative and quantitative evaluation/analytic methodologies with experience in conducting
UN evaluations;
• Have a minimum of MA/MSc level in Education, Social Sciences, Public Policy Analysis or
other relevant fields related to Education. Experience of at least 5 years of work in the
consultation domain;
• Evidence/proof of experience in a similar field;
• Strong communication (written and oral) and networking skills;
• Have proven competencies in research, facilitation skills, good communication, excellent
English language and writing skills.
• Knowledge of Education in the East Asia and Pacific region is an asset.
Other team members should have the following qualifications:
• Advanced degree in related field with relevant work experience.
• Minimum 10-15 years relevant professional experience, ideally in education and large-scale
assessment.
• Previous experience working in issues relating to education, large-scale assessment and/or
related fields.
• Strong experience in research, program management, monitoring and reporting.
• Excellent interpersonal skills and previous experience communicating with partners at different
levels.
• Demonstrated ability in work planning and report preparation.
• Demonstrated skills in professional high-quality writing in English.
• Experience working in the East Asia Pacific region highly desirable.
• Excellent communication skills in English.
• Experience working for a United Nations organization an asset.
50
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Annex B Access, quality and equity of education in SEA
B.1 Access and equity across the region
Increased enrolment, retention and completion rates and decreased gender gaps have been
observed within some countries in the region, however many children remain out of school (OOS) or
lack access to quality education (UNICEF EAPRO, 2020).
Across the eleven countries in Southeast Asia, recent progress in improving access, quality and
equity varies. As can be seen in Table 9, (based on available UIS data) Brunei Darussalam’s
progress is constrained across many indicators, Cambodia is making positive progress in reducing
overage enrolment and gender equity, and Indonesia is making positive process in reducing the
percentage of Out of School Children (OOSC). In Indonesia primary school life expectancy has
improved for boys, while secondary school life expectancy has improved for girls. Laos is making
positive progress in reducing overage and underage enrolment, increasing access to Early Childhood
Development (ECD), gender parity in primary enrolment and secondary school life expectancy.
Malaysia has reduced overage enrolment and primary school life expectancy for boys, with
improvements for girls remaining stagnant or small. Myanmar has made advancements in reducing
the rate of OOSC, increasing school life expectancy at the primary and secondary levels and
improving the graduation ratio at the primary level. The Philippines has reduced overage enrolment
and improved secondary school life expectancy. Thailand has made improvements in access, school
life expectancy at the primary and secondary levels, reduced repetition in primary school and
graduation ratio at the primary level. Timor-Leste has reduced the rate of OOSC, improved gender
parity in enrolments at the primary level, improved school life expectancy at the secondary level,
reduced the rate of primary repetition and improved primary graduation ratios. Based on the available
data, Vietnam has improved across the full range of indicators, with the expectations of reducing
underage enrolment, school life expectancy for primary boys and improving gross graduation ratios
(where indicators have remained reasonably stable).
Gross graduation ratios remain reasonably stable in Brunei Darussalam, Indonesia, Malaysia, the
Philippines and Vietnam because starting ratios were near or at 100 percent in 2013.
There is very little comparable data available for these indicators in Singapore.
51
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Table 9 Trends across access, enrolment, schooling late expectancy, repetition and graduation indicators (2013 to 2019), using stoplight rating17
y ra m irp fo C S O O fo e ta R s e x e s h to b ,e g a lo o h c s )% ( y ra m irp fo C S O O fo e ta R )% ( s lrig ,e g a lo o h c s y ra m irp fo C S O O fo e ta R )% ( s y o b ,e g a lo o h c s s tn e d u ts fo e g a tn e c re P y ra m irp n i d e llo rn e re v o e ra o h w n o ita c u d e - )% ( s e x e s h to b ,e g a s tn e d u ts fo e g a tn e c re P y ra m irp n i d e llo rn e re d n u e ra o h w n o ita c u d e - )% ( s e x e s h to b ,e g a ,o ita r tn e m lo rn e s s o rG d o o h d lih c y lra e tn e m p o le v e d la n o ita c u d e s e x e s h to b ,s e m m a rg o rp )% ( ,o ita r tn e m lo rn e s s o rG o h d lih c y lr a d e o tn e m p o le v e d la n o ita c u d e )% ( s lrig ,s e m m a rg o rp ,o ita r tn e m lo rn e s s o rG d o o h d lih c y lra e tn e m p o le v e d la n o ita c u d e )% ( s y o b ,s e m m a rg o rp tn e m lo rn e y ra m irp s s o rG )% ( s e x e s h to b ,o ita r ,o ita r tn e m lo rn e s s o rG y tira p re d n e g ,y ra m irp x e d n i ,y c n a tc e p x e e fil lo o h c S )s ra e y ( e la m e f ,y ra m p ir ,y c n a tc e p x e e fil lo o h c S )s ra e y ( e la m ,y ra m irp ,y c n a tc e p x e e fil lo o h c S )s ra e y ( e la m e f ,y ra d n o c e s ,y c n a tc e p x e e fil lo o h c S )s ra e y ( e la m ,y ra d n o c e s s re ta e p e r fo e g a tn e c re P lla ,n o ita c u d e y ra m irp n i )% ( s e d a rg o ita r n o ita u d a rg s s o rG n o ita c u d e y ra m m o ir r p f )% (
Brunei
Darussalam
Cambodia
Indonesia
Laos
Malaysia
Myanmar
Philippines
Singapore
Thailand
Timor-Leste
Vietnam
17 Green stoplight rating denotes improvements, yellow denotes stagnation or small movements in trends and red denotes deterioration. Grey reflects an absence of data. Source UIS data 2013-2020.
52
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
B.2 Learning assessment programmes and Southeast Asia
Underscoring the impetus behind SEA-PLM, data on learning outcomes are not available for many
countries in the region, particularly in reading and writing. Of the eleven countries, only Singapore has
participated in PIRLS and Malaysia, Singapore and Thailand have participated in the Grade 8 TIMMS
(Mullis, Martin, Foy, & Hooper, 2016).
As can be seen in Figure 9 and Figure 10, average proficiency scores in Singapore (in both Maths
and Science) have sat between the High and Advanced International Benchmarks since 1995.
Thailand’s average proficiency scores have sat below or near the Intermediate International
Benchmark for both domains and Malaysia’s average proficiency scores have declined and then
improved both above and below the Intermediate International benchmark, since 1999.
Figure 9 Trends in Grade 8 Maths Proficiency, TIMMS
649
599 609 604 605 611 621
593 Malaysia
e
r
o
c s 549 Singapore
e
la
c 519 Thailand
S 508
s 499
h ta
474
465 Advanced International
M 467 Benchmark
440
449 High Intenational Benchmark
441
427 431
Intermediate International
399 Benchmark
1995 1999 2003 2007 2011 2015 Low International Benchmark
Year
Figure 10 Trends in Grade 8 Science Proficiency, TIMMS
649
599 Malaysia
590 597
e 580 578
r o 568 567 Singapore
c
S 549
e
la Thailand
c
S
e 492 510
c n 499 Advanced International
e
ic S 482 447711 471 Benchmark
456 High Intenational Benchmark
449 451
426 Intermediate International
399 Benchmark
1995 1999 2003 2007 2011 2015 Low International Benchmark
Year
Gender differences in proficiency can be observed in Malaysia, Singapore and Thailand (TIMMS
2015) in Figures 11 and 12. Girls perform higher than boys in each country in Maths and perform
higher in Science in Malaysia and Thailand, while boys perform higher in Science in Singapore.
53
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Figure 11 Average Proficiency in Maths, by Gender (TIMMS 2015)
626 616
600
500 470 461 440
422
400
300
200
100
0
Malaysia Singapore Thailand
Girls Boys
Figure 12 Average Proficiency in Science, by Gender (TIMMS 2015)
586 597
600
500 476 466 465 445
400
300
200
100
0
Malaysia Singapore Thailand
Girls Boys
The higher performance of girls in maths is also observable in Vietnam, reflected in the Young Lives
Secondary School wave one and wave two survey results. However, larger inequalities exist between
districts, ethnic group and, for some measures, urban and rural divide (Iyer, Azubuike, & Rolleston,
2017, p. 31).18 This highlights that forms of inequality beyond gender are also important to understand
how disadvantage takes hold across contexts.
B.3 Education System Quality Indicators in SEA-PLM participating countries
Of these eleven countries, six countries have participated in SEA-PLM, including Cambodia, the
People’s Democratic Republic of Lao, Malaysia, Myanmar, Philippines and Vietnam.
These six education systems are diverse in terms of size, quality and equity (Figures 13-17). The
primary school age population in PDR Lao is just over 700,000, over 7 million in Vietnam, and roughly
2 million in Cambodia. The pupil-teacher ratio in primary education ranges from approximately 12 in
Malaysia to approximately 43 in Cambodia. Similarly, approximately 93 percent of schools in Malaysia
have basic handwashing facilities, while in Cambodia just under half of all schools have these
facilities. For other education quality indicators, the countries diverge less. For example, the
percentage of trained teachers across the countries are reasonably similar, as is the Gender Parity
Index for primary school enrolment.
18 This was also observable for English and Transferable Skills performance and for wave 1 and wave 2.
54
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Figure 15 Percentage of trained primary teachers21
Figure 13 Primary school age population (in millions) 201919
100.0 97.5 99.7 100.0 99.8
7.70305 100
8 7.369125
90
7
80
6 70
5 4.64197 60
50
4
40
3
2.031602 30
2 20
1 10
0
0
Cambodia Lao People's Malaysia Philippines Viet Nam
Cambodia Lao People's Myanmar Viet Nam
Democratic
Democratic
Republic
Republic
Figure 16 Proportion of primary schools with basic handwashing
Figure 14 Pupil-teacher ratio in primary education20
facilities22
45
99.3
40
100.0
35
30 80.0
25
20 60.0 48.8 49.5
15
40.0
10
5 20.0
0
Cambodia Lao Malaysia Myanmar Philippines Viet Nam 0.0
People's Cambodia Malaysia Philippines
Democratic
Republic
19 Source UIS 21 Source UIS
20 Source UIS 22 Source UIS
55
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Figure 17 Primary Gross enrolment ratio, Primary, Gender parity Index23
1.04
1.02
1.00
0.98
0.96
0.94
0.92
0.90
Cambodia Lao People's Malaysia Myanmar Philippines Viet Nam
Democratic
Republic
B.4 Access, retention, survival and equity in SEA-PLM participating
countries
Across the six SEA-PLM participating countries, primary school life expectancy tends to be
higher for boys than for girls (Figure 188), yet survival rates to the last grade of primary
education is higher for girls (Figure 199). This indicates a high rate of repetition for boys,
which can be observed in Error! Reference source not found.0.
Figure 18 Primary school life expectancy, by gender 201724
8.0
7.0
6.0
5.0
4.0
3.0
2.0
1.0
0.0
Cambodia Lao People's Malaysia Myanmar Philippines Viet Nam
Democratic
Republic
Girls Boys
23 Source UIS
24 Source UIS
56
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Figure 19 Survival rate to the last grade of primary education, by gender25
97.3 95.5 95.2
100 90.7
90 78.8 82.6 79.7
80 73.9
70
60
50
40
30
20
10
0
Cambodia Lao People's Malaysia Philippines
Democratic Republic
Girls Boys
Figure 20 Number of repeaters across primary grades, by gender26
139,794
140,000
120,000
100,000 85,001
80,000
58,306
50,713
60,000 44,220 Girls
40,000
20,610 18,919
13,427 Boys
20,000
0
Cambodia Lao People's Philippines Viet Nam
Democratic
Republic
In Myanmar, Philippines, PDR Lao and Cambodia more than ten percent of the students
enrolled in primary school are overage for their grade (Figure 21).
25 Source UIS
26 Source UIS
57
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Figure 21 Percent of enrolled primary school students who are overage and
underage27
14.4
14.0 12.7
12.0 10.4 9.8
10.0
8.0
6.0
3.0 Overage
4.0
1.3 1.4
2.0 0.0 0.2 0.1 Underage
0.0
Cambodia Lao People's Malaysia Myanmar Philippines Viet Nam
Democratic
Republic
27 Source UIS
58
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Annex C SEA-PLM logframe
Programme Impact ASEAN and SEAMEO Member countries have an enhanced understanding of factors affecting learning achievements in primary
education and take actions to ensure that all children achieve meaningful learning outcomes.
Programme Enhanced capacity to generate and analyse assessment data at regional, national and sub-national levels
Outcome 1
Outputs Indicator Assumptions/Risks
Output 1.1: National officials have necessary
capacity to conduct SEA/PLM surveys
Activity areas 2015- 2016:
Assessment tools developed, tested, and finalised for Assessment tools ready for use in main survey Process at country level includes key
use in national surveys experts and decision-makers
Trained staff will be utilised in actual
Key staff have required competences to carry out survey
Key staff trained at regional and national levels in item survey exercise.
effectively
development, test design and administration
Output 1.2: SEA/PLM surveys meet expected
standards of efficiency and effectiveness
Activity areas 2015-2016: Instruments field tested in at least 6 countries (2 rounds of field Funding is sufficient to ensure that
testing planned) national surveys are carried out
Field testing of SEA/PLM instruments
efficiently and effectively.
National surveys completed in at least 6 countries
Activity areas 2017:
Final reports meet global standards for assessments
National surveys completed
59
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Programme Impact ASEAN and SEAMEO Member countries have an enhanced understanding of factors affecting learning achievements in primary
education and take actions to ensure that all children achieve meaningful learning outcomes.
Programme Enhanced capacity to utilise assessment data for education improvement and more equitable learning outcomes at regional,
Outcome 2 national and sub-national levels
Outputs Indicator Assumptions/Risks
Output 2.1: Future national sector policies and SEA/PLM data cited in sector policies and plans
plans respond to SEA/PLM findings
Activity areas 2015 - 2016: High level political buy in at Ministerial
level to accept, share and utilise survey
Country visits by SEAMEO, ACER and UNICEF to # advocacy visits to potential SEA/PLM countries and visit
findings
explain how SEA/PLM can contribute to enhanced reports as shared with SEA/PLM Steering Committee
learning and equity. Inclusion of all key stakeholders in the
# and quality of workshops
planning and dissemination processes
Dissemination workshops at national level with key
stakeholders Timing of SEA/PLM surveys in relation
# and quality of policy briefs to education reform opportunities
Policy briefs developed based on SEA/PLM field
trial findings
Output 2.2: Regional analyses on progress in SEA/PLM data cited in regional analyses of education
education include data from SEA/PLM surveys
Activity areas 2017: Regional education co-ordination
structure for Post 2015 remains strong
Regional meetings to discuss and compare # of meetings and quality of recommendations
and harmonious
SEA/PLM findings across Member Countries
Regional analyses must not lead to
Regional analytical briefs developed.
# of briefs developed and evidence of impact negative competition and league tables
that shame low performers but to
collaborative approaches to enhancing
learning for all children
60
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Programme Enhanced ASEAN integration in the education sector in terms of common approaches to assessment and enhancement of learning,
Outcome 3 quality and equity
Outputs Indicator Assumptions/Risks
Output 3.1: SEA/PLM endorsed as regional priority # of states agreeing to engage in SEA/PLM increases
by most ASEAN and SEAMEO Member Countries (Baseline. July 2015: 4)
Activity areas 2015 -2016:
Regional SEA/PLM meetings hosted by SEAMES
# of regional meetings on SEA/PLM/Assessment SEAMES advocacy with member states
include advocacy around regional assessment
will be critical
Advocacy for SEA/PLM during SEAMES regular
Some countries may resist due to
meetings and visits abroad #number of SEAMEO meetings and visits where SEA/PLM
commitments to other surveys
was presented
Communication strategy for SEA/PLM, including
brochure and web portal, developed and shared # of people using new portal and
across all SEAMEO Member Countries
#of brochures distributed
Regional SEA/PLM country communication
package
# and quality of adapted country communication documents
Advocacy with ASEAN to ensure inclusion of
SEA/PLM in ASEAN workplan SEA/PLM cited in ASEAN plan
ASEAN yet to engage with SEA/PLM as
Resource mobilisation for SEA/PLM through # of new donors
regional priority.
SEAMEO and ASEAN structures and through
Level of funding raised
engagement of additional donors Need ensure SEA/PLM is not seen to be
Level of national investment in SEA/PLM donor driven
Activity areas 2016 - 2017
Donor resistant to investing in less
Review of 2015 field trials shared at key meetings
tangible areas
Key findings enhance survey design
Formative external evaluation of SEA/PLM progress
and impact Key findings used to shape development of SEA/PLM
61
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Annex D Reconstructed ToC evidence
D.1 Impact
Various documents refer to the impact of SEA-PLM, but often statements combine
outcomes and impact statements. For example the SEA-PLM (2015) Background,
Rationale and Programme Logic states that the desired programme impact is: “Countries
from the Association of Southeast Asian Nations (ASEAN) and the Southeast Asian
Ministers of Education Organization (SEAMEO) have an enhanced understanding of
factors affecting learning achievements in primary education and take actions to ensure
that all children achieve meaningful learning outcomes” (SEA-PLM, 2015, p. 4).
The same document refers to a vision, stating “SEA/PLM will contribute towards improving
and redefining learning outcomes by providing a regional culturally appropriate metrics and
thereby towards a more equitable and meaningful education for all children across the
region”.
In the 2019 SEA-PLM Communication Strategy the overall goal of SEA-PLM is expressed
as “by 2025, all Southeast Asia countries have regional standardized quality measures to
assess learning outcomes for grade 5 students in the domains of reading, writing, math
and global citizenship and consistently use these to inform improvement within the
education sector” (SEA-PLM SEAMEO UNICEF, 2019b, p. 16).
Both of these statements combine impact, outcomes and process. However, taken
together, the three phrases confirm that the desired impact of SEA-PLM is:
Improved, and more equitable, relevant student learning outcomes across
Southeast Asia.
D.2 Longer term and intermediate Outcomes
The ToC mapping included in the Communications Strategy (2019) indicates that the three
goals of SEA-PLM are:
● Enhanced capacity to generate and analyse assessment data at regional, national and
sub-national levels
● Meaningful utilization of assessment data for education improvement and more
equitable learning outcomes; and
● Enhanced ASEAN integration in terms of approaches to assessment.
When SEA-PLM was initiated in 2012, its key objectives were to develop a regional
learning metric to allow for a common approach to assessing learning outcomes at age 10
or Grade 5, which countries would adopt over time across the Southeast Asia region. It
was assumed that the initiative would take shape through the combination of SEAMEO’s
political leadership, technical input from an expert contractor (ACER), and programmatic
support of UNICEF, in conjunction with SEAMEO member countries (Ahmad, 2015, p. 4).
Similarly, the SEA-PLM Assessment Framework (2017) states that the ‘SEA-PLM initiative
has three core goals:
● To provide policy makers with relevant, sound and comparable data on contextual and
learning outcomes that can directly inform local education policy development.
● To develop indicators of educational outcomes that enable meaningful comparisons of
quality.
● To enhance the existing capacities of participating countries to design data collection
activities that will assist all aspects of the policy cycle: to develop and implement a
62
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
reliable, valid and rigorous survey-based assessment and reporting program; and to
appropriately analyse, interpret and disseminate assessment data with a view to
informing education policy through relevant evidence.’ (UNICEF and SEAMEO, 2019a,
p. 5)
Other desired outcomes include utilizing the data towards policy priorities. These include
informing teaching practices, informing curriculum scope and sequence and informing
community based programs (Spink, 2019). The intended use for the data is explicated in
Spink (2019); “Assessment results must help us understand where children are at, so we
can best understand where we want children to move to next. Regional and global
reporting requires us to compare but we must also be able to understand if we are to make
a change.”
The desire for data to be used for decision-making is also reflected in UNICEF’s global
Education Strategy 2019-2030. The Strategy highlights systems strengthening in
partnership with governments and also that ‘the generation and use of data and evidence
will be (further) enhanced, particularly related to levels of learning, to generate a better
understanding of the children being left behind, and the effectiveness of education systems
in meeting the learning needs of every child’. (UNICEF, n.d.)
These goals and outcomes expressed within the documentation cover outputs,
intermediate outcomes and longer-term outcomes. In order to use the ToC for evaluation
purposes, the logical results chain must link longer term outcomes to impact and
intermediate outcomes to longer term outcomes etc. For this reason, it is important that the
sequencing of the logical chain follows the theory of how change occurs. For example, to
link longer term outcomes to impact, the longer-term outcome would need to be able to be
directly linked to how improved, and more equitable, relevant student learning outcomes
across Southeast Asia would take place. “Enhanced ASEAN integration in terms of
approaches to assessment” is not an outcome that can logically directly lead to
improvements in learning outcomes. For this reason, the outcomes and goals referred to in
SEA-PLM documentation have been reordered in the reconstruction, to take account for
the need for logical sequencing along the results chain.
The longer-term outcomes of SEA-PLM, as expressed in the documentation, and the
mechanism by which impact can take place is:
Sustainable, effective, efficient, responsive and more equitable education
systems at the regional and national levels, able to meet the needs of all
children, including those left behind across Southeast Asia
Many of the objectives referred to in the documentation are theorised to lead to the longer-
term outcome of sustainable, effective, efficient, response and more equitable education
systems. Therefore, the intermediate outcomes of SEA-PLM are:
1. Policy makers use data to meaningfully support decision-making;
2. Enhanced capacity to generate and analyse assessment data at regional, national
and sub-national levels;
3. Sustainable regional integration of relevant large-scale assessment and outcomes;
4. Enhanced ASEAN integration in terms of assessment; and
5. SEA-PLM is on the regional agenda and national government support is re-
enforced.
D.3 Outputs
There are many specific outputs within the SEA-PLM programme that span the technical,
political, logistical and partnership aspects of the programme. Within the ToC the broad
63
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
and most important outputs have been identified, as referenced in the SEA-PLM
conference paper (Ahmad, 2015), technical reports (SEA-PLM, 2019a), work plans (SEA-
PLM SEAMEO UNICEF , 2019a and ASEAN Secretariat, 2016), communication strategy
documentation (SEA-PLM SEAMEO UNICEF, 2019b), program rationale and logic
documents (SEA-PLM, 2015), ToR (UNICEF EAPRO and UNICEF, 2019) and the SEA-
PLM Assessment Framework (UNICEF and SEAMEO, 2019a).
These broad outputs are:
1. Reliable, valid, relevant and rigorous learning and contextual data;
2. A regional common learning metric;
3. SEA-PLM alignment to the global UIS learning outcomes scale;
4. Regional and national workshops and Learning and Development (L&D) products;
5. Strategies for analysing and disseminating data;
6. Common tools, protocols and standards to generate and utilize assessment data
across ASEAN countries; and
7. Funding agreements with donors and participant countries.
D.4 Activities
SEA-PLM ToC mapping documentation identifies four main areas of activities to achieve
SEA-PLM goals, including:
● Capacity building: to improve procedures and capacities at individual and institutional
levels for completing standardized assessment of grade 5 learner achievement
● Data production: to improve the common design of tools, protocols and standards to
generate and utilize assessment data across ASEAN Member States
● Information use: so, data can be analysed, understood, and used by skilled decision-
makers who take and promote evidence-based actions to improve education policy and
curricula
● Regional integration: to ensure sustainable regional integration of large scale
assessment to monitor achievements towards the ASEAN Community Blueprints and
SDG 4.
In many ways, these activities articulate four groups of activities and then describe how
they lead to outputs and outcomes. Therefore, in reviewing the range of SEA-PLM regional
documentation, the main activities were identified. These activities are presented in Error! R
eference source not found.. This information was used to categorise and articulate the
broad categories of activities for the ToC. These are:
1. Field studies and main survey data collection;
2. Assessment framework development, item development, sampling, analysis and
reporting;
3. Technical capacity building activities;
4. Develop and disseminate communications strategy;
5. Regional endorsement activities;
6. Fundraising with donors and countries.
64
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Table 10 SEA-PLM Activities
2012 2013 2014 2015 2016 2017 2018 2019 2020
Inception Research Formal collaboration Audit of all Vietnam, Malaysia First round of Planning and First in country Regional report of
paper on established between SEAMEO and the field trial budgeting for review of regional and cross-country
education SEAMES and countries curricula Philippines confirm results in main survey national activity plan for findings launched
curriculum s UNICEF for the completed their participation Brunei, 2020 prepared by SEA-PLM
realization of SEA- in the field trail Cambodia and Secretariat and reviewed by
PLM Laos the TAG
Working Research ACER contracted to Assessment Remaining Malaysia, Completion of Regional Steering Committee Technical
group paper on develop tools for Framework participating Philippines Governance in-person meeting to initiate delivery: Creation
established assessment regional assessment developed countries trained and Vietnam Analysis reflection and strategize on of one provisional
programmes several technical complete their a second version of regional regional database
areas field trials and national activity plan for with weighting.
2020.
Conceptuali Regional Brunei, Laos and Assessment Laos, Cambodia TOR and In-country consultation on One regional
zation Seminar held Cambodia signed up Framework and Myanmar Contracts for national activity plan for 2020 technical meeting
to agree key for field trail reviewed at complete the field implementation or webinar to
features and regional workshop trial of main survey share
scope of SEA- at national and psychometric
PLM regional levels conclusions
Formal launching Country visits from SEA-PLM is Regional Final validation of regional and Technical support:
ceremony with high- SEAMES and included in Consultation on national activity plans for one psychometric
level representation EAPRO to meet ASEAN 2016- next steps for 2020 comprising work plan, report
and endorsement of with High Officials 2020 Education SEA-PLM timeline, and budget
national governance to discuss SEA- Work Plan
structure PLM
Regional item Main Survey New communication strategy, Technical support:
development Orientation and including newsletter diffusion, one competency
workshop Sampling publication edition and first scale by domain
workshop held in step new branding and
6 countries program design for 2020
65
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
2012 2013 2014 2015 2016 2017 2018 2019 2020
Regional Code of Practice Implementation of the regional Technical support:
Orientation drafted work plan and nationals’ work one analysis plan
workshop (up-start plan in countries
of field trails)
Myanmar confirms National Regional report of cross- Technical support:
their participation Sampling design country findings launch (SEA- one provisional
in the field trial completed in 6 PLM SEAMEO UNICEF , outline for the
countries 2019a) main regional
report
Brunei D Key findings Technical support:
completes their from the one data analysis
field trial Governance user manual
Analysis
presented-
Regional SEA-
PLM
Consultation
Accuracy of life Technical support:
skills learning one
assessment methodological
workshop report.
66
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
D.5 Inputs
The inputs to SEA-PLM fit into four broad areas:
1. Technical Assistance;
2. Data collection costs;
3. Regional and country governmental human resources; and
4. UNICEF coordination.
The financial inputs for SEA-PLM are complex and not easily traced. However, several
documents made available for the Desk Study, provided some information on historical costs
and future budgetary figures.
Technical support contracted through the UNICEF Long Term Agreement (LTA) totalled US
$2,127,623.20 between February 2017 and December 2019 (UNICEF, 2019). Overall
institutional contracts established to date total just over $5 million (UNICEF, 2019).
Data collection costs are not easily identifiable as discreet costs; however, documentation
indicates that country implementation for field trials in 2018 totalled US $1.66 million across
Cambodia, Malaysia, Vietnam, Laos, Myanmar and the Philippines. This doesn’t include
technical support for field trails or other institutional contracts associated with field trails.
Regional costs for 2020 are currently costed at US $715,176 and country costs are US
$727,794. In addition, Secretariat staff costs (not counting existing staff) are estimated at $1.5
million over 2019-2021.
Of course, in addition to the direct costs associated with implementing SEA-PLM, human
resources make up a large contribution of inputs from across various partners. For example,
according to documentation from 2018, the SEAMES and UNICEF staff assigned to SEA-PLM
included a Deputy Director, a Finance Manager, a Programme Officer, a Programme
Coordinator, a Programme Assistant, a Regional Education Advisor, Education Specialist, a
Senior Expert and a Programme Manager (UNICEF EAPRO).
According to the documentation, the funding gap for 2020 is significant, at approximately 75
percent of the regional and national costs (not accounting for any further institutional contracts)
(Marivin, 2019).
D.6 Assumptions to be tested
Assumptions form the underlying beliefs about how a project will work, about the people
involved in the programme and about the stakeholders. Assumptions help to explain the ToC
process and the connections along the logical results chain. They help explain how and why
proposed activities are expected to bring the desired changes about. Assumptions are often
taken for granted and they may be based on opinions or beliefs.
Assumptions are often implicit but by stating them explicitly, the extent to which they hold can
be assessed. Where assumptions do not hold, the causal chain is at risk and a new pathway to
impact can be identified.
The following assumptions have been identified from inputs to activities, activities to outputs,
outputs to intermediate outcomes, intermediate outcomes to longer term outcomes and longer-
term outcomes to impact.
67
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
D.6.1 Inputs to activities
● Stakeholders have the opportunities (resources, time, conducive environment) to engage in
SEA-PLM activities;
● Stakeholders have the motivation and incentives to participate in SEA-PLM activities; and
● The Secretariat has sufficient leverage within SEA countries to influence SEA-PLM
participation and monitor SEA-PLM activities.
D.6.2 Activities to outputs
● Donors and participating countries have sufficient resources to invest in SEA-PLM and
consider SEA-PLM Value for Money (VfM);
● Technical and capacity building inputs are sufficient to result in the desired outputs;
● There is sufficient common ground between countries to find agreement on the analysis and
dissemination of SEA-PLM results;
● There is sufficient domain coverage, international and regional data and political will to
support a common regional and global metric; and
● The data generated by countries is of sufficient quality to support a regional common
learning metric.
D.6.3 Outputs to intermediate outcomes
● There is sufficient national capacity (technical capabilities, political will, resources) to analyse
available data;
● Policy makers have sufficient capacity, incentives and political space to access and utilize
data to inform decisions;
● The reach, fidelity and dose of capacity building activities are sufficient at national and
regional levels;
● The costs of SEA-PLM are sustainable during integration into regional systems and beyond;
and
● ASEAN countries see the value of a common approach to regional assessment and believe
SEA-PLM is sustainable.
D.6.4 Intermediate outcomes to longer term outcomes
● The use of SEA-PLM data leads to the improvements of previous shortcomings in relation to
relevance, equity and learning in the education system;
● Stakeholders have the capacity and incentives to solve education sector issues; and
● Stakeholders have the opportunities (resources, time and a conducive environment) to solve
education sector issues.
D.6.5 Longer term outcomes to impact
● Changes in the education system positively affect relevant learning outcomes and equity.
68
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Annex E Evaluation matrix
Evaluation Evaluation question Judgement criteria Indicators Research Source of Link to the
Criteria methodology information ToC
Relevance 1. To what extent is 1.1 Evidence of National stakeholder support for Desk Country policies, Context,
SEA-PLM aligned to national SEA-PLM research, UNICEF national, inputs and
i) the national development QCA regional and activities
Country and national institutional
development policies policies and global
needs against programme delivery
and priorities of priorities documentation,
national education alignment with UNICEF policy and programme Programme
stakeholders and ii) SEA-PLM documentation aligns with SEA- documentation,
UNICEF’s national, objectives PLM ToC KIIs
regional and global
objectives and 1.2 Evidence of There is sufficient domain
intended impacts? UNICEF global, coverage, international and
national and regional data and political will to
regional support a common regional and
objectives global metric
aligning with
SEA-PLM
intended impacts
2. What are the 2.1 Evidence of any Programme delivery is not on track Desk Programme Context,
programming gaps activities do not against work plans research, documentation, inputs,
or unaddressed work within the QCA KIIs activities
When delivery is on track,
needs? What could context/s and outputs
additional needs are identified by
be done better?
2.2 Evidence that stakeholders
needs (regional,
Programme delivery does not
national or sub-
cover the needs of specific
group), as
stakeholders or beneficiaries
identified by
stakeholders, are Programme activities do not lead
not covered by to expected outputs
SEA-PLM
Stakeholders have the
opportunities (resources, time,
conducive environment) to engage
in SEA-PLM activities
69
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Evaluation Evaluation question Judgement criteria Indicators Research Source of Link to the
Criteria methodology information ToC
Stakeholders have the motivation
and incentives to participate in
SEA-PLM activities
The reach, fidelity and dose of
capacity building activities are
sufficient at national and regional
levels
Effectiveness 3. To what extent have 3.1 Evidence that Data is on track to be available to QCA KIIs Outcomes
the objectives and policy makers are policy makers and impact
expected outcomes preparing to use
Policy makers have sufficient
of SEA-PLM been SEA-PLM data to
capacity, incentives and political
achieved or are likely inform decisions
space to access and utilize data to
to be achieved?
3.2 Evidence of inform decisions
improved regional
Policy makers are aware of when
and national
the data will be released and how
capacity to
it can be used
produce, analyse
and disseminate There is sufficient common ground
data between countries to find
agreement on the analysis and
3.3 Evidence
dissemination of SEA-PLM results
stakeholders are
aligning national The data generated by countries is
assessments with of sufficient quality to support a
SEA-PLM regional common learning metric
3.4 Evidence that There is sufficient national capacity
SEA-PLM is (technical capabilities, political will,
supported by
resources) to analyse available
national
data
governments and
is on the regional
agenda
70
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Evaluation Evaluation question Judgement criteria Indicators Research Source of Link to the
Criteria methodology information ToC
4. What have been the 4.1 Strength of Current programme Desk SEA-PLM Activities,
major factors evidence linking implementation status against research, programme outputs and
influencing the activities to workplans QCA documentation outcomes
achievement or non- outputs to
Progress against activities and KIIs
achievement of outcomes
outputs listed in the developed
SEA-PLM objectives
4.2 Evidence of ToC
and activities? What
triangulated
were the enabling Stakeholder perspectives on
perceptions
factors, barriers and programme activities, outputs and
regarding
bottlenecks? outcomes
enabling factors,
barriers and Secretariat leverage within SEA
bottlenecks from countries and influence re SEA-
key stakeholders PLM participation and SEA-PLM
activities
5. What can UNICEF 5.1 Evidence of A particular set of links in the ToC Desk SEA-PLM Complete
and its partners do particularly useful are stronger and able to be research, programme results
to ensure the mechanisms of replicated QCA documentation chain
objectives of SEA- change along the
A particular set of links in the ToC KIIs
PLM are met in the results chain
are weaker and require
future? What kind of
strengthening and an alternative
initiatives should
pathway to impact
UNICEF prioritize at
the country and
regional?
Efficiency 6. How well have SEA- 1.1 Evidence Value for Money principles are Desk SEA-PLM Inputs to
PLM activities been UNICEF technical embedded in SEA-PLM activities research, programme outputs
managed by and financial QCA documentation
Management structure and
UNICEF in terms of support has been
partnership engagement KIIs
the technical and provided on time
approaches support efficiency
financial resources? and within scope
Have SEA-PLM
1.2 Evidence of VfM
activities been
principles
implemented in the
embedded within
most cost-efficient
SEA-PLM
way compared to
activities
71
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Evaluation Evaluation question Judgement criteria Indicators Research Source of Link to the
Criteria methodology information ToC
alternative
approaches?
7. What were the 7.1 Evidence that Desk Cost data and Inputs to
Planned against actual costs
strengths and SEA-PLM inputs Research, unit prices outputs
weaknesses of SEA- were converted to Inputs were managed to achieve QCA
SEA-PLM
PLM management outputs in the
planned outputs within the programme
processes? How most efficient way
resource envelope documentation
could management possible
of SEA-PLM Planned activities led to planned Official
7.2 Evidence that
activities be outputs documentation
inputs could have
improved? To what on partner
been converted to
extent and how did engagement
outputs more
the UNICEF
Regional Office efficiently KIIs
contribute to 7.3 Evidence of
efficiency of SEA- triangulated
PLM in education stakeholder
programming? perspectives and
programme
documentation
regarding activity
and output
efficiency
Likely Impact 8. How has SEA-PLM 8.1 Evidence of Desk SEA-PLM Outputs to
The use of SEA-PLM data leads to
contributed to changes in the Research, programme outcomes
the improvements of previous
national education topics, focus and QCA documentation
systems and amount of shortcomings in relation to
Country
assessment education relevance, equity and learning in
engagement in
practices and dialogue at the the education system
regional and
policies and country and
Stakeholders have the capacity international
discourses thus far? regional levels
And at the regional and incentives to solve education activities
level, what positive 8.2 Evidence in sector issues Country
changes to
and negative education plans
national ASEAN countries see the value of
changes has SEA-
assessment a common approach to regional
PLM brought about?
practices
72
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Evaluation Evaluation question Judgement criteria Indicators Research Source of Link to the
Criteria methodology information ToC
assessment and believe SEA-PLM Education Sector
is sustainable. Analysis reports
Stakeholders have the KIIs
opportunities (resources, time and
a conducive environment) to solve
education sector issues
Sustainability 9.1 Evidence of Desk SEA-PLM Inputs to
9. To what extent Costs of participating in SEA-PLM
capacity Research, programme impact
can SEA-PLM against other education
expectations to QCA documentation
activities, implement future expenditure in country
Country
plans and rounds being Turnover of staff in participating
education
strategies be achievable within countries
budgets
TA budgets
fully integrated Technical and capacity building
KIIs
and 9.2 Evidence of inputs are sufficient to result in the
implemented national technical desired outputs
capacity to
by the The costs of SEA-PLM are
undertake
government expected sustainable during integration into
regional systems and beyond
(s), both technical tasks
technically and 9.3 Evidence of Donors and participating countries
financially? To funding have sufficient resources to invest
what extent availability and in SEA-PLM and consider SEA-
are they likely SEA-PLM PLM Value for Money (VfM)
affordability for
to continue?
future rounds
10.1 Evidence
10. What are the
countries do not
key barriers
plan to participate
and in the future
bottlenecks
10.2 Evidence of
towards
reasons for non-
achieving participation
sustainability
73
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Evaluation Evaluation question Judgement criteria Indicators Research Source of Link to the
Criteria methodology information ToC
of SEA-PLM
activities?
11.1 Evidence of ways N/A Desk Literature review N/A
11. How can SEA-
to overcome research,
PLM attract KIIs
barriers and QCA
other countries bottlenecks from
in Southeast other regional
Asia and better experiences
link other
international
and regional
initiatives?
Equity and 12. To what extent is 12.1 Evidence that Items are checked for differential Desk Programme Outputs to
gender equality SEA-PLM equity and performance (DIFF analysis) by research, documents impact
conducive to inclusion has gender or location QCA
KIIs
supporting the most been
Policy makers are considering
marginalized mainstreamed Data analysis
equity, inclusion and gender in
populations and into programme plans
their policy concerns
genders (including activities
those furthest left- SEA-PLM data is able to capture
12.2 Evidence the data
behind)? inequalities and countries know
will be used to
how to disaggregate analysis by
inform equity
social group
decisions
74
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Annex F Evaluation Framework and Methodology
F.1 Conceptual framework
We propose to undertake a non-experimental, theory-based Process Evaluation to answer the
evaluation questions. Therefore, the principal organising structure and framework for the
evaluation is the ToC, however we will follow the key functions of process evaluation.
Process Evaluation does not make use of a comparison group, seeking to make causal
inferences, but rather seeks to make credible claims about the SEA-PLM programme’s progress
along the inputs, activities, outputs and initial outcomes of the ToC. This is important during the
early stages of an intervention, to inform the final implementation model.
The process evaluation will be informed by Pawson and Tilley’s Realist Evaluation (1997).
Realist evaluation understands change as the results of actions of agents operating in a specific
context whereby the action leads to outcomes by triggering mechanisms. It emphasises that
mechanisms are contingent on the context and that outcomes are produced by the interactions
of context and mechanisms. Therefore, evaluations are based on Context-Mechanism-Outcome
configurations.
The aim of the process evaluation is to explain how SEA-PLM works and will examine the
processes through which the programme generates outcomes in different contexts. Figure 22
illustrates schematically key functions of process evaluation.
Figure 22 Key functions of process evaluation, from Moore et al, 2014
75
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
F.2 Evaluation approach
The evaluation will explore the interactions between context, SEA-PLM design and causal
assumptions, how delivery was achieved, how participants responded to and interacted with
SEA-PLM and will assess the validity of the results chain, as described in the ToC.
The evaluation team will:
1. Identify barriers to delivery, uptake, individual change and institutional reform.
2. Review how delivery is achieved, by (1) identifying how the programme has been
established, including institutional factors, partnership factors and resourcing; (2)
Identifying what contextual factors affect implementation; (3) Identifying any adaptations
that might be required to improve implementation, take up and individual and
institutional change; and (4) Checking the fidelity of the programme against the ToC
and programme documentation28.
3. Identify the likely mechanisms of impact29, including exploring country responses to,
and interactions with SEA-PLM and exploring any unanticipated pathways and
consequences of the programme.
4. Identify any contextual issues that shape theories of how the intervention works,
contextual factors which affect (and may be affected by) implementation, intervention
mechanisms and outcomes and any causal mechanisms present within the context
which act to sustain the status quo or enhance effects and the mechanisms for impact.
5. This will be done by empirically verifying activities, outputs, and outcomes and
assumptions posited along the causal chains in the ToC and draw conclusions about
what is working and what is not working in SEA-PLM and likely impact.
F.3 Data
In this section of the report, we outline the data collection methods, data collection instruments
and our approach to data collection consistency and quality.
F.3.1 Data collection methods
F.3.1.1 Inception report development and Desk Research
The process undertaken to develop this inception report has been outlined in Section 1.2. This
report has been developed to outline the key approach, methodology and workplan for the
evaluation. It includes a reconstructed ToC which has been developed based on SEA-PLM
programme documentation and an evaluation matrix based on the final evaluation questions,
and outlines the judgement criteria, indictors, data sources, and links to the ToC.
The ToC has been developed to inform the evaluation and provide the evaluation team and
programme participants with a common language regarding how change is theorised to take
place through the programme. An inception workshop will take place during the inception period
and will include a validation of the ToC.
28 Fidelity evaluation is the “extent to which delivery of an intervention adheres to the protocols and
program model originally developed” (Mobray, Holter and Teague, 2003)
29 It is recognised that as the programme has not completed a full cycle yet, impact of student learning
cannot be assessed. However, the likely mechanisms for impact can be explored by testing the
assumptions that exist along the results chain.
76
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
As part of the inception period, the team reviewed available programme materials including
strategic planning documents, any programme logic models, implementation work plans, and
available data from the programme.
In addition, a review of literature on international and regional assessments has been
undertaken during the inception period. This includes a review of the types and history of large-
scale assessments, evidence of the impact of assessments and the enablers and barriers to
achieving impact.
F.3.1.2 Qualitative data collection
Our evaluation methods are predominantly qualitative in nature, allowing us to understand what
change is occurring, how and why. The project has a focus on capacity development, which is
not readily quantifiable and many of the evaluation questions are ‘how’ and ‘why’ questions
(rather than ‘how much’). The purpose of the qualitative analysis will be to enable the evaluation
team to answer evaluation questions related to participation barriers, the extent to which
individual and institutional capacity interacts with achieving programme outcomes and the
validity of the ToC in different settings. The main focus will be to collect data along the results
chain from inputs, activities, outputs to outcomes, and identify any weak links in the ToC.
A country case study approach will be utilized, and qualitative data collection will be conducted
in three of the six participating countries through virtual or face-to-face country visits.30
Consultations with countries other than those featured as case studies will be light touch and
limited to a conversation with one government and one UNICEF respondent for each country.
The purpose of these interviews is to situate the SEA-PLM experience of the three case studies
within the wider context of the five SEAMEO countries yet to participate and the three
participating countries with which case studies are not conducted.
F.3.1.3 Country selection for case studies
The selection of case study sites is informed by consideration of a theorised sampling approach
that will produce data from a diverse set of countries that are collectively likely to produce data
on a range of experiences to inform future decisions.
Sampling ‘is a core design issue because the purposeful sample will determine what you learn
about’ (Patton 2015). Through a carefully chosen sampling strategy, the most relevant sample
can be selected, and rigour can be ensured. The chosen approach to sampling for the country
case studies is designed to generate responses from small numbers of individuals and groups
that are representative (though not statistically) of groups relevant to SEA-PLM and which allow
some identification of heterogenous contributions and experiences.
Purposive sampling can serve this purpose. Purposive sampling involves selecting a sample of
‘typical’ and ‘extreme’ cases based on available data. In essence, the aim is to select countries
that cover the range of national and education system features.
Therefore, our proposed selection for the three case studies is:
1. Lao PDR;
2. Cambodia; and
3. Malaysia.
30 See Section 7 on Risk Management and COVID related scenarios
77
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Lao PDR represents a small education system with relatively less experience in large scale
assessments. Cambodia is a larger system, with somewhat more established experience and
Malaysia is a country with significant previous participation in international surveys.
Additionally, as has been outlined in Section 2 of this report, Cambodia has seen a deterioration
in many education indicators, Laos has had some very positive progress but remains
challenged in some areas and Malaysia has shown progress and very little deterioration across
the range of education indicators. This sample also represent the countries with the highest
(Cambodia) and the lowest (Malaysia) pupil-teacher ratios of the SEA-PLM participating
countries, the highest (Cambodia) and the lowest (Lao) primary school life expectancy, the
highest (Malaysia) and the lowest (Cambodia) survival rates to the last grade of primary
education and the highest (Cambodia) and lowest (Malaysia) percentages of enrolled overage
children.
We propose that the country for the first case study is Malaysia. This will also serve as a pilot for
the other two. That first case study will be undertaken by our Team Leader and evaluation
methodology lead.
F.3.2 Data collection instruments
The qualitative research tools will consist of Key Informant Interviews. Semi-structured
questionnaires will be designed in alignment to the evaluation questions, will be informed by the
evaluation matrix and will be conducted with a broad variety of stakeholders. Instrumentation
will be tailored according to the category of respondent to ensure the most relevant and
valuable insights are obtained.
Instruments will follow the results chain from inputs to outcomes but will use the language of
SEA-PLM activities and SEA-PLM expected outcomes rather than the language of evaluators.
Instruments will begin with requesting information on the informant’s role and previous roles in
order to ascertain early in the interview where the participant’s input will be most valuable to the
evaluation. Evaluators will then focus on those questions.
F.3.3 Data collection consistency and quality
Data collection instruments will be standardised across the three countries, by sets of
informants.
The first country study will act as a pilot to inform tool finalisation. After each interview, the
evaluator will reflect informally on the tools and process and identify areas for improvement. The
data collection tools will thus be refined iteratively within the first of the case study visits.
F.4 Data analysis
The evaluation team will use qualitative comparative analysis and synthesis techniques,
carefully ensuring non-bias using triangulation and informed judgements.
F.4.1 Qualitative Comparative Analysis
Within the structure of the overarching ToC, the evaluation will use elements of Qualitative
Comparative Analysis (QCA). QCA is a useful method to identify different factors or
combinations of factors that are likely to have led to a specific outcome, in a given context.31 In
line with Contribution analysis, the report will not claim to reliably identify which, of the factors
31 See, for example: Baptist, C. and Befani, B. (2015): Qualitative Comparative Analysis: A Rigorous Qualitative Method for Assessing
Impact. Available at: http://www.coffey.com/assets/Ingenuity/Qualitative-Comparative-Analysis-June-2015.pdf . See also: Qualitative
Comparative Analysis and the Study of Policy Processes. Journal of Comparative Policy Analysis, Research and Practice Volume
19, 2017 - Issue 4: Special Issue: Validating methods for comparing public policy: Perspectives from academics and “pracademics”.
78
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
tested, are necessary or sufficient to obtain envisaged results, rather it assesses the factors that
are likely to have facilitated or hindered achieving the desired results.
The qualitative analysis will allow the evaluation team to identify the underlying assumptions
related to the ToC and the extent to which these did or did not hold in the different contexts.
F.4.2 Synthesis
To ensure efficient analysis of the data, we will keep a running record of evidence against each
evaluation question in an evidence matrix in an Excel document. The Desk Research outputs
will be reviewed to populate the evidence matrix. In the case of the country case studies,
immediately after each interview, the interviewer will add new evidence emerging from the
interview to the evidence matrix. The evidence matrix will be reviewed and discussed by the full
team regularly, with a particular focus on points where new or conflicting evidence is emerging
that needs to be explored further. In some cases, a need for a short follow-up conversation or
email with a respondent may be identified to clarify or follow-up on a response.
F.4.3 Triangulation, bias and informed judgements
With qualitative interviews, it is often likely that respondents will provide biased answers. This is
why a range of respondents is needed for triangulation. The data of each respondent will be
organised around the themes / questions which then provides an overview of the frequency and
strength of responses for each theme. Furthermore, data from interviews and document reviews
is triangulated in order to provide nuanced answers to the evaluation questions. In the case of
contradictory and opposing views the evaluation team will represent these views in our analysis.
Both for primary and secondary data, the evaluation matrix will help to structure the results and
the findings.
In addition, when synthesising data, the evaluation team will be cognisant that SEA-PLM has
been implemented only recently. So, it is likely to have involved activities and levels of
resources and commitment associated with the introduction of an innovation, as distinct from its
long-term continuation as an institutional programme. The data on the experience to date will
thus not be reflective of what would apply in a longer-term continuation, in terms of resource,
interest and commitment or technical capacity. Informed judgements will be made as part of the
evaluation looking forward, which cannot be based simply on extrapolating experience to date
looking backward.
A validation workshop will be held with stakeholders to review preliminary findings and
conclusions and to prioritise recommendations. From this workshop and other meetings, the
draft report will be revised and finalised.
F.5 Equity and gender
Our approach to mainstreaming equity and gender into the evaluation is captured in the
Evaluation Matrix (Annex B). In order to answer the evaluation question “to what extent is SEA-
PLM conducive to supporting the most marginalized populations and genders (including those
left behind)?” the evaluation team will seek evidence that equity and inclusion has been
mainstreamed into programme activities and that the data SEA-PLM produces will be used to
inform equity decisions. The evaluation team will check if assessment items are checked for
differential item functioning (DIFF analysis) by gender and location and will explore if SEA-PLM
data is able to capture inequality. Furthermore, the evaluation team will explore country capacity
to disaggregate data by social groups and explore if SEA-PLM policy makers are considering
equity, inclusion and gender as an essential component of their policy questions.
79
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
F.6 Ethics
The design and conduct of this evaluation is guided by Cambridge Education’s core values of
integrity, respect and excellence as we constantly stretch our thinking to find extra value for our
clients and their end-users. These values also guide our rigorous internal ethical standards,
which are aligned with the United Nations policies and standards for ethics and evaluations.
Applicable ethical norms and procedures of UNICEF and Cambridge Education will be applied
throughout the evaluation.
The evaluation methodology will be consistent with the UNICEF Evaluation Policy (2018), the
United Nations Evaluation Group (UNEG) Norms and Standards for Evaluation (2016), UN
SWAP Evaluation Performance Indicator, UNEG Ethical Guidelines (2008), UNICEF Procedure
for Ethical Standards and Research, Evaluation and Data Collection and Analysis (2015), as
specified in the Terms of Reference. The final evaluation report will conform to the UNICEF-
Adapted UNEG Evaluation Report Standards (2017), including consideration of a gender
equality perspective and human rights based approach, including child rights.
Photographs, audio and video recording may be used for gathering and analysis of data and for
presentation and dissemination purposes. Permission for using any photographic, video or
audio recording as part of the evaluation will be sought in advance. Before conducting any
interviews or group discussions, participants will be made aware of the extent to which the
information they provide is confidential and will be given the opportunity to withdraw at any
point.
Cambridge Education requires all its staff, suppliers and implementing partners to comply with
the highest standards of ethical behaviour in all respects and at all times, as reflected in
Cambridge Education’s mandatory adoption of its Ethics Policy and engagement in relevant
training. All Cambridge Education evaluation team members involved have satisfied Cambridge
Education’s due diligence process and prior to commencing any fieldwork will have signed
Cambridge Education’s Ethics Policy Statement.
In addition, any potential conflicts of interest are taken into account when engaging staff,
suppliers and implementing partners on any assignment and issues of payment and
compensation are clearly laid out in all contracts and managed by CE’s Commercial experts.
Cambridge Education has zero tolerance around issues of fraud, bribery, corruption, child
safeguarding and harassment; and where necessary will cooperate with the appropriate
authorities to investigate alleged transgressions, including sanctions where required.
The methodology underwent an internal ethical review. An external ethical review was not
deemed to be required since the KIIs were undertaken with only staff of following organisations:
UNICEF, Regional governance institutions; National governance institutions, ACER and TAG.
80
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Annex G Data collection tools
1. Introduction and explanation of the evaluation
2. Informed consent information
3. Confirmed or not confirmed consent [if not end interview]
Table 11 Interview items by stakeholder group
[Questions to be tailored to the specific respondent.]
e e
c c
F E C IN la n o ig e n a n r e v o la n o ita n a n r e v o R E C G A
U Rg Ng A T
Relevance
Can you please tell me about your role and how you’ve come to be
X X X X X
involved in SEA-PLM?
How did you first come to know about SEA-PLM? When was this? X X X X X
What were your first impressions about the need for SEA-PLM? X X X X X
Have our views changed since that time? X X X X X
Can you tell me more about your institution/unit and its goals and
X X X X X
aims?
Do you think these goals and aims align with the aims of SEA-
X X X X X
PLM?
Is programme delivery of track at the moment? [if no] What are the
X X X X X
main barriers to keeping to SEA-PLM workplans?
Are there any activities that aren’t included in SEA-PLM that you
X X X X X
think should be?
Efficiency
What SEA-PLM activities have you been involved in? X X X X X
Are the activities and outputs of SEA-PLM achievable?
[If no]
X X X X X
Probes: What have been the challenges? Can you give me an
example of a time when it was challenging to achieve the activities
or the outputs for SEA-PLM? How did you solve the problem?
Do you feel the technical support within SEA-PLM is provided when
X X X X X
needed and to a sufficient standard? [Prompt for information]
Do you feel the financial support provided by UNICEF has been
X X X
provided when needed?
Is the financial support sufficient to cover agreed SEA-PLM
X X X X
activities?
Do you feel UNICEF’s technical support is sufficient for
X X X X X
participating countries and delivered on time?
81
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
[Questions to be tailored to the specific respondent.]
e e
c c
F E C IN la n o ig e n a n r e v o la n o ita n a n r e v o R E C G A
U Rg Ng A T
What are some of the challenges with providing technical resources
X X X
within SEA-PLM?
What are some of the challenges with managing financial
X
resources to support SEA-PLM?
What are your thoughts on the new governance structure for SEA-
X X X X X
PLM?
Do you think there are ways UNICEF, regional government,
X X X X X
national governments and contractors could work better together?
What stage is the country/region [depending on participant] on
track to have the data from the main study clean and ready for X X X X X
analysis and dissemination?
Can you tell me more about the development of the common
learning metric? Has this process been completed yet? Where X X X X X
there any challenges?
Can you tell me more about alignment of SEA-PLM to the UIS
X X X
global learning outcome scale?
Have you participated in any technical or L&D workshops? [if so]
X X X X
Which ones?
What was your experience of the workshop/s? X X X X
What did you learn as a result of the workshop/s? X X X X
Have you applied what you’ve learnt in any other aspects of your
X X X
work?
In [country] have any of the approaches taken in SEA-PLM
X X X
influenced the approach to national assessment?
Can you tell me about the common tools and protocols that have
been developed as part of SEA-PLM?
X X X X
Probe: Are they useful? Where are they available? Can you give
me an example of when you’ve used them?
There has been a range of attempts to fundraise for SEA-PLM,
what has been successful? What hasn’t worked? What has the X X
feedback from potential funders been?
SEA-PLM aims to raise funds with donors and from participating
countries. How does [country] currently contribute to the costs of
X X X
participating in SEA-PLM? What do you think contributions will look
like in the future?
Effectiveness
What are the main policy questions you think SEA-PLM can
X X X X X
provide information on?
What questions do you have, that you think SEA-PLM data will
X X X
shed light on?
82
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
[Questions to be tailored to the specific respondent.]
e e
c c
F E C IN la n o ig e n a n r e v o la n o ita n a n r e v o R E C G A
U Rg Ng A T
Do you think there will be any challenges with disseminating the
X X X X X
data and ensuring it is used?
What are some of the key decisions you intend to make, based on
X X
SEA-PLM results?
Do you think that decisions in [country] are based on evidence?
Probe: Why/why not? What enables or inhibits evidence-based X X X
policy making?
Who will analyse the data if a policy maker or a minister has a
specific question that SEA-PLM can answer? X X X
Probe: Will there be any challenges?
The Secretariat seeks to influence SEA countries to participate in
SEA-PLM. What kind of activities have you engaged in to achieve
X X
this? What kind of outcomes have you seen? What are the best
challenging aspects of influencing SEA countries?
Likely impact
What do you think would be different in [country or region] if SEA-
X X X X X
PLM didn’t exist?
What do you think are the biggest challenges to achieving
X X X X X
education quality for all in [country or region]?
How does SEA-PLM contribute to solving education sector issues
X X X X X
in [country or region]?
What is the value-add of a common approach to assessment in
X X X X X
SEA?
Sustainability
Do you think [country] will participate in future SEA-PLM rounds?
X X X X X
Probe: Why or why not?
What technical support will countries need to participate in the next
X X X X X
round of SEA-PLM?
Has there been much turnover of staff who have received technical
X X X X X
training in [country or region]?
What are the plans for meeting the funding gap for SEA-PLM in
X X X
2019 and 2020?
What are the plans for meeting the costs of SEA-PLM in future
X X X
survey rounds?
Is SEA-PLM value for money? X X X
Equity and Gender items are mainstreamed into the other items
83
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Annex H Interview Guide
These guidelines are not intended as questionnaires. It will not be possible to cover all issues in
all categories with all individuals or groups. The evaluation team members will use their
judgement and focus on areas which are likely to add most to the team’s existing knowledge,
while allowing interviewees and groups to highlight the issues that are most important to them.
The evaluators will formulate questions in a (non-technical) way that respondents can easily
relate to, while generating evidence that is relevant to the evaluation questions that the
evaluators have in mind.
G.1 Approach to Interviews
Interviews will be a major source of information for this evaluation. These will be a means to
extract evidence, as well as to triangulate evidence drawn from other interviews and the
document review and will form part of the consultative process.
A stakeholder analysis as presented in baseline report will inform the selection of interviewees.
Over the evaluation period the evaluation team aims to target a comprehensive range of
stakeholders that fully represent all significant institutional, policy and beneficiary interests. The
team will periodically review the list of those interviewed to ensure that any potential gaps are
addressed and to prevent under-representation of key stakeholders.
All interviews will comply with the team’s commitment to the respective evaluation ethics. (The
work of the evaluation team will be guided by: OECD-DAC Evaluation Quality Standards for
Development Evaluation; UNEG Norms, Standards, Ethical Guidelines and Code of Conduct for
Evaluation in the UN System; the World Bank’s principles and standards for evaluating global
and regional partnership programs; ALNAP’s Evaluation of Humanitarian Action Guide; the
Sphere Handbook and Standards for Monitoring and Evaluation; and guidance on Ethical
Research Involving Children.)
Interviews will be conducted in confidence and usually on a one-to-one or one-to-two basis (to
enable note taking). Reports will not quote informants by name and will not include direct quotes
where it could risk revealing the participant’s identity or attribution without prior consent.
A protocol and standard format for recording interview notes is presented below. This will be
used for all interviews and will ensure systematic recording of details, while allowing for flexibility
in the specific questions asked. Interview notes will be written up, consolidated into an interview
compendium and shared among team members via the internal team-only e-library. To respect
interviewee confidentiality, the interview notes will be accessible only to team members. The
compendium of interview notes will facilitate analysis across all interviews and will enable
searches on key thematic terms, initiatives and so on. This will maximise the analytical potential
of interviews and the possibilities for triangulation.
G.2 Focus group discussions
The evaluation team may also make use of focus group discussions (FGDs). Similar to the
interview guides, the sub-headings and discussion guide points used are linked to the areas of
enquiry and evaluation questions set out in the evaluation matrix, and are intended as a guide
only, for the evaluation team to follow flexibly in order to maximise its learning from each
discussion group.
84
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Annex I Case Study Country findings against the ToC
Table 12 Assessing inputs across case study countries
Inputs Malaysia current status Philippines current status Lao PDR current status
While the provision of technical assistance is The coordination and delivery of technical The stakeholders in Lao PDR were generally
Technical
appreciated, it is considered to not entirely support is seen as an effective mechanism in the positive about the technical assistance (TA)
Assistance
recognise the expertise already within the Philippines. Technical assistance throughout the aspects of SEA-PLM and view the engagement
EPRD.. Country level stakeholders reported that SEA-PLM cycle is provided by ACER, between Lao PDR technical staff and regional
a more collaborative approach between EPRD, coordinated and financed by the UNICEF experts from SEAMEO and ACER as a critical
ACER, UNICEF should be used in the analysis Country Office. feature of the SEA-PLM experience. The validity
and reporting phase of the study by utilising the of the data (and findings) generated are
skills within the EPRD. This in turn will further acknowledged by key stakeholders, attesting to
enhance the capacity building within the successful TA engagement.
department.
The funding arrangement for Malaysia is based Data collection costs are covered by DepEd, and The cost of SEA-PLM and other sample-based
Data collection
on splitting costs between EAPRO, UNICEF includes a commitment for the next round. Data assessments is extremely high in comparison to
costs
Malaysia CO and the MoE. The actual cost for collection costs are seen as provisionally available budgets in Lao PDR. These high costs
the main survey is reported as having been sustainable and representing good VfM. include data collection and engagement in
significantly higher than had been originally However, there are concerns around the cost of technical workshops. These costs are unlikely to
communicated. Stakeholders articulated this as a Technical Assistance provision and sustainability be sustainable for Lao PDR.
major issue, with the initial commitment having due to the economic downturn resulting from the
been made on the basis of the initial budget, but COVID-19 pandemic
the time that had already been invested
prevented Malaysia from pulling out of SEA-PLM.
This demonstrates the critical importance of
transparency and predictability in costing,
particularly for the next cycle of SEA-PLM, where
domestic education budgets are likely to be
threatened by the COVID-19 economic downturn.
Technical capacity in the MoE to deliver SEA-PLM coordinated by the Bureau of Overall RIES is favourable about the SEA-PLM
Regional and
assessments is seen as being high. Education Assessment in the Department of experience, but there were aspects of SEA-PLM
country
Assessments are the mandate of the EPRD Education (BEA). BEA has capacity to implement that put considerable pressure on the human
governmental
(Education Planning and Research Division), who SEA-PLM, building on previous participation in resources within RIES.
human also coordinated the collection of data for PISA PISA and TIMSS.
resources 2018 and TIMSS 2019. Assessments and the
use of assessment data is a key priority for the
MoE, as evidenced by the use of previous TIMSS
85
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Inputs Malaysia current status Philippines current status Lao PDR current status
and PISA data as a basis for the Malaysia
Education Blueprint32.
Both UNICEF EAPRO and the UNICEF Malaysia UNICEF coordination takes place through both A national consultant hired by UNICEF provided
UNICEF co-
CO have played a key role in facilitating the country office and the East Asia and Pacific key support during SEA-PLM implementation in
ordination
conversations and relationships to deliver SEA- Regional Office (EAPRO), and has been well Lao PDR. Overall the feedback is positive from
PLM. This includes hosting the launch and other regarded in the Philippines. government counterparts, although there were
meetings during the design phase, as well as some delays due to UNICEF systems for budget
facilitating the relationship with ACER to deliver approval .
training and the development of the country
report.
Table 13 Assessing translating inputs into activities
Activities Malaysia current progress Philippines current progress Lao PDR current progress
Both the field trial and main survey activities Field study and main survey data collection, The field study (trial) and main data collection
Field studies and
were successful, with all intended schools and analysis and reporting have all been completed. activities were successfully completed, with
main survey data
95 percent of intended students being reached. Some delays and logistical complications have some delays and logistical complications.
collection
There was a delay in reaching some schools as taken place, but overall, these activities are
they had to be closed due to a haze regarded as successful.
phenomena. This forced some adaptations to
the schedule but did not affect the overall
timeline or reach of the survey.
Assessment Training was delivered on data collection, The assessment framework was developed by The core technical aspects of the work were led
framework coding and management, which were the ACER. There has been significant debate over by ACER, especially for framework
development, responsibility of the EPRD. EPRD’s role not the decision by the DepEd to test in English. development, item development, sampling and
item including the sampling and analysis procedure Many perceive that the complications this regional report preparation. The local report was
development, represent a potential missed opportunity, both to decision has introduced were foreseeable. prepared by an international consultant.
sampling, increase the efficiency of the conversion of
analysis and inputs to activities and to build capacity.
reporting
86
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Activities Malaysia current progress Philippines current progress Lao PDR current progress
Technical Training on test administration, coding and data ACER led capacity building activities. These ACER-led capacity development activities
capacity building management was delivered nationally and then were seen as sufficient for current needs but focused on sampling, data collection, data entry
activities cascaded to field teams and test administrators. limited in scope and design. Stakeholders see a and data coding. RIES technical capacity was
Stakeholders report that the training was significant gap in building sustainable capacity improved in these areas, although there are
adequate to deliver on the requirements of to analyse and interpret the results of some concerns about this capacity being
administering SEA-PLM. However, there is a assessments, with current activities focused on specific to the “ACER model”.
need for the technical assistance regarding data collection and management.
analysis of data and reporting to be more
comprehensive and given enough time,
including enhancing the opportunity for capacity
building among EPRD personnel (echoing the
desire to be more involved in the analysis of
data).
Develop and Dissemination of SEA-PLM results is an ongoing SEA-PLM is part of a wider participation in MoES communication and dissemination
disseminate process in Malaysia. ACER has been ILSAs, and dissemination of results was done in strategy is largely absent for SEA-PLM.
communications commissioned by UNICEF Malaysia CO to tandem with TIMSS 2019 results. Public Stakeholders mainly informed about SEA-PLM
strategy prepare Malaysia’s country report. The final draft appetite for results from ILSAs is strong, which during the release of the final results. This is
was received and approved by Malaysia in supported the wide dissemination of results. related to larger concerns about assessment
August. However, the report has yet to dissemination and communication in Lao PDR.
disseminated publicly pending the final copy
(which will include design and infographics)
which ACER plan to deliver by end of
September 2021.
Regional No data. The Philippines participated in the regional Lao PDR leadership participated in regional
endorsement launch of SEA-PLM results. This was well events, and there was positive feedback from
activities received, though disappointment was expressed stakeholders about participation in regional
with the decision to visually rank participating assessment.
countries in the regional report.
Fundraising with While the proposal for participating in the next SEA-PLM is currently jointly funded by UNICEF Lao PDR SEA-PLM engagement is financed
donors and round has been approved by the Ministry of and the DepEd, with a similar arrangement entirely by UNICEF.
countries Education, it still requires final ministerial expected for the next round.
approval. The value of SEA-PLM is widely
agreed upon as being an important data set for
analysing primary school achievement, however
cost effectiveness and availability of resources
is the key area of concern.
87
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Table 14 Accessing translating activities into outputs
Outputs Malaysia current progress Philippines current progress Lao PDR current progress
Reliable, valid, The value of SEA-PLM is widely agreed The results of SEA-PLM and of ILSAs in There is widespread acceptance of the validity of
relevant and upon as being an important data set for general have been well received and are results among MoES stakeholders and a strong sense
rigorous learning analysing primary school achievement. regarded as robust by all parties. that the SEA-PLM “seal of approval” ensures quality
and contextual Reports from Malaysia indicate that SEA- information. However, some concerns were expressed
data PLM data quality is well regarded. about the utility of Global Citizenship questions, as well
as the efficiency of the background questionnaires.
A regional There is broad enthusiasm at the idea of There is broad enthusiasm at the idea of There was general agreement among stakeholders that
common learning having regional benchmarks and the having regional benchmarks and the one of the main purposes of participating is to compare
metric opportunity to learn from neighbouring opportunity to learn from neighbouring Lao performance against a regional standard. No major
countries. However, this is tempered by countries. However, this is tempered by a concerns were expressed about development of
a reluctance to be ranked by country. reluctance to be ranked by country. Mathematics and language test questions, or
relevance/applicability to Lao PDR curriculum.
SEA-PLM There was an acknowledgement from There is a desire in some quarters for SEA- There was an acknowledgement from development
alignment to the development partners that SEA-PLM fills PLM to be comparable to other ILSAs. Options partners that SEA-PLM fills a gap for the region in
global UIS learning a gap for the region in terms of include technical linkages with other terms of international monitoring of student
outcomes scale international monitoring of student International Association for the Evaluation of achievement and SDG goals.
achievement and SDG goals. Educational Achievement (IEA) studies.
Regional and Transferability of L&D products were limited Dissemination has begun mainly with high level
national by the narrow scope of training conducted as discussions of SEA-PLM results.
workshops and part of SEA-PLM. Regional, and national
Learning and dissemination workshops were well regarded.
Development (L&D)
products
Strategies for While the country report for Malaysia has There are many questions and priorities for There is general agreement among government and
analysing and not yet been released. However, there is supplementary analysis, but this has been development partner stakeholders that not enough is
disseminating data a need for further explanations and hampered by poor strategic coordination, and being done with analysis and dissemination of LSAs in
discussions on how the data was a lack of advance agreement with ACER on Lao PDR, including SEA-PLM. There is no clear
analysed and to ensure stakeholders what data would be made available to DepEd. dissemination strategy in place (see above). Capacity
understand the national dataset. limitations stand out as the main constraint in analysis
and dissemination activities.
Common tools, Tools are seen as useful, and there is a There was positive feedback about common tools for
protocols and desire to use the written questions and measuring student achievement, but much work
standards to contextual questionnaires either directly, or to remains in terms of using the results. An area where
generate and inform national assessment activities. learning from other country experiences may be very
utilise assessment helpful.
data across
ASEAN countries
88
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Outputs Malaysia current progress Philippines current progress Lao PDR current progress
Funding Reaching a funding agreement for the next Continued funding for data collection agreed Consistent with previous LSAs, development partners
agreements with cycle of SEA-PLM is in process and will with DepEd. (UNICEF) are supporting all costs for SEA-PLM. Given
donors and present a significant challenge. While the the Lao PDR government budget situation this support
participant proposal for participating in the next round will be necessary for the next round of SEA-PLM, but is
countries has been approved by the Ministry of yet to be confirmed.
Education, it still requires final ministerial
approval. Cost effectiveness and availability
of resources is a key area of concern.
Table 15 Assessing progress towards achieving outcomes
Outcomes and impact Malaysia current progress Philippines current progress Lao PDR current progress
Policy makers use data There is some suggestion that SEA-PLM There are strong, consistent intentions to Policymakers are aware of the SEA-PLM findings,
to meaningfully support could be used to help teachers develop use SEA-PLM data to inform decision and there is evidence that they are having more of
decision-making their own assessment approaches and making in DepEd. DepEd joined SEA-PLM an effect on the dialogue than previous student
measure learning loss due to the as part of a departmental strategy to use assessment survey results. Nevertheless, the use
pandemic Using SEA-PLM items to ILSA data to inform policy. While the results of the information has been limited to sharing the
encourage teachers to teach and assess of the first round have only been released main findings. What is largely missing is a process
critical thinking and the application of recently, there does not seem to be a clear that effectively translates results into actions, with
knowledge (rather than more rote strategy to deliver on this strategy. coordination across departments.
approaches) is potentially a valuable
contribution of SEA-PLM.
Enhanced capacity to Capacity for the generation and analysis of BEA is adapting national assessment RIES technical capacity has been enhanced in the
generate and analyse assessment data ais strong in Malaysia. As approaches based on participation in SEA- areas of data collection, data entry and data
assessment data at a long time participant in both TIMSS and PLM. However, capacity to analyse coding, however capacity is still limited in key areas
regional, national and PISA, it is hard to attribute any change in assessment data to inform policy is of data analysis and report writing. MoES
sub-national levels the assessment capacity of the ministry of constrained and still largely supported by stakeholders also face challenges in using results
education to SEA-PLM at this stage. external actors. from SEA-PLM and previous assessments, which
includes reaching down to districts, schools and
teachers with useful findings. This is a major area
for improvement, and could be supported through
peer-to-peer learning regarding the experiences in
other countries.
Sustainable regional No data There are mixed views on the feasibility or Government respondents were positive about the
integration of relevant desirability of greater regional integration. SEA-PLM experience, and are eager to continue,
large-scale assessment While the regional nature of SEA-PLM is however there are some concerns from
and outcomes valued, there are concerns around the use development partners about costs and overall
value for money.
89
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Outcomes and impact Malaysia current progress Philippines current progress Lao PDR current progress
of assessment data to rank and compare
countries.
Enhanced ASEAN The ASEA “flavour” of the GC content Stakeholders agree that there is an There is strong commitment from government to
integration in terms of was viewed positively by stakeholders in opportunity for greater ASEAN integration the overall mission of SEA-PLM, and recognition of
assessment Malaysia. While the same concerns in terms of knowledge exchange around the validity of regional data. However, work
around the ranking of countries exist in assessment data and policy implications. remains to develop capacity to the regional
Malaysia as in other countries, Several stakeholder felt SEAMEO could be standard.
participants are generally positive about a stronger advocate for regional integration
further integration of assessments in the on the back of SEA-PLM.
region.
SEA-PLM is on the The Ministry of Education of Malaysia has National government support is strong at There is strong support from government
regional agenda and publicly affirmed its intention to participate present, but threats remain. Firstly, it is not stakeholders for SEA-PLM engagement, and
national government in future rounds of SEA-PLM. However, absolute that budget will remain available evidence that SEA-PLM results are generating
support is re-enforced there is a process of budget allocation for future rounds in the current fiscal more discussions about student learning than
which needs to be undertaken before the environment. However, more importantly, previous assessments. Again, there are some
final support can be confirmed. This government support will wane if the concerns from development partners about costs,
process is viewed as presenting a learning from SEA-PLM is not translated but also recognition that SEA-PLM has raised the
challenge for the team seeking to confirm into improvements in scores. profile of the education sector in the region, and will
the participating in future rounds. also support future assessment work across the
region.
90
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Annex J Documents related to ethical clearance
91
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
92
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
93
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
94
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
95
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
96
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
97
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
Singed document available for all team members. Please contact following to
obtain copies of these documents.
Mott MacDonald Limited trading as Cambridge Education. Registered in England and Wales no.
1243967. Registered office: Mott MacDonald House, 8-10 Sydenham Road, Croydon CR0 2EE,
United Kingdom
United Nations Children’s Fund (UNICEF)
East Asia and the Pacific Regional Office
Evaluation Section
98
Cambridge Education | Evaluation of Southeast Asia-Primary Learning Metrics
Evaluation Report
camb-ed.com
99